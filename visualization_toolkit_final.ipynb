{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d42a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b250ddb3",
   "metadata": {},
   "source": [
    "# Visualization Toolkit Usage Guide\n",
    "\n",
    "This notebook provides a suite of semantic analysis and visualization tools to explore word-level patterns and relationships in qualitative text data. It supports multiple clustering and embedding strategies for t-SNE, semantic networks, and word clouds.\n",
    "\n",
    "### What You Can Do with This Toolkit\n",
    "\n",
    "- **Generate Word Clouds**  \n",
    "  Visualize the most frequent and salient terms across your dataset or within filtered subsets based on keywords or project name.\n",
    "\n",
    "- **Plot t-SNE Semantic Maps**  \n",
    "  Reduce high-dimensional similarity or co-occurrence matrices into 2D using t-SNE to highlight semantic proximity between words. Seed words are emphasized to anchor interpretation.\n",
    "\n",
    "- **Create Word-Based Heatmaps and Semantic Networks**  \n",
    "  Explore how terms relate to each other in both visual space and co-occurrence structure:\n",
    "  \n",
    "  - **Basic Heatmap**: Highlights semantically clustered keywords based on selected embeddings or similarity matrices.\n",
    "  - **Heatmap + Network (Black & White)**: Adds a basic network graph on top of the heatmap using default node/edge colors (no bundling, no category styling).\n",
    "  - **Heatmap + Network (Colored Nodes & Edges)**: Fully stylized version including colored clusters, semantic links, and optional edge styling.\n",
    "\n",
    "- **Visualize Code Co-Occurrence Heatmaps**  \n",
    "  Plot the frequency with which qualitative codes appear together in the same entries. \n",
    "\n",
    "These tools help you visually investigate language patterns, conceptual clustering, and topic proximityâ€”whether youâ€™re doing grounded theory, thematic analysis, or exploratory semantic mapping.\n",
    "\n",
    "### How to Use\n",
    "1. **Prepare Your Dataset**  \n",
    "   Make sure your dataset is a `.csv` or DataFrame with at least:\n",
    "   - A `text` column (raw text)\n",
    "   - Optionally, a `project` column (for subsetting)\n",
    "   - optionally a `codes' column` (for subsetting, and analysis)\n",
    "   - Please see READ.md for the complete schema and more possibilities.\n",
    "\n",
    "2. **Set Key Parameters**  \n",
    "   Most functions accept:\n",
    "   - `stopwords_path`: path to extra stopwords (optional)\n",
    "   - `clustering_method`: 1 = RoBERTa, 2 = Jaccard, 3 = PMI, 4 = TF-IDF\n",
    "   - `distance_metric`: \"cosine\" or \"default\" (used for similarity matrix choice)\n",
    "\n",
    "3. **Run Visualizations**\n",
    "You can explore and generate a variety of visual outputs using the execution blocks provided in the **Analytics Tools** and **Advanced Analytics Tools** sections.\n",
    "\n",
    "These include:\n",
    "- **Word Clouds** for highlighting high-frequency terms in selected texts or projects.\n",
    "- **t-SNE Semantic Maps** to project word relationships into 2D space for visualizing proximity and clusters.\n",
    "- **Word-Based Heatmaps** to show how frequently words co-occur.\n",
    "- **Semantic Network Graphs** (with optional heatmaps), including:\n",
    "  - show relationships in text\n",
    "  - basic  node/seed networks,\n",
    "  - networks with customized node colors,\n",
    "  - and networks with or without edge bundling.\n",
    "- **Code-based Heatmap** to visualize co-occurrence patterns among qualitative codes\n",
    "\n",
    "ðŸ“ Simply scroll down to the relevant execution cells to run and customize these visualizations based on your dataset.\n",
    "\n",
    "### File Structure Notes\n",
    "- Define constants like `DATA_DIR`, `OUTPUT_DIR`, and stopword files before running.\n",
    "- Ensure required files and embeddings are preloaded or generated using prior pipeline steps.\n",
    "\n",
    "> Tip: Start by testing on one project or topic before scaling to all data.\n",
    "\n",
    "# TESTING, PLEASE DO NOT SHARE. CITE WITHOUT WRITTEN PERMISSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1316a766",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294f11c0",
   "metadata": {},
   "source": [
    "### Packages Loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee1d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python built-ins\n",
    "# Using Python 3.11.13\n",
    "import os\n",
    "import urllib.request\n",
    "from functools import lru_cache\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import ast\n",
    "import sys\n",
    "import platform\n",
    "import importlib\n",
    "\n",
    "# Data loading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Natural Language Processing (NLP)\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure required NLTK resources are available\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt_tab\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt_tab\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"corpora/stopwords\")\n",
    "except LookupError:\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"corpora/wordnet\")\n",
    "except LookupError:\n",
    "    nltk.download(\"wordnet\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"taggers/averaged_perceptron_tagger\")\n",
    "except LookupError:\n",
    "    nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "# Sentence / transformer embeddings\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Machine Learning / Math\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "import scipy.cluster.hierarchy as hierarchy\n",
    "import torch\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import matplotlib.cm as cm\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image, ImageDraw\n",
    "import pprint\n",
    "\n",
    "# Validation & helpers\n",
    "from pydantic import BaseModel, FilePath, ValidationError, field_validator\n",
    "from typing import List, Optional, Union, Any\n",
    "import nbimporter  # import from notebook\n",
    "# Project-Local Modules\n",
    "# âš ï¸ Environment Warning\n",
    "# To successfully import local Python modules (e.g., `vis_tool_core.py`)\n",
    "# into this Jupyter notebook, ensure that the `.py` file is located in the **same directory**\n",
    "# as this notebook.\n",
    "import warnings \n",
    "\n",
    "from function import vis_tool_core\n",
    "# Force reload the module to apply any code changes\n",
    "importlib.reload(vis_tool_core)\n",
    "from function.vis_tool_core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66067b34",
   "metadata": {},
   "source": [
    "#### Version Check\n",
    "\n",
    "This section checks current version of packages loaded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d97b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version_check.py \n",
    "import platform, importlib\n",
    "from packaging.version import Version\n",
    "from packaging.specifiers import SpecifierSet\n",
    "\n",
    "SUPPORTED = {\n",
    "    \"python\": \">=3.10,<3.13.2\",\n",
    "    \"numpy\": \">=1.24,<2.2\",\n",
    "    \"pandas\": \">=1.5,<2.3\",\n",
    "    \"nltk\": \">=3.8,<4\",\n",
    "    \"gensim\": \">=4.3,<5\",\n",
    "    \"sklearn\": \">=1.2,<1.5\",\n",
    "    \"sentence_transformers\": \">=2.2,<3.6\",\n",
    "    \"transformers\": \">=4.36,<5.1\",\n",
    "    \"torch\": \">=2.1,<2.8\",\n",
    "    \"matplotlib\": \">=3.7,<4\",\n",
    "    \"seaborn\": \">=0.12,<1\",\n",
    "    \"networkx\": \">=3,<4\",\n",
    "    \"wordcloud\": \">=1.9,<2\",\n",
    "    \"dash\": \">=2.10,<3\",\n",
    "    \"plotly\": \">=5.0,<6\",  \n",
    "    \"tqdm\": \">=4.65,<5\",\n",
    "    \"joblib\": \">=1.2,<2\",     \n",
    "    \"dill\": \">=0.3.6,<0.4\",\n",
    "    \"python-dotenv\": \">=0.15,<2\",\n",
    "    \"pydantic\": \">=1.10,<3\",\n",
    "}\n",
    "print(f\"\\n Environment Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "if not check_versions(SUPPORTED):\n",
    "    print(\"Please align your environment to the required versions above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e794c1a",
   "metadata": {},
   "source": [
    "### Environment Configuration\n",
    "\n",
    "In this section, we define the core directory structure used throughout the replication project. These paths help organize:\n",
    "\n",
    "- **raw data**,  \n",
    "- **trained models**,  \n",
    "- **clustering results**, and  \n",
    "- **final outputs**.\n",
    "\n",
    "This setup also makes it easier for users to customize where intermediate results and final outputs will be saved. For example, by changing these directory names, users can **create their own versions of model runs or clustering outputs without overwriting previous results.**\n",
    "\n",
    "All directories will be automatically created if they don't already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2546e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"input\", exist_ok=True)\n",
    "\n",
    "# Define base directory\n",
    "BASE_DIR = os.getcwd()\n",
    "\n",
    "# Define project directories\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "INPUT_DIR = os.path.join(BASE_DIR, \"input\")\n",
    "BACKUP_DIR = os.path.join(BASE_DIR, \"backup\")\n",
    "MODEL_DIR = os.path.join(DATA_DIR, \"models\", \"auto_model\")\n",
    "CLUSTERING_DIR = os.path.join(DATA_DIR, \"models\", \"clusterings\")\n",
    "OUTPUT_DIR = os.path.join(DATA_DIR, \"outputs\")\n",
    "LAST_CSV_PATH = os.path.join(OUTPUT_DIR, \"last_csv_path.txt\")\n",
    "\n",
    "# Paths\n",
    "CSV_PATH = os.path.join(DATA_DIR, \"data.csv\")\n",
    "STOP_LIST_FILE = os.path.join(INPUT_DIR, \"additional_stops.txt\")\n",
    "\n",
    "# Cache \n",
    "LAST_CONFIG_PATH = \"last_run_config.json\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for d in (BACKUP_DIR, MODEL_DIR, CLUSTERING_DIR, OUTPUT_DIR):\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# Initialize NLTK components\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Initialize lemmatizer (only once)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Torch Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# HuggingFace token for private models\n",
    "load_dotenv()\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\") or \"\" #this is free to get, but this might be packageable with one of the BERT models. Currently using roBERTa, but the distilled version is doable.\n",
    "\n",
    "# Enable GPU acceleration if available\n",
    "USE_GPU_ACCELERATION = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\") #not optimized for MPS on mac, but should access.\n",
    "\n",
    "# Define and load the preferred model\n",
    "# Use 'roberta-base' for tests (smaller, faster), and switch back to 'all-roberta-large-v1' for full runs\n",
    "MODEL_NAME = \"roberta-base\"  \n",
    "\n",
    "print(f\"Loading '{MODEL_NAME}'...\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# Load HuggingFace model\n",
    "# âš ï¸ Note on add_pooling_layer:\n",
    "# - Plain RoBERTa checkpoints (e.g., \"roberta-base\", \"roberta-large\") do NOT include a pooling layer.\n",
    "#   If you load them with the default settings, HuggingFace will create a random pooler and issue a warning.\n",
    "#   To avoid this, we explicitly set add_pooling_layer=False for these cases.\n",
    "# - For Sentence-Transformers models (e.g., \"all-roberta-large-v1\"), KEEP the default (pooling layer included).\n",
    "#   These models rely on pooling for producing sentence embeddings.\n",
    "#\n",
    "# So: \n",
    "#   MODEL = AutoModel.from_pretrained(MODEL_NAME, add_pooling_layer=False)  # for roberta-base / roberta-large\n",
    "#   MODEL = AutoModel.from_pretrained(MODEL_NAME)                          # for embedding models like all-roberta-large-v1\n",
    "\n",
    "MODEL = AutoModel.from_pretrained(MODEL_NAME, add_pooling_layer=False)\n",
    "MAX_TOKENS = TOKENIZER.model_max_length\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"Some weights of RobertaModel were not initialized\"\n",
    ")\n",
    "\n",
    "# Move model to the appropriate device\n",
    "MODEL.to(device)\n",
    "\n",
    "# Optional: Enable CUDA optimizations for better performance on NVIDIA GPUs\n",
    "if device.type == 'cuda':\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "    print(f\"CUDA memory reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "\n",
    "print(f\"Loaded '{MODEL_NAME}' on {device}\")\n",
    "print(f\"Maximum length of tokens is {MAX_TOKENS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc74655",
   "metadata": {},
   "source": [
    "### Stopword Expansion and Semantic Word Family Definitions\n",
    "\n",
    "This section defines a comprehensive list of stopwords, extending NLTKâ€™s default stopword set with:\n",
    "- **punctuation**, \n",
    "- **contractions**, \n",
    "- **common filler words**, and \n",
    "- **project-specific conversational terms** that are semantically uninformative in analysis.\n",
    "\n",
    "We also build a custom `WORD_FAMILIES` dictionary, which groups related words into unified concepts (e.g., \"death\", \"caregiver\", \"memory\"). This allows the model to:\n",
    "- **compress synonyms and variations** into semantically meaningful units,\n",
    "- **reduce noise** in the embedding space,\n",
    "- and **support cultural/qualitative interpretation** of the results.\n",
    "\n",
    "This section also includes validation checks to:\n",
    "- Ensure **no accidental overlaps** between stopwords and key analytical terms,\n",
    "- Detect **redundant words across families**, \n",
    "- And print summaries for user verification.\n",
    "\n",
    "These definitions are critical for interpretability in downstream visualization and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536db12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base stop words from NLTK and punctuation\n",
    "default_stop_words = set(stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "# Common contractions and special characters\n",
    "common_special_chars = {'...', \"''\", '\"\"', \"``\", \"--\", \"n't\", \"'s\", '|'}\n",
    "default_stop_words.update(common_special_chars)\n",
    "\n",
    "# Additional stop words for conversation analysis\n",
    "additional_stops = {\n",
    "    # Common verbs that don't add semantic value\n",
    "    'would', 'could', 'may', 'also', 'one', 'like', 'get', 'well', \n",
    "    'many', 'much', 'even', 'said', 'say', 'says', 'see', 'seen',\n",
    "    'use', 'used', 'using', 'way', 'ways', 'make', 'makes', 'made',\n",
    "    'take', 'takes', 'took', 'taken', 'go', 'goes', 'going', 'went',\n",
    "    'come', 'comes', 'coming', 'came', 'try', 'tries', 'tried',\n",
    "    \n",
    "    # General placeholders\n",
    "    'thing', 'things', 'something', 'anything', 'everything',\n",
    "    'someone', 'anyone', 'everyone', 'somebody', 'anybody', 'everybody',\n",
    "    \n",
    "    # Filler words and conversational markers\n",
    "    'um', 'uh', 'hmm', 'oh', 'huh', 'uhhuh', 'yeah', 'okay',\n",
    "    'sorta', 'kinda', 'basically', 'literally', 'honestly', 'anyway',\n",
    "    'whatever', 'actually', 'really', 'just', 'pretty', 'right',\n",
    "    \n",
    "    # Common pronouns and contractions\n",
    "    'im', 'youre', 'shes', 'hes', 'theyre', 'ive', 'dont', 'cant',\n",
    "    'doesnt', 'didnt', 'thats', 'theres', 'heres', 'couldnt', 'shouldnt', \n",
    "    'wouldnt', 'lets', 'youve', 'weve', 'theyve', 'whats', 'whos', 'hows', \n",
    "    'wheres', 'gotta', 'gonna', 'wanna', 'aint', 'alot', 'isnt', 'wont',\n",
    "    \n",
    "    # Enhanced common words to exclude from auto-selection\n",
    "    'tell', 'told', 'right', 'lot', 'way', 'kind', 'bit', 'maybe', \n",
    "    'still', 'stuff', 'sure', 'getting', 'gets', 'goes', 'gone',\n",
    "    'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten',\n",
    "    'first', 'second', 'third', 'fourth', 'fifth', 'last', 'next',\n",
    "    'should', 'might', 'must', 'may', 'can', 'cannot',\n",
    "    'ah', 'wow', 'yes', 'no', 'nope', 'ok',\n",
    "    'hey', 'hi', 'hello', 'bye', 'goodbye', 'etc', 'etc.',\n",
    "    \n",
    "    # Single letters and 2 letter pairs\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "    'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "    'aa', 'ab', 'ac', 'ad', 'ae', 'af', 'ag', 'ah', 'ai', 'aj', 'ak', 'al', 'am',\n",
    "    'an', 'ao', 'ap', 'aq', 'ar', 'as', 'at', 'au', 'av', 'aw', 'ax', 'ay', 'az',\n",
    "    'ba', 'bb', 'bc', 'bd', 'be', 'bf', 'bg', 'bh', 'bi', 'bj', 'bk', 'bl', 'bm',\n",
    "    'bn', 'bo', 'bp', 'bq', 'br', 'bs', 'bt', 'bu', 'bv', 'bw', 'bx', 'by', 'bz',\n",
    "    'ca', 'cb', 'cc', 'cd', 'ce', 'cf', 'cg', 'ch', 'ci', 'cj', 'ck', 'cl', 'cm',\n",
    "    'cn', 'co', 'cp', 'cq', 'cr', 'cs', 'ct', 'cu', 'cv', 'cw', 'cx', 'cy', 'cz',\n",
    "    'da', 'db', 'dc', 'dd', 'de', 'df', 'dg', 'dh', 'di', 'dj', 'dk', 'dl', 'dm',\n",
    "    'dn', 'do', 'dp', 'dq', 'dr', 'ds', 'dt', 'du', 'dv', 'dw', 'dx', 'dy', 'dz',\n",
    "    'ea', 'eb', 'ec', 'ed', 'ee', 'ef', 'eg', 'eh', 'ei', 'ej', 'ek', 'el', 'em',\n",
    "    'en', 'eo', 'ep', 'eq', 'er', 'es', 'et', 'eu', 'ev', 'ew', 'ex', 'ey', 'ez',\n",
    "    'fa', 'fb', 'fc', 'fd', 'fe', 'ff', 'fg', 'fh', 'fi', 'fj', 'fk', 'fl', 'fm',\n",
    "    'fn', 'fo', 'fp', 'fq', 'fr', 'fs', 'ft', 'fu', 'fv', 'fw', 'fx', 'fy', 'fz',\n",
    "    'ga', 'gb', 'gc', 'gd', 'ge', 'gf', 'gg', 'gh', 'gi', 'gj', 'gk', 'gl', 'gm',\n",
    "    'gn', 'go', 'gp', 'gq', 'gr', 'gs', 'gt', 'gu', 'gv', 'gw', 'gx', 'gy', 'gz',\n",
    "    'ha', 'hb', 'hc', 'hd', 'he', 'hf', 'hg', 'hh', 'hi', 'hj', 'hk', 'hl', 'hm',\n",
    "    'hn', 'ho', 'hp', 'hq', 'hr', 'hs', 'ht', 'hu', 'hv', 'hw', 'hx', 'hy', 'hz',\n",
    "    'ia', 'ib', 'ic', 'id', 'ie', 'if', 'ig', 'ih', 'ii', 'ij', 'ik', 'il', 'im',\n",
    "    \n",
    "    # Project-specific stopwords from file\n",
    "    'a', 'um', 'an', 'the', 'have', 'dont', 'get', 'know', 'there', 'org', 'happen', 'find'\n",
    "}\n",
    "\n",
    "# Remove 'few', 'many', and 'more' from stop words as they're needed in word families\n",
    "# if 'few' in additional_stops:\n",
    "#     additional_stops.remove('few')\n",
    "# if 'many' in additional_stops:\n",
    "#     additional_stops.remove('many')\n",
    "# if 'more' in additional_stops:\n",
    "#     additional_stops.remove('more')\n",
    "# if 'few' in default_stop_words:\n",
    "#     default_stop_words.remove('few')\n",
    "# if 'many' in default_stop_words:\n",
    "#     default_stop_words.remove('many')\n",
    "# if 'more' in default_stop_words:\n",
    "#     default_stop_words.remove('more')\n",
    "\n",
    "# Update the default stop words with our additional list\n",
    "default_stop_words.update(additional_stops)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Normalized Word-Family Map\n",
    "# ------------------------------------------------\n",
    "\n",
    "\n",
    "# word families allow compression to move from embedding space (semantic), to meaning space (cultural); Basically, the idea is to use to get at more meaningful concepts and minimize false negatives for use with qual analyses (see Li and Abramson 2021; Abramson 2011)\n",
    "WORD_FAMILIES = {\n",
    "    \"education\": [\"college\", \"schooling\", \"graduate school\", \"university\"],\n",
    "    \"people\": [\"person\", \"student\", \"teacher\"],\n",
    "    \"dementia\": [\"dementia\"]\n",
    "}\n",
    "\n",
    "# Remove empty word families\n",
    "WORD_FAMILIES = {k: v for k, v in WORD_FAMILIES.items() if v}\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Summary and checks\n",
    "# ------------------------------------------------\n",
    "\n",
    "# Print summary information\n",
    "print(f\"Total stop words: {len(default_stop_words)}\")\n",
    "print(f\"Total word family compressions: {len(WORD_FAMILIES)}\")\n",
    "\n",
    "# Check for redundancies in stop words\n",
    "duplicate_stops = [item for item in additional_stops if item in default_stop_words and item not in additional_stops]\n",
    "if duplicate_stops:\n",
    "    print(f\"Warning: Found {len(duplicate_stops)} redundant stop words\")\n",
    "else:\n",
    "    print(\"No redundant stop words found\")\n",
    "\n",
    "# Check for redundancies in word families\n",
    "all_words = []\n",
    "word_to_families = {}\n",
    "for family, words in WORD_FAMILIES.items():\n",
    "    for word in words:\n",
    "        all_words.append(word)\n",
    "        if word not in word_to_families:\n",
    "            word_to_families[word] = []\n",
    "        word_to_families[word].append(family)\n",
    "    \n",
    "duplicate_words = [word for word in all_words if all_words.count(word) > 1]\n",
    "if duplicate_words:\n",
    "    print(f\"Warning: Found {len(set(duplicate_words))} words appearing in multiple word families:\")\n",
    "    for word in sorted(set(duplicate_words)):\n",
    "        families = word_to_families[word]\n",
    "        print(f\"  '{word}' appears in: {', '.join(families)}\")\n",
    "else:\n",
    "    print(\"No words appear in multiple word families\")\n",
    "\n",
    "# Check for overlap between word families and stop words\n",
    "stop_words_set = set(default_stop_words) | set(additional_stops)\n",
    "overlap_words = []\n",
    "overlap_by_family = {}\n",
    "\n",
    "for family, words in WORD_FAMILIES.items():\n",
    "    family_overlaps = [word for word in words if word in stop_words_set]\n",
    "    if family_overlaps:\n",
    "        overlap_by_family[family] = family_overlaps\n",
    "        overlap_words.extend(family_overlaps)\n",
    "\n",
    "if overlap_words:\n",
    "    print(f\"\\nWarning: Found {len(set(overlap_words))} words that appear in both word families and stop words:\")\n",
    "    for family, words in sorted(overlap_by_family.items()):\n",
    "        print(f\"  Family '{family}' has {len(words)} stop words: {', '.join(sorted(words))}\")\n",
    "else:\n",
    "    print(\"\\nNo overlap between word families and stop words\")\n",
    "\n",
    "# Verify no words in word families are in stop words after fixing\n",
    "stop_words_set = set(default_stop_words)\n",
    "all_family_words = [word for family_words in WORD_FAMILIES.values() for word in family_words]\n",
    "remaining_overlaps = [word for word in all_family_words if word in stop_words_set]\n",
    "print(f\"\\nAfter fixes, remaining overlaps between word families and stop words: {len(remaining_overlaps)}\")\n",
    "if remaining_overlaps:\n",
    "    print(f\"Remaining overlapping words: {', '.join(sorted(remaining_overlaps))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6833a002",
   "metadata": {},
   "source": [
    "### Global Variables Updates\n",
    "\n",
    "Sync updated variables from notebook into vis_tool_core module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d0b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word family mapping\n",
    "vis_tool_core.WORD_FAMILIES = WORD_FAMILIES  \n",
    "\n",
    "# Stopword lists\n",
    "vis_tool_core.additional_stops = additional_stops  \n",
    "vis_tool_core.default_stop_words = default_stop_words  \n",
    "\n",
    "# NLP tools\n",
    "vis_tool_core.TOKENIZER = TOKENIZER  \n",
    "vis_tool_core.lemmatizer = lemmatizer  \n",
    "vis_tool_core.MODEL = MODEL\n",
    "vis_tool_core.MAX_TOKENS = MAX_TOKENS\n",
    "\n",
    "# Output and cache directories\n",
    "vis_tool_core.OUTPUT_DIR = OUTPUT_DIR  \n",
    "vis_tool_core.CLUSTERING_DIR = CLUSTERING_DIR  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f08c41",
   "metadata": {},
   "source": [
    "### Validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d885044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress Pydantic deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pydantic\")\n",
    "\n",
    "# Temporarily using compatibility mode\n",
    "\n",
    "class VisualsInput(BaseModel):\n",
    "    filepath: str\n",
    "    stop_list: Optional[str] = None\n",
    "    num_words: int = 10\n",
    "    clustering_method: int = 1\n",
    "    distance_metric: str = \"default\" # \"default\" | \"cosine\"\n",
    "    reuse_clusterings: bool = False\n",
    "    window_size: int = 5\n",
    "    min_word_frequency: int = 2\n",
    "    cross_pos_normalize: bool = False\n",
    "    projects: Optional[List[str]] = None\n",
    "    data_groups: Optional[List[str]] = None\n",
    "    codes: Optional[List[str]] = None\n",
    "    seed_words: Optional[str] = None\n",
    "\n",
    "    # ---------- validators ----------\n",
    "    @field_validator(\"num_words\")\n",
    "    def validate_num_words(cls, v):\n",
    "        if v <= 0:\n",
    "            raise ValueError(\"num_words must be greater than 0\")\n",
    "        return v\n",
    "\n",
    "    @field_validator(\"clustering_method\")\n",
    "    def validate_clustering_method(cls, v):\n",
    "        if v not in [1, 2, 3, 4]:\n",
    "            raise ValueError(\"clustering_method must be 1-4\")\n",
    "        return v\n",
    "\n",
    "    @field_validator(\"window_size\")\n",
    "    def validate_window_size(cls, v):\n",
    "        if v <= 0:\n",
    "            raise ValueError(\"window_size must be greater than 0\")\n",
    "        return v\n",
    "\n",
    "    @field_validator(\"min_word_frequency\")\n",
    "    def validate_min_word_frequency(cls, v):\n",
    "        if v <= 0:\n",
    "            raise ValueError(\"min_word_frequency must be greater than 0\")\n",
    "        return v\n",
    "\n",
    "    # ---------- Configuration ----------\n",
    "    model_config = ConfigDict(extra=\"allow\", validate_assignment=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bb964b",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3b5947",
   "metadata": {},
   "source": [
    "### 1. Wordcloud \n",
    "\n",
    "This plot shows the most-frequent, non-trivial words in the selected textsâ€”bigger words = higher frequencyâ€”so you can spot dominant topics at a glance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85d19d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud Function\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def make_circular_mask(diam: int = 1600, border: int = 5) -> np.ndarray:\n",
    "    img = Image.new(\"L\", (diam, diam), 0)\n",
    "    ImageDraw.Draw(img).ellipse([(border, border), (diam - border, diam - border)], fill=255)\n",
    "    return 255 - np.array(img)  # WordCloud expects black = non-fillable\n",
    "\n",
    "\n",
    "def generate_wordcloud(\n",
    "    text_series,\n",
    "    stopwords_path=None,\n",
    "    title=\"Wordcloud\",\n",
    "    out_dir=OUTPUT_DIR,\n",
    "    categories=None\n",
    "):\n",
    "    print(\"\\nâœ” [OK] Building word-cloudâ€¦\")\n",
    "\n",
    "    # Stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    if stopwords_path and os.path.exists(stopwords_path):\n",
    "        with open(stopwords_path, 'r') as f:\n",
    "            stop_words.update(f.read().splitlines())\n",
    "\n",
    "    # Tokenize and filter\n",
    "    combined_text = ' '.join(text_series.dropna().astype(str))\n",
    "    tokens = word_tokenize(combined_text.lower())\n",
    "    filtered_tokens = [w for w in tokens if w.isalnum() and w not in stop_words and len(w) > 2]\n",
    "    paragraph_count = len(text_series)\n",
    "\n",
    "    # Mask\n",
    "    mask = make_circular_mask()\n",
    "    print(f\"âœ” [OK] Mask ready {mask.shape}\")\n",
    "\n",
    "    # Frequencies\n",
    "    word_freq = Counter(filtered_tokens)\n",
    "    print(f\"âœ” [OK] {len(word_freq):,} unique tokens\")\n",
    "\n",
    "    # Categories\n",
    "    if categories is None:\n",
    "        categories = {}\n",
    "    \n",
    "    word2cat = {w: cat for cat, info in categories.items() for w in info[\"words\"]}\n",
    "\n",
    "    # Color function\n",
    "    def colour_for_word(word, **_):\n",
    "        cat = word2cat.get(word.lower())\n",
    "        if cat:\n",
    "            r, g, b, _ = categories[cat][\"color\"]\n",
    "            return f\"#{int(r * 255):02x}{int(g * 255):02x}{int(b * 255):02x}\"\n",
    "        return \"#bcbcbc\"\n",
    "\n",
    "    # Generate WordCloud\n",
    "    wc = (WordCloud(\n",
    "        width=1600, height=1600, mask=mask, background_color=\"white\",\n",
    "        max_words=600, min_font_size=5, max_font_size=160, font_step=1,\n",
    "        margin=1, prefer_horizontal=0.3, random_state=42,\n",
    "        collocations=False, repeat=True, mode=\"RGBA\"\n",
    "    )\n",
    "        .generate_from_frequencies(word_freq)\n",
    "        .recolor(color_func=colour_for_word, random_state=42))\n",
    "\n",
    "    print(\"âœ” [OK] WordCloud generated\")\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 12), dpi=300)\n",
    "    fig.patch.set_facecolor(\"white\")\n",
    "    ax.imshow(wc.to_array(), interpolation=\"bilinear\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    fig.suptitle(title, fontsize=36, fontweight=\"bold\", y=1.05, color=\"black\")\n",
    "    fig.text(0.5, 0.975, f\"Analysis of {paragraph_count:,} Paragraphs of Text\",\n",
    "             ha=\"center\", va=\"top\", fontsize=18, style=\"italic\", color=\"#333333\")\n",
    "\n",
    "    handles = [Patch(color=info[\"color\"], label=cat) for cat, info in categories.items()]\n",
    "    legend = ax.legend(handles=handles,\n",
    "                       loc=\"lower center\", bbox_to_anchor=(0.5, -0.085),\n",
    "                       ncol=3, frameon=False, fontsize=14)\n",
    "    for txt in legend.get_texts():\n",
    "        txt.set_color(\"#333333\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "\n",
    "    # Save\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    base = \"wordcloud_latest\"\n",
    "\n",
    "    fig.savefig(os.path.join(out_dir, f\"{base}.png\"),\n",
    "                dpi=300, bbox_inches=\"tight\", format=\"png\")\n",
    "    print(f\"âœ” [OK] Saved {base}.png\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d705bf3",
   "metadata": {},
   "source": [
    "### 2. Word-based Heatmap \n",
    "\n",
    "This plot produces a word-by-speaker heatmapâ€”columns clustered by cosine similarity or co-occurrence, so you can quickly see which keywords co-occur across interviews and how they group into thematic clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45872ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_heatmap_pipeline(input_data):\n",
    "    \"\"\"\n",
    "    Main function to run the heatmap pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_data : run_heatmap_pipeline\n",
    "        Object containing all input parameters\n",
    "    \"\"\"\n",
    "    # Build reverse mapping for word families\n",
    "    word_to_base = {}\n",
    "    for base_word, variants in WORD_FAMILIES.items():\n",
    "        for variant in variants:\n",
    "            word_to_base[variant.lower()] = base_word\n",
    "\n",
    "    seed_groups, seed_words, use_group_label = {}, [], False\n",
    "\n",
    "    df = pd.read_csv(input_data.filepath)\n",
    "\n",
    "    # Normalize alternative column names if needed\n",
    "    if 'text' not in df.columns:\n",
    "        alternatives = [col for col in df.columns if 'text' in col.lower() or 'content' in col.lower() or 'body' in col.lower()]\n",
    "        if alternatives:\n",
    "            print(f\"'text' column not found, using '{alternatives[0]}' instead.\")\n",
    "            df.rename(columns={alternatives[0]: 'text'}, inplace=True)\n",
    "        else:\n",
    "            print(\"Error: No suitable text column found.\")\n",
    "            return\n",
    "    \n",
    "    # We'll use the stop list instead of hardcoding additional common words\n",
    "    additional_common_words = set()\n",
    "            \n",
    "    # Use the pre-loaded stopwords if available\n",
    "    if hasattr(input_data, 'custom_stopwords') and input_data.custom_stopwords:\n",
    "        stop_words = input_data.custom_stopwords\n",
    "    else:\n",
    "        # Load stop words the old way if not pre-loaded\n",
    "        stop_words = manage_stop_list(input_data.stop_list, default_stop_words)\n",
    "        # Add our additional common words to the stop list\n",
    "        stop_words = stop_words.union(additional_common_words)\n",
    "\n",
    "    \n",
    "    # Fix list columns that may be stored as strings\n",
    "    for col in ['data_group', 'codes']:\n",
    "        if col in df.columns:\n",
    "            # Check if first non-null value is a string that looks like a list\n",
    "            sample = df[col].dropna().iloc[0] if not df[col].dropna().empty else None\n",
    "            if isinstance(sample, str) and (sample.startswith('[') or ',' in sample):\n",
    "                df[col] = df[col].apply(lambda x: eval(x) if isinstance(x, str) and x.strip() else \n",
    "                                       ([] if pd.isna(x) else [x]))\n",
    "    \n",
    "    # Apply Metadata Filters\n",
    "    if input_data.projects and 'project' in df.columns:\n",
    "        before = len(df)\n",
    "        df = df[df['project'].isin(input_data.projects)]\n",
    "        print(f\"Project filter: {before} â†’ {len(df)} rows\")\n",
    "    \n",
    "    if input_data.data_groups and 'data_group' in df.columns:\n",
    "        before = len(df)\n",
    "        try:\n",
    "            # Handle both list and non-list data_group columns\n",
    "            if df['data_group'].apply(lambda x: isinstance(x, list)).any():\n",
    "                mask = df['data_group'].apply(lambda x: \n",
    "                    isinstance(x, list) and any(item in input_data.data_groups for item in x))\n",
    "            else:\n",
    "                mask = df['data_group'].isin(input_data.data_groups)\n",
    "            df = df[mask]\n",
    "            print(f\"Data group filter: {before} â†’ {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in data_group filtering: {e}\")\n",
    "            print(f\"Sample data_group values: {df['data_group'].head()}\")\n",
    "    \n",
    "    if input_data.codes and 'codes' in df.columns:\n",
    "        before = len(df)\n",
    "        try:\n",
    "            # Handle both list and non-list codes columns\n",
    "            if df['codes'].apply(lambda x: isinstance(x, list)).any():\n",
    "                mask = df['codes'].apply(lambda x: \n",
    "                    isinstance(x, list) and any(item in input_data.codes for item in x))\n",
    "            else:\n",
    "                mask = df['codes'].isin(input_data.codes)\n",
    "            df = df[mask]\n",
    "            print(f\"Codes filter: {before} â†’ {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in codes filtering: {e}\")\n",
    "            print(f\"Sample codes values: {df['codes'].head()}\")\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"Error: All rows were filtered out. Please check your filter criteria.\")\n",
    "        return\n",
    "    \n",
    "    sentences = df['text'].dropna().tolist()\n",
    "\n",
    "    # Add filtered sentences tracking here\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if not isinstance(sentence, str):\n",
    "            print(f\"Not a string: type={type(sentence)}, value={sentence}\")\n",
    "        tokens = tokenize_and_filter([sentence], stop_list=stop_words, \n",
    "                                   lemmatize=True, \n",
    "                                   cross_pos_normalize=input_data.cross_pos_normalize)\n",
    "        if tokens:  # Only keep sentences that have tokens after filtering\n",
    "            filtered_sentences.append(sentence)\n",
    "    print(f\"After filtering: {len(filtered_sentences)} valid text segments\")\n",
    "\n",
    "    # Filter out excluded codes if specified\n",
    "    if hasattr(input_data, 'excluded_codes') and input_data.excluded_codes and 'codes' in df.columns:\n",
    "        before = len(df)\n",
    "        try:\n",
    "            # Handle both list and non-list codes columns\n",
    "            if df['codes'].apply(lambda x: isinstance(x, list)).any():\n",
    "                # Keep rows where NONE of the excluded codes are present\n",
    "                mask = df['codes'].apply(lambda x: \n",
    "                    isinstance(x, list) and not any(item in input_data.excluded_codes for item in x))\n",
    "            else:\n",
    "                # Keep rows where the code is not in excluded_codes\n",
    "                mask = ~df['codes'].isin(input_data.excluded_codes)\n",
    "            df = df[mask]\n",
    "            print(f\"Excluded codes filter: {before} â†’ {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in excluded_codes filtering: {e}\")\n",
    "    \n",
    "        # Define a custom word filter function\n",
    "    def custom_word_filter(word):\n",
    "        # First normalize with word families\n",
    "        word_lower = word.lower()\n",
    "        if word_lower in word_to_base:\n",
    "            word = word_to_base[word_lower]\n",
    "        \n",
    "        # Manual exclusion of common words that should be filtered\n",
    "        manual_exclusions = {'got', 'get', 'just', 'like', 'many', 'much', 'very', 'really', 'make'}\n",
    "        \n",
    "        return (word.lower() not in stop_words and\n",
    "                word.lower() not in manual_exclusions and\n",
    "                len(word) > 2 and  # Exclude very short words\n",
    "                not any(c.isdigit() for c in word) and  # Exclude words with numbers\n",
    "                re.match(r'^[a-z]+$', word.lower()))  # Only pure alphabetic words\n",
    "                \n",
    "    # Check if seed words were provided in the input\n",
    "    if hasattr(input_data, 'seed_words') and input_data.seed_words and input_data.seed_words.strip().lower() != \"none\":\n",
    "        seed_input = input_data.seed_words.strip()\n",
    "\n",
    "        if \":\" in seed_input:\n",
    "            use_group_label = True\n",
    "            for part in seed_input.split(\";\"):\n",
    "                part = part.strip()\n",
    "                if \":\" in part:\n",
    "                    group_label, word_str = part.split(\":\", 1)\n",
    "                    group_label = group_label.strip().lower()\n",
    "                    words = [w.strip().lower() for w in word_str.split(\",\") if w.strip()]\n",
    "                    seed_groups[group_label] = set(words)\n",
    "                    seed_words.append(group_label)\n",
    "                    print(f\"Group mode: all {words} will be treated as '{group_label}'\")\n",
    "                else:\n",
    "                    individuals = [w.strip().lower() for w in part.split(\",\") if w.strip()]\n",
    "                    seed_words.extend(individuals)\n",
    "                    print(f\"Individual mode: adding {individuals}\")\n",
    "        else:\n",
    "            seed_words = [w.strip().lower() for w in seed_input.split(\",\") if w.strip()]\n",
    "            print(f\"Pure individual word mode: using {seed_words}\")\n",
    "        if use_group_label:\n",
    "                sentences = [replace_group_words(text, seed_groups) for text in sentences]\n",
    "    else:\n",
    "        # Process sentences to get word frequencies for auto-selection of top words\n",
    "        print(\"WARNING: No seed words provided or 'NONE' specified. Using top frequent words as seeds... \")\n",
    "        excluded_words = stop_words.union(set(WORD_FAMILIES.keys()))\n",
    "        all_tokens = []\n",
    "        for sentence in sentences:\n",
    "            tokens = tokenize_and_filter([sentence], stop_list=stop_words,\n",
    "                                        lemmatize=True,\n",
    "                                        cross_pos_normalize=input_data.cross_pos_normalize)\n",
    "            filtered_tokens = [token.lower() for token in tokens if custom_word_filter(token)]\n",
    "            all_tokens.extend(filtered_tokens)\n",
    "        word_counts = Counter(all_tokens)\n",
    "        top_words = [word for word, _ in word_counts.most_common(30)\n",
    "                    if word.lower() not in excluded_words][:min(10, len(word_counts))]\n",
    "        seed_words = top_words\n",
    "        print(f\"Top frequent words as seeds: {seed_words}\")\n",
    "       \n",
    "    start = time.time()\n",
    "\n",
    "    # Clean and normalize seed words, but they're already preprocessed with word families\n",
    "    print(f\"Original seed words before cleaning: {seed_words}\")\n",
    "    clean_seed_words = clean_words(seed_words)\n",
    "    print(f\"Seed words after cleaning: {clean_seed_words}\")\n",
    "    clean_seeds = clean_words(seed_words)\n",
    "    seed_words = normalize_words(clean_seeds, stop_words, lemmatize=True, cross_pos_normalize=input_data.cross_pos_normalize)\n",
    "    seed_words = list(set(seed_words))\n",
    "    print(\"Final normalized seed words:\", seed_words)\n",
    "\n",
    "\n",
    "    # choose context source: keep stop-words only for RoBERTa\n",
    "    if input_data.clustering_method == 1:          # 1 = RoBERTa\n",
    "        sentences_for_embedding = sentences        # full context\n",
    "    else:                                          # 2-4 = Jaccard/PMI/TF-IDF\n",
    "        sentences_for_embedding = filtered_sentences\n",
    "\n",
    "    word_embeddings, similarity_matrix, co_occurrence_matrix = train_embedding(\n",
    "        sentences_for_embedding,\n",
    "        context_window = input_data.window_size,\n",
    "        stop_list  = stop_words, \n",
    "        seed_words = seed_words, \n",
    "        clustering_method  = input_data.clustering_method,\n",
    "        num_words = input_data.num_words, \n",
    "        lemmatize = True, \n",
    "        min_word_frequency = input_data.min_word_frequency,\n",
    "        reuse_clusterings  = input_data.reuse_clusterings,\n",
    "        cross_pos_normalize= getattr(input_data, 'cross_pos_normalize', False),\n",
    "        distance_metric = getattr(input_data, 'distance_metric', 'default'),\n",
    "        custom_word_filter = custom_word_filter\n",
    "    )\n",
    "\n",
    "    elapsed_time = time.time() - start\n",
    "    print(f\"Embedding generation completed in {elapsed_time:.1f} seconds\")\n",
    "\n",
    "    if word_embeddings is None:\n",
    "        print(\"Error: Failed to generate embeddings. Please check your input data and parameters.\")\n",
    "        return\n",
    "    # Plot Similarity Heatmap\n",
    "    \n",
    "    print(\"Plotting similarity heatmap...\")\n",
    "    fig = plot_heatmap(\n",
    "        input_data.clustering_method, word_embeddings, \n",
    "        similarity_matrix, co_occurrence_matrix, input_data.distance_metric\n",
    "    )\n",
    "\n",
    "    filename = f\"ONLY_heatmap_{input_data.clustering_method}_{input_data.distance_metric}.png\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    out_path = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "    fig.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"âœ” [OK] Saved {out_path}\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    print(\"\\nAnalysis complete\")\n",
    "    print(f\"\\nNetwork visualization method: {input_data.clustering_method}\")\n",
    "    \n",
    "    if input_data.clustering_method == 1:\n",
    "        print(\"Method: RoBERTa â€“ Shows semantic relationships based on contextual embeddings\")\n",
    "    elif input_data.clustering_method == 2:\n",
    "        if input_data.distance_metric == \"cosine\":\n",
    "            print(\"Method: Jaccard (cosine) â€“ Uses context window vectors to compute cosine-based similarity between word usage patterns\")\n",
    "        elif input_data.distance_metric == \"default\":\n",
    "            print(\"Method: Jaccard (default) â€“ Uses binary co-occurrence counts within a context window to capture word overlap\")\n",
    "    elif input_data.clustering_method == 3:\n",
    "        print(\"Method: PMI â€“ Highlights statistically significant word associations based on pointwise mutual information\")\n",
    "    elif input_data.clustering_method == 4:\n",
    "        if input_data.distance_metric == \"cosine\":\n",
    "            print(\"Method: TF-IDF (cosine) â€“ Uses TF-IDF-weighted context vectors to compute cosine similarity between words\")\n",
    "        elif input_data.distance_metric == \"default\":\n",
    "            print(\"Method: TF-IDF (default) â€“ Uses raw TF-IDF-weighted co-occurrence scores for word associations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbe32fa",
   "metadata": {},
   "source": [
    "### 3. tSNE \n",
    "\n",
    "This function creates a 2-D t-SNE map of your vocabularyâ€”using either the similarity matrix (embeddings) or the co-occurrence matrixâ€”so that spatial distance â‰ˆ lexical / semantic closeness; seed words are drawn larger and in red to spotlight how neighbouring terms cluster around them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238caf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tsne_pipeline(input_data):\n",
    "    \"\"\"\n",
    "    Main function to run the semantic network analysis pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_data : run_heatmap_pipeline\n",
    "        Object containing all input parameters\n",
    "    \"\"\"\n",
    "    # Build reverse mapping for word families\n",
    "    word_to_base = {}\n",
    "    for base_word, variants in WORD_FAMILIES.items():\n",
    "        for variant in variants:\n",
    "            word_to_base[variant.lower()] = base_word\n",
    "\n",
    "    seed_groups, seed_words, use_group_label = {}, [], False\n",
    "    auto_selected_seeds = False  # Flag to track if seeds were auto-selected\n",
    "\n",
    "    df = pd.read_csv(input_data.filepath)\n",
    "    \n",
    "    # Normalize alternative column names if needed\n",
    "    if 'text' not in df.columns:\n",
    "        alternatives = [col for col in df.columns if 'text' in col.lower() or 'content' in col.lower() or 'body' in col.lower()]\n",
    "        if alternatives:\n",
    "            print(f\"'text' column not found, using '{alternatives[0]}' instead.\")\n",
    "            df.rename(columns={alternatives[0]: 'text'}, inplace=True)\n",
    "        else:\n",
    "            print(\"Error: No suitable text column found.\")\n",
    "            return\n",
    "    \n",
    "    # We'll use the stop list instead of hardcoding additional common words\n",
    "    additional_common_words = set()\n",
    "            \n",
    "    # Use the pre-loaded stopwords if available\n",
    "    if hasattr(input_data, 'custom_stopwords') and input_data.custom_stopwords:\n",
    "        stop_words = input_data.custom_stopwords\n",
    "    else:\n",
    "        # Load stop words the old way if not pre-loaded\n",
    "        stop_words = manage_stop_list(input_data.stop_list, default_stop_words)\n",
    "        # Add our additional common words to the stop list\n",
    "        stop_words = stop_words.union(additional_common_words)\n",
    "\n",
    "    \n",
    "    # Fix list columns that may be stored as strings\n",
    "    for col in ['data_group', 'codes']:\n",
    "        if col in df.columns:\n",
    "            # Check if first non-null value is a string that looks like a list\n",
    "            sample = df[col].dropna().iloc[0] if not df[col].dropna().empty else None\n",
    "            if isinstance(sample, str) and (sample.startswith('[') or ',' in sample):\n",
    "                df[col] = df[col].apply(lambda x: eval(x) if isinstance(x, str) and x.strip() else \n",
    "                                       ([] if pd.isna(x) else [x]))\n",
    "    \n",
    "    # Apply Metadata Filters\n",
    "    if input_data.projects and 'project' in df.columns:\n",
    "        before = len(df)\n",
    "        df = df[df['project'].isin(input_data.projects)]\n",
    "        print(f\"Project filter: {before} â†’ {len(df)} rows\")\n",
    "    \n",
    "    if input_data.data_groups and 'data_group' in df.columns:\n",
    "        before = len(df)\n",
    "        try:\n",
    "            # Handle both list and non-list data_group columns\n",
    "            if df['data_group'].apply(lambda x: isinstance(x, list)).any():\n",
    "                mask = df['data_group'].apply(lambda x: \n",
    "                    isinstance(x, list) and any(item in input_data.data_groups for item in x))\n",
    "            else:\n",
    "                mask = df['data_group'].isin(input_data.data_groups)\n",
    "            df = df[mask]\n",
    "            print(f\"Data group filter: {before} â†’ {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in data_group filtering: {e}\")\n",
    "            print(f\"Sample data_group values: {df['data_group'].head()}\")\n",
    "    \n",
    "    if input_data.codes and 'codes' in df.columns:\n",
    "        before = len(df)\n",
    "        try:\n",
    "            # Handle both list and non-list codes columns\n",
    "            if df['codes'].apply(lambda x: isinstance(x, list)).any():\n",
    "                mask = df['codes'].apply(lambda x: \n",
    "                    isinstance(x, list) and any(item in input_data.codes for item in x))\n",
    "            else:\n",
    "                mask = df['codes'].isin(input_data.codes)\n",
    "            df = df[mask]\n",
    "            print(f\"Codes filter: {before} â†’ {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in codes filtering: {e}\")\n",
    "            print(f\"Sample codes values: {df['codes'].head()}\")\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"Error: All rows were filtered out. Please check your filter criteria.\")\n",
    "        return\n",
    "    \n",
    "    sentences = df['text'].dropna().tolist()\n",
    "    print(f\"Final dataset: {len(sentences)} text segments ready for processing\")\n",
    "\n",
    "    # Add filtered sentences tracking here\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if not isinstance(sentence, str):\n",
    "            print(f\"Not a string: type={type(sentence)}, value={sentence}\")\n",
    "        tokens = tokenize_and_filter([sentence], stop_list=stop_words, \n",
    "                                   lemmatize=True, \n",
    "                                   cross_pos_normalize=input_data.cross_pos_normalize)\n",
    "        if tokens:  # Only keep sentences that have tokens after filtering\n",
    "            filtered_sentences.append(sentence)\n",
    "    print(f\"After filtering: {len(filtered_sentences)} valid text segments\")\n",
    "    \n",
    "\n",
    "    # Filter out excluded codes if specified\n",
    "    if hasattr(input_data, 'excluded_codes') and input_data.excluded_codes and 'codes' in df.columns:\n",
    "        before = len(df)\n",
    "        try:\n",
    "            # Handle both list and non-list codes columns\n",
    "            if df['codes'].apply(lambda x: isinstance(x, list)).any():\n",
    "                # Keep rows where NONE of the excluded codes are present\n",
    "                mask = df['codes'].apply(lambda x: \n",
    "                    isinstance(x, list) and not any(item in input_data.excluded_codes for item in x))\n",
    "            else:\n",
    "                # Keep rows where the code is not in excluded_codes\n",
    "                mask = ~df['codes'].isin(input_data.excluded_codes)\n",
    "            df = df[mask]\n",
    "            print(f\"Excluded codes filter: {before} â†’ {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in excluded_codes filtering: {e}\")\n",
    "    \n",
    "        # Define a custom word filter function\n",
    "    def custom_word_filter(word):\n",
    "        # First normalize with word families\n",
    "        word_lower = word.lower()\n",
    "        if word_lower in word_to_base:\n",
    "            word = word_to_base[word_lower]\n",
    "        \n",
    "        # Manual exclusion of common words that should be filtered\n",
    "        manual_exclusions = {'got', 'get', 'just', 'like', 'many', 'much', 'very', 'really', 'make'}\n",
    "        \n",
    "        return (word.lower() not in stop_words and\n",
    "                word.lower() not in manual_exclusions and\n",
    "                len(word) > 2 and  # Exclude very short words\n",
    "                not any(c.isdigit() for c in word) and  # Exclude words with numbers\n",
    "                re.match(r'^[a-z]+$', word.lower()))  # Only pure alphabetic words\n",
    "                \n",
    "    # Check if seed words were provided in the input\n",
    "    if hasattr(input_data, 'seed_words') and input_data.seed_words and input_data.seed_words.strip().lower() != \"none\":\n",
    "        seed_input = input_data.seed_words.strip()\n",
    "\n",
    "        if \":\" in seed_input:\n",
    "            use_group_label = True\n",
    "            for part in seed_input.split(\";\"):\n",
    "                part = part.strip()\n",
    "                if \":\" in part:\n",
    "                    group_label, word_str = part.split(\":\", 1)\n",
    "                    group_label = group_label.strip().lower()\n",
    "                    words = [w.strip().lower() for w in word_str.split(\",\") if w.strip()]\n",
    "                    seed_groups[group_label] = set(words)\n",
    "                    seed_words.append(group_label)\n",
    "                    print(f\"Group mode: all {words} will be treated as '{group_label}'\")\n",
    "                else:\n",
    "                    individuals = [w.strip().lower() for w in part.split(\",\") if w.strip()]\n",
    "                    seed_words.extend(individuals)\n",
    "                    print(f\"Individual mode: adding {individuals}\")\n",
    "        else:\n",
    "            seed_words = [w.strip().lower() for w in seed_input.split(\",\") if w.strip()]\n",
    "            print(f\"Pure individual word mode: using {seed_words}\")\n",
    "        if use_group_label:\n",
    "                sentences = [replace_group_words(text, seed_groups) for text in sentences]\n",
    "    else:\n",
    "        # Process sentences to get word frequencies for auto-selection of top words\n",
    "        print(\"WARNING: No seed words provided or 'NONE' specified. Using top frequent words as seeds... \")\n",
    "        excluded_words = stop_words.union(set(WORD_FAMILIES.keys()))\n",
    "        all_tokens = []\n",
    "        for sentence in sentences:\n",
    "            tokens = tokenize_and_filter([sentence], stop_list=stop_words,\n",
    "                                        lemmatize=True,\n",
    "                                        cross_pos_normalize=input_data.cross_pos_normalize)\n",
    "            filtered_tokens = [token.lower() for token in tokens if custom_word_filter(token)]\n",
    "            all_tokens.extend(filtered_tokens)\n",
    "        word_counts = Counter(all_tokens)\n",
    "        top_words = [word for word, _ in word_counts.most_common(30)\n",
    "                    if word.lower() not in excluded_words][:min(10, len(word_counts))]\n",
    "        seed_words = top_words\n",
    "        print(f\"Top frequent words as seeds: {seed_words}\")\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Clean and normalize seed words, but they're already preprocessed with word families\n",
    "    print(f\"Original seed words before cleaning: {seed_words}\")\n",
    "    clean_seed_words = clean_words(seed_words)\n",
    "    print(f\"Seed words after cleaning: {clean_seed_words}\")\n",
    "    clean_seeds = clean_words(seed_words)\n",
    "    seed_words = normalize_words(clean_seeds, stop_words, lemmatize=True, cross_pos_normalize=input_data.cross_pos_normalize)\n",
    "    seed_words = list(set(seed_words))\n",
    "    print(\"Final normalized seed words:\", seed_words)\n",
    "\n",
    "    # choose context source: keep stop-words only for RoBERTa\n",
    "    if input_data.clustering_method == 1:          # 1 = RoBERTa\n",
    "        sentences_for_embedding = sentences        # full context\n",
    "    else:                                          # 2-4 = Jaccard/PMI/TF-IDF\n",
    "        sentences_for_embedding = filtered_sentences\n",
    "\n",
    "    word_embeddings, similarity_matrix, co_occurrence_matrix = train_embedding(\n",
    "        sentences_for_embedding,\n",
    "        context_window = input_data.window_size,\n",
    "        stop_list  = stop_words, \n",
    "        seed_words = seed_words, \n",
    "        clustering_method  = input_data.clustering_method,\n",
    "        num_words = input_data.num_words, \n",
    "        lemmatize = True, \n",
    "        min_word_frequency = input_data.min_word_frequency,\n",
    "        reuse_clusterings  = input_data.reuse_clusterings,\n",
    "        cross_pos_normalize= getattr(input_data, 'cross_pos_normalize', False),\n",
    "        distance_metric = getattr(input_data, 'distance_metric', 'default'),\n",
    "        custom_word_filter = custom_word_filter\n",
    "    )\n",
    "\n",
    "    elapsed_time = time.time() - start\n",
    "    print(f\"Embedding generation completed in {elapsed_time:.1f} seconds\")\n",
    "\n",
    "    if word_embeddings is None:\n",
    "        print(\"Error: Failed to generate embeddings. Please check your input data and parameters.\")\n",
    "        return\n",
    "\n",
    "    # Generate t-SNE Dimensional Reduction Plot\n",
    "    print(\"-----------------------------------------------\")\n",
    "    print(\"Generating t-SNE dimensional reduction plot...\")\n",
    "    try:\n",
    "        plot_tsne_dimensional_reduction(\n",
    "            word_embeddings=word_embeddings,\n",
    "            similarity_matrix=similarity_matrix,\n",
    "            co_occurrence_matrix=co_occurrence_matrix,\n",
    "            clustering_method=input_data.clustering_method,\n",
    "            seed_words=seed_words, \n",
    "            distance_metric=getattr(input_data, 'distance_metric', 'default')\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"t-SNE plot error: {e}\")\n",
    "    \n",
    "    print(\"\\nAnalysis complete\")\n",
    "    print(f\"\\nNetwork visualization method: {input_data.clustering_method}\")\n",
    "    \n",
    "    if input_data.clustering_method == 1:\n",
    "        print(\"Method: RoBERTa â€“ Shows semantic relationships based on contextual embeddings\")\n",
    "    elif input_data.clustering_method == 2:\n",
    "        if input_data.distance_metric == \"cosine\":\n",
    "            print(\"Method: Jaccard (cosine) â€“ Uses context window vectors to compute cosine-based similarity between word usage patterns\")\n",
    "        elif input_data.distance_metric == \"default\":\n",
    "            print(\"Method: Jaccard (default) â€“ Uses binary co-occurrence counts within a context window to capture word overlap\")\n",
    "    elif input_data.clustering_method == 3:\n",
    "        print(\"Method: PMI â€“ Highlights statistically significant word associations based on pointwise mutual information\")\n",
    "    elif input_data.clustering_method == 4:\n",
    "        if input_data.distance_metric == \"cosine\":\n",
    "            print(\"Method: TF-IDF (cosine) â€“ Uses TF-IDF-weighted context vectors to compute cosine similarity between words\")\n",
    "        elif input_data.distance_metric == \"default\":\n",
    "            print(\"Method: TF-IDF (default) â€“ Uses raw TF-IDF-weighted co-occurrence scores for word associations\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e11b66",
   "metadata": {},
   "source": [
    "### 4. Word-based Heatmp and Semantic Network (no custom colors, no edge bundling)\n",
    "\n",
    "This function generates a frequency-by-speaker heat-map plus a plain semantic-network graphâ€”default blue nodes, gray edges, no category colours and no edge-bundlingâ€”so you get a quick, unbiased view of word overlap and overall connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551315ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Run the Pipeline\n",
    "def run_heatmap_network_plain_pipeline(input_data):\n",
    "    \"\"\"\n",
    "    Main function to run the semantic network analysis pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_data : VisualsInput\n",
    "        Object containing all input parameters\n",
    "    \"\"\"\n",
    "    # Build reverse mapping for word families\n",
    "    word_to_base = {}\n",
    "    for base_word, variants in WORD_FAMILIES.items():\n",
    "        for variant in variants:\n",
    "            word_to_base[variant.lower()] = base_word\n",
    "\n",
    "    seed_groups, seed_words, use_group_label = {}, [], False\n",
    "    auto_selected_seeds = False  # Flag to track if seeds were auto-selected\n",
    "\n",
    "    df = pd.read_csv(input_data.filepath)\n",
    "  \n",
    "    # Normalize alternative column names if needed\n",
    "    if 'text' not in df.columns:\n",
    "        alternatives = [col for col in df.columns if 'text' in col.lower() or 'content' in col.lower() or 'body' in col.lower()]\n",
    "        if alternatives:\n",
    "            print(f\"'text' column not found, using '{alternatives[0]}' instead.\")\n",
    "            df.rename(columns={alternatives[0]: 'text'}, inplace=True)\n",
    "        else:\n",
    "            print(\"Error: No suitable text column found.\")\n",
    "            return\n",
    "    \n",
    "    # We'll use the stop list instead of hardcoding additional common words\n",
    "    additional_common_words = set()\n",
    "            \n",
    "    # Use the pre-loaded stopwords if available\n",
    "    if hasattr(input_data, 'custom_stopwords') and input_data.custom_stopwords:\n",
    "        stop_words = input_data.custom_stopwords\n",
    "    else:\n",
    "        # Load stop words the old way if not pre-loaded\n",
    "        stop_words = manage_stop_list(input_data.stop_list, default_stop_words)\n",
    "        # Add our additional common words to the stop list\n",
    "        stop_words = stop_words.union(additional_common_words)\n",
    "\n",
    "    \n",
    "    # Fix list columns that may be stored as strings\n",
    "    for col in ['data_group', 'codes']:\n",
    "        if col in df.columns:\n",
    "            # Check if first non-null value is a string that looks like a list\n",
    "            sample = df[col].dropna().iloc[0] if not df[col].dropna().empty else None\n",
    "            if isinstance(sample, str) and (sample.startswith('[') or ',' in sample):\n",
    "                df[col] = df[col].apply(lambda x: eval(x) if isinstance(x, str) and x.strip() else \n",
    "                                       ([] if pd.isna(x) else [x]))\n",
    "    \n",
    "    # Apply Metadata Filters\n",
    "    if input_data.projects and 'project' in df.columns:\n",
    "        before = len(df)\n",
    "        df = df[df['project'].isin(input_data.projects)]\n",
    "        print(f\"Project filter: {before} â†’ {len(df)} rows\")\n",
    "    \n",
    "    if input_data.data_groups and 'data_group' in df.columns:\n",
    "        before = len(df)\n",
    "        try:\n",
    "            # Handle both list and non-list data_group columns\n",
    "            if df['data_group'].apply(lambda x: isinstance(x, list)).any():\n",
    "                mask = df['data_group'].apply(lambda x: \n",
    "                    isinstance(x, list) and any(item in input_data.data_groups for item in x))\n",
    "            else:\n",
    "                mask = df['data_group'].isin(input_data.data_groups)\n",
    "            df = df[mask]\n",
    "            print(f\"Data group filter: {before} â†’ {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in data_group filtering: {e}\")\n",
    "            print(f\"Sample data_group values: {df['data_group'].head()}\")\n",
    "    \n",
    "    if input_data.codes and 'codes' in df.columns:\n",
    "        before = len(df)\n",
    "        try:\n",
    "            # Handle both list and non-list codes columns\n",
    "            if df['codes'].apply(lambda x: isinstance(x, list)).any():\n",
    "                mask = df['codes'].apply(lambda x: \n",
    "                    isinstance(x, list) and any(item in input_data.codes for item in x))\n",
    "            else:\n",
    "                mask = df['codes'].isin(input_data.codes)\n",
    "            df = df[mask]\n",
    "            print(f\"Codes filter: {before} â†’ {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in codes filtering: {e}\")\n",
    "            print(f\"Sample codes values: {df['codes'].head()}\")\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"Error: All rows were filtered out. Please check your filter criteria.\")\n",
    "        return\n",
    "    \n",
    "    sentences = df['text'].dropna().tolist()\n",
    "    print(f\"Final dataset: {len(sentences)} text segments ready for processing\")\n",
    "\n",
    "    # Add filtered sentences tracking here\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if not isinstance(sentence, str):\n",
    "            print(f\"Not a string: type={type(sentence)}, value={sentence}\")\n",
    "        tokens = tokenize_and_filter([sentence],stop_list=stop_words, \n",
    "                                   lemmatize=True, \n",
    "                                   cross_pos_normalize=input_data.cross_pos_normalize)\n",
    "        if tokens:  # Only keep sentences that have tokens after filtering\n",
    "            filtered_sentences.append(sentence)\n",
    "    print(f\"After filtering: {len(filtered_sentences)} valid text segments\")\n",
    "    \n",
    "\n",
    "    # Filter out excluded codes if specified\n",
    "    if hasattr(input_data, 'excluded_codes') and input_data.excluded_codes and 'codes' in df.columns:\n",
    "        before = len(df)\n",
    "        try:\n",
    "            # Handle both list and non-list codes columns\n",
    "            if df['codes'].apply(lambda x: isinstance(x, list)).any():\n",
    "                # Keep rows where NONE of the excluded codes are present\n",
    "                mask = df['codes'].apply(lambda x: \n",
    "                    isinstance(x, list) and not any(item in input_data.excluded_codes for item in x))\n",
    "            else:\n",
    "                # Keep rows where the code is not in excluded_codes\n",
    "                mask = ~df['codes'].isin(input_data.excluded_codes)\n",
    "            df = df[mask]\n",
    "            print(f\"Excluded codes filter: {before} â†’ {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in excluded_codes filtering: {e}\")\n",
    "    \n",
    "        # Define a custom word filter function\n",
    "    def custom_word_filter(word):\n",
    "        # First normalize with word families\n",
    "        word_lower = word.lower()\n",
    "        if word_lower in word_to_base:\n",
    "            word = word_to_base[word_lower]\n",
    "        \n",
    "        # Manual exclusion of common words that should be filtered\n",
    "        manual_exclusions = {'got', 'get', 'just', 'like', 'many', 'much', 'very', 'really', 'make'}\n",
    "        \n",
    "        return (word.lower() not in stop_words and\n",
    "                word.lower() not in manual_exclusions and\n",
    "                len(word) > 2 and  # Exclude very short words\n",
    "                not any(c.isdigit() for c in word) and  # Exclude words with numbers\n",
    "                re.match(r'^[a-z]+$', word.lower()))  # Only pure alphabetic words\n",
    "                \n",
    "    # Check if seed words were provided in the input\n",
    "    if hasattr(input_data, 'seed_words') and input_data.seed_words and input_data.seed_words.strip().lower() != \"none\":\n",
    "        seed_input = input_data.seed_words.strip()\n",
    "\n",
    "        if \":\" in seed_input:\n",
    "            use_group_label = True\n",
    "            for part in seed_input.split(\";\"):\n",
    "                part = part.strip()\n",
    "                if \":\" in part:\n",
    "                    group_label, word_str = part.split(\":\", 1)\n",
    "                    group_label = group_label.strip().lower()\n",
    "                    words = [w.strip().lower() for w in word_str.split(\",\") if w.strip()]\n",
    "                    seed_groups[group_label] = set(words)\n",
    "                    seed_words.append(group_label)\n",
    "                    print(f\"Group mode: all {words} will be treated as '{group_label}'\")\n",
    "                else:\n",
    "                    individuals = [w.strip().lower() for w in part.split(\",\") if w.strip()]\n",
    "                    seed_words.extend(individuals)\n",
    "                    print(f\"Individual mode: adding {individuals}\")\n",
    "        else:\n",
    "            seed_words = [w.strip().lower() for w in seed_input.split(\",\") if w.strip()]\n",
    "            print(f\"Pure individual word mode: using {seed_words}\")\n",
    "        if use_group_label:\n",
    "                sentences = [replace_group_words(text, seed_groups) for text in sentences]\n",
    "    else:\n",
    "        # Process sentences to get word frequencies for auto-selection of top words\n",
    "        print(\"WARNING: No seed words provided or 'NONE' specified. Using top frequent words as seeds... \")\n",
    "        excluded_words = stop_words.union(set(WORD_FAMILIES.keys()))\n",
    "        all_tokens = []\n",
    "        for sentence in sentences:\n",
    "            tokens = tokenize_and_filter([sentence], stop_list=stop_words,\n",
    "                                        lemmatize=True,\n",
    "                                        cross_pos_normalize=input_data.cross_pos_normalize)\n",
    "            filtered_tokens = [token.lower() for token in tokens if custom_word_filter(token)]\n",
    "            all_tokens.extend(filtered_tokens)\n",
    "        word_counts = Counter(all_tokens)\n",
    "        top_words = [word for word, _ in word_counts.most_common(30)\n",
    "                    if word.lower() not in excluded_words][:min(10, len(word_counts))]\n",
    "        seed_words = top_words\n",
    "        print(f\"Top frequent words as seeds: {seed_words}\")\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    # Clean and normalize seed words, but they're already preprocessed with word families\n",
    "    print(f\"Original seed words before cleaning: {seed_words}\")\n",
    "    clean_seed_words = clean_words(seed_words)\n",
    "    print(f\"Seed words after cleaning: {clean_seed_words}\")\n",
    "    clean_seeds = clean_words(seed_words)\n",
    "    seed_words = normalize_words(clean_seeds, stop_words, lemmatize=True, cross_pos_normalize=input_data.cross_pos_normalize)\n",
    "    seed_words = list(set(seed_words))\n",
    "    print(\"Final normalized seed words:\", seed_words)\n",
    "\n",
    "\n",
    "    # choose context source: keep stop-words only for RoBERTa\n",
    "    if input_data.clustering_method == 1:          # 1 = RoBERTa\n",
    "        sentences_for_embedding = sentences        # full context\n",
    "    else:                                          # 2-4 = Jaccard/PMI/TF-IDF\n",
    "        sentences_for_embedding = filtered_sentences\n",
    "\n",
    "    word_embeddings, similarity_matrix, co_occurrence_matrix = train_embedding(\n",
    "        sentences_for_embedding,\n",
    "        context_window = input_data.window_size, \n",
    "        stop_list  = stop_words, \n",
    "        seed_words = seed_words, \n",
    "        clustering_method  = input_data.clustering_method,\n",
    "        num_words = input_data.num_words, \n",
    "        lemmatize = True, \n",
    "        min_word_frequency = input_data.min_word_frequency,\n",
    "        reuse_clusterings  = input_data.reuse_clusterings,\n",
    "        cross_pos_normalize= getattr(input_data, 'cross_pos_normalize', False),\n",
    "        distance_metric = getattr(input_data, 'distance_metric', 'default'),\n",
    "        custom_word_filter = custom_word_filter\n",
    "    )\n",
    "\n",
    "    elapsed_time = time.time() - start\n",
    "    print(f\"Embedding generation completed in {elapsed_time:.1f} seconds\")\n",
    "\n",
    "    if word_embeddings is None:\n",
    "        print(\"Error: Failed to generate embeddings. Please check your input data and parameters.\")\n",
    "        return\n",
    "    # Plot Similarity Heatmap\n",
    "    \n",
    "    print(\"Plotting similarity heatmap...\")\n",
    "    fig_heat = plot_heatmap(\n",
    "        input_data.clustering_method, word_embeddings, \n",
    "        similarity_matrix, co_occurrence_matrix, input_data.distance_metric\n",
    "    )\n",
    "\n",
    "    filename = f\"heatmap_plain_m{input_data.clustering_method}_{input_data.distance_metric}.png\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    out_path = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "    fig_heat.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"âœ” [OK] Saved {out_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(\"-----------------------------------------------\")\n",
    "    print(\"Plotting semantic network (plain, no categories)...\")\n",
    "    fig_sn1 = plot_semantic_network(\n",
    "        word_embeddings,\n",
    "        [] if auto_selected_seeds else seed_words,\n",
    "        input_data.clustering_method,\n",
    "        similarity_matrix, co_occurrence_matrix,\n",
    "        semantic_categories=None,               \n",
    "        link_threshold = input_data.link_threshold,\n",
    "        link_color_threshold= input_data.link_color_threshold,\n",
    "        distance_metric=getattr(input_data, 'distance_metric', 'default'))\n",
    "    filename = f\"semantic_network_plain_m{input_data.clustering_method}_{input_data.distance_metric}.png\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    out_path = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "    fig_sn1.suptitle(\"Semantic Network (Plain)\", fontsize=30, y=0.98, fontweight='bold')\n",
    "    fig_sn1.subplots_adjust(top=0.95)\n",
    "\n",
    "    fig_sn1.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"âœ” [OK] Saved {out_path}\")\n",
    "\n",
    "    plt.show()\n",
    "        # Add a second network visualization that removes seed nodes\n",
    "    print(\"\\nGenerating secondary network with seed nodes hidden...\")\n",
    "    \n",
    "    # Filter out seed words from word embeddings and matrices\n",
    "    non_seed_words = [word for word in word_embeddings.keys() if word not in seed_words]\n",
    "    \n",
    "    if len(non_seed_words) > 5:  # Only proceed if we have enough nodes to make a meaningful network\n",
    "        non_seed_embeddings = {word: word_embeddings[word] for word in non_seed_words}\n",
    "        \n",
    "        # Create filtered similarity/co-occurrence matrices\n",
    "        if similarity_matrix is not None:\n",
    "            words = list(word_embeddings.keys())\n",
    "            indices = [words.index(word) for word in non_seed_words]\n",
    "            filtered_similarity = similarity_matrix[np.ix_(indices, indices)]\n",
    "        else:\n",
    "            filtered_similarity = None\n",
    "            \n",
    "        if co_occurrence_matrix is not None:\n",
    "            words = list(word_embeddings.keys())\n",
    "            indices = [words.index(word) for word in non_seed_words]\n",
    "            filtered_co_occurrence = co_occurrence_matrix[np.ix_(indices, indices)]\n",
    "        else:\n",
    "            filtered_co_occurrence = None\n",
    "        \n",
    "        # Plot the filtered network\n",
    "        print(f\"Plotting secondary network with {len(non_seed_words)} nodes (seeds hidden)...\")\n",
    "        fig_sn2 = plot_semantic_network(\n",
    "            non_seed_embeddings, [], \n",
    "            input_data.clustering_method, \n",
    "            filtered_similarity, filtered_co_occurrence, \n",
    "            semantic_categories=None,\n",
    "            link_threshold=input_data.link_threshold,\n",
    "            link_color_threshold=input_data.link_color_threshold,\n",
    "            distance_metric=getattr(input_data, 'distance_metric', 'default')\n",
    "        )\n",
    "        filename = f\"semantic_network_noseeds_m{input_data.clustering_method}_{input_data.distance_metric}.png\"\n",
    "        out_path = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "        fig_sn2.suptitle(\"Semantic Network (Seeds Hidden)\", fontsize=30, y=0.98, fontweight='bold')\n",
    "        fig_sn2.subplots_adjust(top=0.95)\n",
    "\n",
    "        fig_sn2.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"âœ” [OK] Saved {out_path}\")\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "   \n",
    "    # If custom coloring was used in the first visualization, also do it for the second\n",
    "    if hasattr(input_data, 'custom_colors') and input_data.custom_colors and hasattr(input_data, 'semantic_categories'):\n",
    "        semantic_categories = input_data.semantic_categories\n",
    "    else:\n",
    "        print(\"Not enough non-seed nodes to generate a meaningful secondary network.\")\n",
    "    \n",
    "    # Print the list of actual nodes used in the network (excluding seeds)\n",
    "    print(\"\\nFinal nodes used in network (excluding seeds):\")\n",
    "    if 'non_seed_words' in locals() and len(non_seed_words) > 0:\n",
    "        # Get frequencies for each node and sort by frequency (highest first)\n",
    "        print(f\"Total non-seed words: {len(non_seed_words)}\")\n",
    "        \n",
    "        # Check if word_frequencies is defined, otherwise use the word_counts from earlier\n",
    "        if 'word_frequencies' not in locals() and 'word_counts' in locals():\n",
    "            word_frequencies = word_counts\n",
    "        elif 'word_frequencies' not in locals():\n",
    "            print(\"Warning: Word frequency information not available\")\n",
    "            # Just print the words without frequencies\n",
    "            for word in sorted(non_seed_words):\n",
    "                print(f\"- {word}\")\n",
    "        else:\n",
    "            # Create list of (word, frequency) pairs and sort by frequency\n",
    "            freq_sorted_words = [(word, word_frequencies[word]) for word in non_seed_words if word in word_frequencies]\n",
    "            freq_sorted_words.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            for word, freq in freq_sorted_words:\n",
    "                print(f\"- {word}: {freq}\")\n",
    "    else:\n",
    "        print(\"No non-seed nodes were used in the network.uelse\")\n",
    "    \n",
    "    # Print the number of filtered sentences used\n",
    "    if 'filtered_sentences' in locals():\n",
    "        print(f\"\\nTotal number of filtered sentences used: {len(filtered_sentences)}\")\n",
    "        if 'seed_words' in locals() and seed_words:\n",
    "            # Enhanced seed word detection using word families\n",
    "            seed_containing_sentences = 0\n",
    "            for sentence in filtered_sentences:\n",
    "                sentence_lower = sentence.lower()\n",
    "                contains_seed = False\n",
    "                for seed in seed_words:\n",
    "                    # Check direct match\n",
    "                    if seed.lower() in sentence_lower:\n",
    "                        contains_seed = True\n",
    "                        break\n",
    "                    # Check word family variants\n",
    "                    for family, variants in WORD_FAMILIES.items():\n",
    "                        if seed.lower() == family.lower() or seed.lower() in [v.lower() for v in variants]:\n",
    "                            if any(variant.lower() in sentence_lower for variant in variants):\n",
    "                                contains_seed = True\n",
    "                                break\n",
    "                    if contains_seed:\n",
    "                        break\n",
    "                if contains_seed:\n",
    "                    seed_containing_sentences += 1\n",
    "            print(f\"Number of filtered sentences containing seed words: {seed_containing_sentences}\")\n",
    "    \n",
    "    print(\"\\nAnalysis complete\")\n",
    "    print(f\"\\nNetwork visualization method: {input_data.clustering_method}\")\n",
    "    \n",
    "    if input_data.clustering_method == 1:\n",
    "        print(\"Method: RoBERTa â€“ Shows semantic relationships based on contextual embeddings\")\n",
    "    elif input_data.clustering_method == 2:\n",
    "        if input_data.distance_metric == \"cosine\":\n",
    "            print(\"Method: Jaccard (cosine) â€“ Uses context window vectors to compute cosine-based similarity between word usage patterns\")\n",
    "        elif input_data.distance_metric == \"default\":\n",
    "            print(\"Method: Jaccard (default) â€“ Uses binary co-occurrence counts within a context window to capture word overlap\")\n",
    "    elif input_data.clustering_method == 3:\n",
    "        print(\"Method: PMI â€“ Highlights statistically significant word associations based on pointwise mutual information\")\n",
    "    elif input_data.clustering_method == 4:\n",
    "        if input_data.distance_metric == \"cosine\":\n",
    "            print(\"Method: TF-IDF (cosine) â€“ Uses TF-IDF-weighted context vectors to compute cosine similarity between words\")\n",
    "        elif input_data.distance_metric == \"default\":\n",
    "            print(\"Method: TF-IDF (default) â€“ Uses raw TF-IDF-weighted co-occurrence scores for word associations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a877f58",
   "metadata": {},
   "source": [
    "### 5. Word-based Heatmp and Semantic Network (custom colors, no edge bundling)\n",
    "\n",
    "This function generates a per-speaker word-frequency heat-map + a semantic-network graph that uses your custom color palette for node groups (so themes pop out) but keeps simple straight/gray edgesâ€”no bundlingâ€”giving a quick colored overview of concept clusters without extra visual wiring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd6f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to Run the Pipeline\n",
    "def run_heatmap_network_pipeline(input_data):\n",
    "    \"\"\"\n",
    "    Main function to run the semantic network analysis pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_data : SemanticNetworkInput\n",
    "        Object containing all input parameters\n",
    "    \"\"\"\n",
    "    # Build reverse mapping for word families\n",
    "    word_to_base = {}\n",
    "    for base_word, variants in WORD_FAMILIES.items():\n",
    "        for variant in variants:\n",
    "            word_to_base[variant.lower()] = base_word\n",
    "\n",
    "    seed_groups, seed_words, use_group_label = {}, [], False\n",
    "    auto_selected_seeds = False  # Flag to track if seeds were auto-selected\n",
    "\n",
    "    df = pd.read_csv(input_data.filepath)\n",
    "    \n",
    "    original_row_count = len(df)\n",
    "    \n",
    "    # Normalize alternative column names if needed\n",
    "    if 'text' not in df.columns:\n",
    "        alternatives = [col for col in df.columns if 'text' in col.lower() or 'content' in col.lower() or 'body' in col.lower()]\n",
    "        if alternatives:\n",
    "            print(f\"'text' column not found, using '{alternatives[0]}' instead.\")\n",
    "            df.rename(columns={alternatives[0]: 'text'}, inplace=True)\n",
    "        else:\n",
    "            print(\"Error: No suitable text column found.\")\n",
    "            return\n",
    "    \n",
    "    # We'll use the stop list instead of hardcoding additional common words\n",
    "    additional_common_words = set()\n",
    "            \n",
    "    # Use the pre-loaded stopwords if available\n",
    "    if hasattr(input_data, 'custom_stopwords') and input_data.custom_stopwords:\n",
    "        stop_words = input_data.custom_stopwords\n",
    "    else:\n",
    "        # Load stop words the old way if not pre-loaded\n",
    "        stop_words = manage_stop_list(input_data.stop_list, default_stop_words)\n",
    "        # Add our additional common words to the stop list\n",
    "        stop_words = stop_words.union(additional_common_words)\n",
    "    \n",
    "    # Fix list columns that may be stored as strings\n",
    "    for col in ['data_group', 'codes']:\n",
    "        if col in df.columns:\n",
    "            # Check if first non-null value is a string that looks like a list\n",
    "            sample = df[col].dropna().iloc[0] if not df[col].dropna().empty else None\n",
    "            if isinstance(sample, str) and (sample.startswith('[') or ',' in sample):\n",
    "                df[col] = df[col].apply(lambda x: eval(x) if isinstance(x, str) and x.strip() else \n",
    "                                       ([] if pd.isna(x) else [x]))\n",
    "    \n",
    "    # Apply Metadata Filters\n",
    "    if input_data.projects and 'project' in df.columns:\n",
    "        before = len(df)\n",
    "        df = df[df['project'].isin(input_data.projects)]\n",
    "        print(f\"Project filter: {before} â†’ {len(df)} rows\")\n",
    "    \n",
    "    if input_data.data_groups and 'data_group' in df.columns:\n",
    "        before = len(df)\n",
    "        try:\n",
    "            # Handle both list and non-list data_group columns\n",
    "            if df['data_group'].apply(lambda x: isinstance(x, list)).any():\n",
    "                mask = df['data_group'].apply(lambda x: \n",
    "                    isinstance(x, list) and any(item in input_data.data_groups for item in x))\n",
    "            else:\n",
    "                mask = df['data_group'].isin(input_data.data_groups)\n",
    "            df = df[mask]\n",
    "            print(f\"Data group filter: {before} â†’ {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in data_group filtering: {e}\")\n",
    "            print(f\"Sample data_group values: {df['data_group'].head()}\")\n",
    "    \n",
    "    if input_data.codes and 'codes' in df.columns:\n",
    "        before = len(df)\n",
    "        try:\n",
    "            # Handle both list and non-list codes columns\n",
    "            if df['codes'].apply(lambda x: isinstance(x, list)).any():\n",
    "                mask = df['codes'].apply(lambda x: \n",
    "                    isinstance(x, list) and any(item in input_data.codes for item in x))\n",
    "            else:\n",
    "                mask = df['codes'].isin(input_data.codes)\n",
    "            df = df[mask]\n",
    "            print(f\"Codes filter: {before} â†’ {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in codes filtering: {e}\")\n",
    "            print(f\"Sample codes values: {df['codes'].head()}\")\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"Error: All rows were filtered out. Please check your filter criteria.\")\n",
    "        return\n",
    "    \n",
    "    sentences = df['text'].dropna().tolist()\n",
    "    print(f\"Final dataset: {len(sentences)} text segments ready for processing\")\n",
    "\n",
    "    # Add filtered sentences tracking here\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if not isinstance(sentence, str):\n",
    "            print(f\"Not a string: type={type(sentence)}, value={sentence}\")\n",
    "        tokens = tokenize_and_filter([sentence],stop_list=stop_words, \n",
    "                                   lemmatize=True, \n",
    "                                   cross_pos_normalize=input_data.cross_pos_normalize)\n",
    "        if tokens:  # Only keep sentences that have tokens after filtering\n",
    "            filtered_sentences.append(sentence)\n",
    "    print(f\"After filtering: {len(filtered_sentences)} valid text segments\")\n",
    "    \n",
    "\n",
    "    # Filter out excluded codes if specified\n",
    "    if hasattr(input_data, 'excluded_codes') and input_data.excluded_codes and 'codes' in df.columns:\n",
    "        before = len(df)\n",
    "        try:\n",
    "            # Handle both list and non-list codes columns\n",
    "            if df['codes'].apply(lambda x: isinstance(x, list)).any():\n",
    "                # Keep rows where NONE of the excluded codes are present\n",
    "                mask = df['codes'].apply(lambda x: \n",
    "                    isinstance(x, list) and not any(item in input_data.excluded_codes for item in x))\n",
    "            else:\n",
    "                # Keep rows where the code is not in excluded_codes\n",
    "                mask = ~df['codes'].isin(input_data.excluded_codes)\n",
    "            df = df[mask]\n",
    "            print(f\"Excluded codes filter: {before} â†’ {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in excluded_codes filtering: {e}\")\n",
    "    \n",
    "        # Define a custom word filter function\n",
    "    def custom_word_filter(word):\n",
    "        # First normalize with word families\n",
    "        word_lower = word.lower()\n",
    "        if word_lower in word_to_base:\n",
    "            word = word_to_base[word_lower]\n",
    "        \n",
    "        # Manual exclusion of common words that should be filtered\n",
    "        manual_exclusions = {'got', 'get', 'just', 'like', 'many', 'much', 'very', 'really', 'make'}\n",
    "        \n",
    "        return (word.lower() not in stop_words and\n",
    "                word.lower() not in manual_exclusions and\n",
    "                len(word) > 2 and  # Exclude very short words\n",
    "                not any(c.isdigit() for c in word) and  # Exclude words with numbers\n",
    "                re.match(r'^[a-z]+$', word.lower()))  # Only pure alphabetic words\n",
    "                \n",
    "    # Check if seed words were provided in the input\n",
    "    if hasattr(input_data, 'seed_words') and input_data.seed_words and input_data.seed_words.strip().lower() != \"none\":\n",
    "        seed_input = input_data.seed_words.strip()\n",
    "\n",
    "        if \":\" in seed_input:\n",
    "            use_group_label = True\n",
    "            for part in seed_input.split(\";\"):\n",
    "                part = part.strip()\n",
    "                if \":\" in part:\n",
    "                    group_label, word_str = part.split(\":\", 1)\n",
    "                    group_label = group_label.strip().lower()\n",
    "                    words = [w.strip().lower() for w in word_str.split(\",\") if w.strip()]\n",
    "                    seed_groups[group_label] = set(words)\n",
    "                    seed_words.append(group_label)\n",
    "                    print(f\"Group mode: all {words} will be treated as '{group_label}'\")\n",
    "                else:\n",
    "                    individuals = [w.strip().lower() for w in part.split(\",\") if w.strip()]\n",
    "                    seed_words.extend(individuals)\n",
    "                    print(f\"Individual mode: adding {individuals}\")\n",
    "        else:\n",
    "            seed_words = [w.strip().lower() for w in seed_input.split(\",\") if w.strip()]\n",
    "            print(f\"Pure individual word mode: using {seed_words}\")\n",
    "        if use_group_label:\n",
    "                sentences = [replace_group_words(text, seed_groups) for text in sentences]\n",
    "    else:\n",
    "        # Process sentences to get word frequencies for auto-selection of top words\n",
    "        print(\"WARNING: No seed words provided or 'NONE' specified. Using top frequent words as seeds... \")\n",
    "        excluded_words = stop_words.union(set(WORD_FAMILIES.keys()))\n",
    "        all_tokens = []\n",
    "        for sentence in sentences:\n",
    "            tokens = tokenize_and_filter([sentence], stop_list=stop_words,\n",
    "                                        lemmatize=True,\n",
    "                                        cross_pos_normalize=input_data.cross_pos_normalize)\n",
    "            filtered_tokens = [token.lower() for token in tokens if custom_word_filter(token)]\n",
    "            all_tokens.extend(filtered_tokens)\n",
    "        word_counts = Counter(all_tokens)\n",
    "        top_words = [word for word, _ in word_counts.most_common(30)\n",
    "                    if word.lower() not in excluded_words][:min(10, len(word_counts))]\n",
    "        seed_words = top_words\n",
    "        print(f\"Top frequent words as seeds: {seed_words}\")\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Clean and normalize seed words, but they're already preprocessed with word families\n",
    "    print(f\"Original seed words before cleaning: {seed_words}\")\n",
    "    clean_seed_words = clean_words(seed_words)\n",
    "    print(f\"Seed words after cleaning: {clean_seed_words}\")\n",
    "    clean_seeds = clean_words(seed_words)\n",
    "    seed_words = normalize_words(clean_seeds, stop_words, lemmatize=True, cross_pos_normalize=input_data.cross_pos_normalize)\n",
    "    seed_words = list(set(seed_words))\n",
    "    print(\"Final normalized seed words:\", seed_words)\n",
    "    \n",
    "    \n",
    "    # choose context source: keep stop-words only for RoBERTa\n",
    "    if input_data.clustering_method == 1:          # 1 = RoBERTa\n",
    "        sentences_for_embedding = sentences        # full context\n",
    "    else:                                          # 2-4 = Jaccard/PMI/TF-IDF\n",
    "        sentences_for_embedding = filtered_sentences\n",
    "\n",
    "    word_embeddings, similarity_matrix, co_occurrence_matrix = train_embedding(\n",
    "        sentences_for_embedding,\n",
    "        context_window = input_data.window_size, \n",
    "        stop_list  = stop_words, \n",
    "        seed_words = seed_words, \n",
    "        clustering_method  = input_data.clustering_method,\n",
    "        num_words = input_data.num_words, \n",
    "        lemmatize = True, \n",
    "        min_word_frequency = input_data.min_word_frequency,\n",
    "        reuse_clusterings  = input_data.reuse_clusterings,\n",
    "        cross_pos_normalize= getattr(input_data, 'cross_pos_normalize', False),\n",
    "        distance_metric = getattr(input_data, 'distance_metric', 'default'),\n",
    "        custom_word_filter = custom_word_filter\n",
    "    )\n",
    "\n",
    "    elapsed_time = time.time() - start\n",
    "    print(f\"Embedding generation completed in {elapsed_time:.1f} seconds\")\n",
    "\n",
    "    if word_embeddings is None:\n",
    "        print(\"Error: Failed to generate embeddings. Please check your input data and parameters.\")\n",
    "        return\n",
    "    # Plot Similarity Heatmap\n",
    "    \n",
    "    print(\"Plotting similarity heatmap...\")\n",
    "    fig_heat = plot_heatmap(\n",
    "        input_data.clustering_method, word_embeddings, \n",
    "        similarity_matrix, co_occurrence_matrix, input_data.distance_metric\n",
    "    )\n",
    "\n",
    "\n",
    "    print(\"-----------------------------------------------\")\n",
    "    print(\"Plotting semantic network (plain, no categories)...\")\n",
    "    fig_sn1 = plot_semantic_network(\n",
    "        word_embeddings,\n",
    "        [] if auto_selected_seeds else seed_words,\n",
    "        input_data.clustering_method,\n",
    "        similarity_matrix, co_occurrence_matrix,\n",
    "        semantic_categories=None,               \n",
    "        link_threshold = input_data.link_threshold,\n",
    "        link_color_threshold= input_data.link_color_threshold,\n",
    "        distance_metric=getattr(input_data, 'distance_metric', 'default'))\n",
    "    \n",
    "    filename = f\"semantic_network_plain_m{input_data.clustering_method}_{input_data.distance_metric}.png\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    out_path = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "    fig_sn1.suptitle(\"Semantic Network (Plain)\", fontsize=30, y=0.98, fontweight='bold')\n",
    "    fig_sn1.subplots_adjust(top=0.95)\n",
    "\n",
    "    fig_sn1.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"âœ” [OK] Saved {out_path}\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    \n",
    "    # Coloured visualisation (only if custom colors are enabled) \n",
    "    if getattr(input_data, 'custom_colors', False):\n",
    "        # Print the actual list of words used in the network so users can match for coloring\n",
    "        print(\"\\n Word list available for coloring:\")\n",
    "        print(sorted(list(word_embeddings.keys())))\n",
    "        print(\"\\nUsing predefined semantic categories for custom grouping\")\n",
    "        # Use seed_words instead of empty list for the colored visualization\n",
    "        fig_sn2 = plot_semantic_network(\n",
    "            word_embeddings,\n",
    "            seed_words,\n",
    "            input_data.clustering_method,\n",
    "            similarity_matrix, co_occurrence_matrix,\n",
    "            semantic_categories = input_data.semantic_categories,\n",
    "            link_threshold     = input_data.link_threshold,\n",
    "            link_color_threshold = input_data.link_color_threshold,\n",
    "            distance_metric=getattr(input_data, 'distance_metric', 'default')\n",
    "        )\n",
    "        filename = f\"semantic_network_customcolor_m{input_data.clustering_method}_{input_data.distance_metric}.png\"\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        out_path = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "        fig_sn2.suptitle(\"Semantic Network (Custom Colors)\", fontsize=30, y=0.98, fontweight='bold')\n",
    "        fig_sn2.subplots_adjust(top=0.95)\n",
    "\n",
    "        fig_sn2.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"âœ” [OK] Saved {out_path}\")\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "    # Add a second network visualization that removes seed nodes\n",
    "    print(\"\\nGenerating secondary network with seed nodes hidden...\")\n",
    "    \n",
    "    # Filter out seed words from word embeddings and matrices\n",
    "    non_seed_words = [word for word in word_embeddings.keys() if word not in seed_words]\n",
    "    \n",
    "    if len(non_seed_words) > 5:  # Only proceed if we have enough nodes to make a meaningful network\n",
    "        non_seed_embeddings = {word: word_embeddings[word] for word in non_seed_words}\n",
    "        \n",
    "        # Create filtered similarity/co-occurrence matrices\n",
    "        if similarity_matrix is not None:\n",
    "            words = list(word_embeddings.keys())\n",
    "            indices = [words.index(word) for word in non_seed_words]\n",
    "            filtered_similarity = similarity_matrix[np.ix_(indices, indices)]\n",
    "        else:\n",
    "            filtered_similarity = None\n",
    "            \n",
    "        if co_occurrence_matrix is not None:\n",
    "            words = list(word_embeddings.keys())\n",
    "            indices = [words.index(word) for word in non_seed_words]\n",
    "            filtered_co_occurrence = co_occurrence_matrix[np.ix_(indices, indices)]\n",
    "        else:\n",
    "            filtered_co_occurrence = None\n",
    "        \n",
    "        # Plot the filtered network\n",
    "        print(f\"Plotting secondary network with {len(non_seed_words)} nodes (seeds hidden)...\")\n",
    "        plt.figure(figsize=(16, 12))\n",
    "        fig_sn3 = plot_semantic_network(\n",
    "            non_seed_embeddings, [], \n",
    "            input_data.clustering_method, \n",
    "            filtered_similarity, filtered_co_occurrence, \n",
    "            semantic_categories=None,\n",
    "            link_threshold=input_data.link_threshold,\n",
    "            link_color_threshold=input_data.link_color_threshold,\n",
    "            distance_metric=getattr(input_data, 'distance_metric', 'default')\n",
    "        )\n",
    "        filename = f\"semantic_network_noseeds_m{input_data.clustering_method}_{input_data.distance_metric}.png\"\n",
    "        out_path = os.path.join(OUTPUT_DIR, filename)\n",
    "        fig_sn3.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"âœ” [OK] Saved {out_path}\")\n",
    "\n",
    "        fig_sn3.suptitle(\"Semantic Network (Seeds Hidden)\", fontsize=30, y=0.98, fontweight='bold')\n",
    "        fig_sn3.subplots_adjust(top=0.95)\n",
    "        plt.show()\n",
    "        \n",
    "        # If custom coloring was used in the first visualization, also do it for the second\n",
    "        if hasattr(input_data, 'custom_colors') and input_data.custom_colors and hasattr(input_data, 'semantic_categories'):\n",
    "            semantic_categories = input_data.semantic_categories\n",
    "            print(\"\\nGenerating secondary network with custom grouping (seeds hidden)...\")\n",
    "            fig_sn4 = plot_semantic_network(\n",
    "                non_seed_embeddings, [], \n",
    "                input_data.clustering_method, \n",
    "                filtered_similarity, filtered_co_occurrence, \n",
    "                semantic_categories=semantic_categories,\n",
    "                link_threshold=input_data.link_threshold,\n",
    "                link_color_threshold=input_data.link_color_threshold,\n",
    "                distance_metric=getattr(input_data, 'distance_metric', 'default')\n",
    "            )\n",
    "            filename = f\"semantic_network_noseeds_customcolor_m{input_data.clustering_method}_{input_data.distance_metric}.png\"\n",
    "            out_path = os.path.join(OUTPUT_DIR, filename)\n",
    "            fig_sn4.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "            print(f\"âœ” [OK] Saved {out_path}\")\n",
    "\n",
    "            fig_sn4.suptitle(\"Semantic Network with Custom Grouping (Seeds Hidden)\", fontsize=30, y=0.98, fontweight='bold')\n",
    "            fig_sn4.subplots_adjust(top=0.95)\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"Not enough non-seed nodes to generate a meaningful secondary network.\")\n",
    "    \n",
    "    # Print the list of actual nodes used in the network (excluding seeds)\n",
    "    print(\"\\nFinal nodes used in network (excluding seeds):\")\n",
    "    if 'non_seed_words' in locals() and len(non_seed_words) > 0:\n",
    "        # Get frequencies for each node and sort by frequency (highest first)\n",
    "        print(f\"Total non-seed words: {len(non_seed_words)}\")\n",
    "        \n",
    "        # Check if word_frequencies is defined, otherwise use the word_counts from earlier\n",
    "        if 'word_frequencies' not in locals() and 'word_counts' in locals():\n",
    "            word_frequencies = word_counts\n",
    "        elif 'word_frequencies' not in locals():\n",
    "            print(\"Warning: Word frequency information not available\")\n",
    "            # Just print the words without frequencies\n",
    "            for word in sorted(non_seed_words):\n",
    "                print(f\"- {word}\")\n",
    "        else:\n",
    "            # Create list of (word, frequency) pairs and sort by frequency\n",
    "            freq_sorted_words = [(word, word_frequencies[word]) for word in non_seed_words if word in word_frequencies]\n",
    "            freq_sorted_words.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            for word, freq in freq_sorted_words:\n",
    "                print(f\"- {word}: {freq}\")\n",
    "    else:\n",
    "        print(\"No non-seed nodes were used in the network.uelse\")\n",
    "    \n",
    "    # Print the number of filtered sentences used\n",
    "    if 'filtered_sentences' in locals():\n",
    "        print(f\"\\nTotal number of filtered sentences used: {len(filtered_sentences)}\")\n",
    "        if 'seed_words' in locals() and seed_words:\n",
    "            # Enhanced seed word detection using word families\n",
    "            seed_containing_sentences = 0\n",
    "            for sentence in filtered_sentences:\n",
    "                sentence_lower = sentence.lower()\n",
    "                contains_seed = False\n",
    "                for seed in seed_words:\n",
    "                    # Check direct match\n",
    "                    if seed.lower() in sentence_lower:\n",
    "                        contains_seed = True\n",
    "                        break\n",
    "                    # Check word family variants\n",
    "                    for family, variants in WORD_FAMILIES.items():\n",
    "                        if seed.lower() == family.lower() or seed.lower() in [v.lower() for v in variants]:\n",
    "                            if any(variant.lower() in sentence_lower for variant in variants):\n",
    "                                contains_seed = True\n",
    "                                break\n",
    "                    if contains_seed:\n",
    "                        break\n",
    "                if contains_seed:\n",
    "                    seed_containing_sentences += 1\n",
    "            print(f\"Number of filtered sentences containing seed words: {seed_containing_sentences}\")\n",
    "    \n",
    "    print(\"\\nAnalysis complete\")\n",
    "    print(f\"\\nNetwork visualization method: {input_data.clustering_method}\")\n",
    "    \n",
    "    if input_data.clustering_method == 1:\n",
    "        print(\"Method: RoBERTa â€“ Shows semantic relationships based on contextual embeddings\")\n",
    "    elif input_data.clustering_method == 2:\n",
    "        if input_data.distance_metric == \"cosine\":\n",
    "            print(\"Method: Jaccard (cosine) â€“ Uses context window vectors to compute cosine-based similarity between word usage patterns\")\n",
    "        elif input_data.distance_metric == \"default\":\n",
    "            print(\"Method: Jaccard (default) â€“ Uses binary co-occurrence counts within a context window to capture word overlap\")\n",
    "    elif input_data.clustering_method == 3:\n",
    "        print(\"Method: PMI â€“ Highlights statistically significant word associations based on pointwise mutual information\")\n",
    "    elif input_data.clustering_method == 4:\n",
    "        if input_data.distance_metric == \"cosine\":\n",
    "            print(\"Method: TF-IDF (cosine) â€“ Uses TF-IDF-weighted context vectors to compute cosine similarity between words\")\n",
    "        elif input_data.distance_metric == \"default\":\n",
    "            print(\"Method: TF-IDF (default) â€“ Uses raw TF-IDF-weighted co-occurrence scores for word associations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c35e797",
   "metadata": {},
   "source": [
    "### 6. Code-based Heatmap \n",
    "\n",
    "This function builds & displays a heat-map of how often the top N interview â€œcodesâ€ appear together across transcriptsâ€”optionally runs hierarchical clustering so related codes sit next to each otherâ€”giving a quick visual of thematic overlap in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55402bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeatmapInput(BaseModel):\n",
    "    filepath: FilePath \n",
    "    num_codes: int = 10\n",
    "    seed_codes: Optional[List[str]] = None  # â† New\n",
    "    projects: Optional[List[str]] = None \n",
    "    data_groups: Optional[List[str]] = None\n",
    "    clustered: bool = True\n",
    "\n",
    "    @field_validator(\"num_codes\")\n",
    "    def validate_num_codes(cls, v):\n",
    "        if v <= 0:\n",
    "            raise ValueError(\"num_codes must be greater than 0\")\n",
    "        return v\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=10000)\n",
    "def parse_string_list(value: Union[str, list, None]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Performance-optimized string-formatted list parser for code lists.\n",
    "    \n",
    "    Args:\n",
    "        value: String, list or None containing codes\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: Cleaned and parsed list of codes\n",
    "    \"\"\"\n",
    "    if pd.isna(value) or value == \"\" or value is None:\n",
    "        return []\n",
    "    \n",
    "    if isinstance(value, list):\n",
    "        return [str(item).lower().strip() for item in value if item]\n",
    "        \n",
    "    if isinstance(value, str):\n",
    "        value = value.strip()\n",
    "        \n",
    "        if value in [\"[]\", \"['']\", '[\"\"]', \"nan\", \"NaN\"]:\n",
    "            return []\n",
    "            \n",
    "        try:\n",
    "            if value.startswith(\"[\") and value.endswith(\"]\"):\n",
    "                parsed = ast.literal_eval(value) \n",
    "                if isinstance(parsed, list):\n",
    "                    return [str(item).lower().strip() for item in parsed if item and str(item).strip()]\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass\n",
    "            \n",
    "        try:\n",
    "            cleaned = value.strip(\"[]\").replace(\"'\", \"\").replace('\"', \"\")\n",
    "            if cleaned:\n",
    "                items = [item.strip().lower() for item in cleaned.split(\",\")]\n",
    "                return [item for item in items if item]\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    return []\n",
    "\n",
    "\n",
    "def create_code_cooccurrence_heatmap(input_data: HeatmapInput):\n",
    "    filepath = input_data.filepath\n",
    "    num_codes = input_data.num_codes\n",
    "    projects = input_data.projects\n",
    "    data_groups = input_data.data_groups\n",
    "    clustered = input_data.clustered\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Filter by projects \n",
    "    if projects:\n",
    "        df = df[df['project'].isin(projects)]\n",
    "\n",
    "    # Filter by data groups\n",
    "    if data_groups:\n",
    "        df = df[df['data_group'].apply(lambda x: any(g in parse_string_list(x) for g in data_groups))]\n",
    "\n",
    "    # Vectorized code parsing\n",
    "    all_codes = []\n",
    "    for codes in df['codes'].dropna():\n",
    "        all_codes.extend(parse_string_list(codes))\n",
    "\n",
    "    # Get top N most frequent codes\n",
    "    if input_data.seed_codes:\n",
    "        seed_set = set(code.lower().strip() for code in input_data.seed_codes)\n",
    "        cooccurrence_counter = Counter()\n",
    "\n",
    "        # Count co-occurring codes with seeds\n",
    "        for codes in df['codes'].dropna():\n",
    "            code_list = set(parse_string_list(codes))\n",
    "            if seed_set & code_list:  # If any seed is in the list\n",
    "                overlapping = code_list - seed_set\n",
    "                cooccurrence_counter.update(overlapping)\n",
    "\n",
    "        # Add top-N co-occurring codes\n",
    "        top_overlap = [code for code, _ in cooccurrence_counter.most_common(num_codes)]\n",
    "        selected_codes = list(seed_set) + top_overlap\n",
    "        print(f\"Selected codes: {selected_codes}\")\n",
    "    else:\n",
    "        # Use top-N most frequent codes in corpus\n",
    "        selected_codes = [code for code, _ in Counter(all_codes).most_common(num_codes)]\n",
    "        print(f\"Top {num_codes} most frequent codes: {selected_codes}\")\n",
    "\n",
    "    # Check if we have any codes to analyze\n",
    "    if not selected_codes:\n",
    "        print(\"No codes found to analyze. Please check your input data.\")\n",
    "        return\n",
    "\n",
    "    # Co-occurrence matrix using vectorized operations\n",
    "    cooc_matrix = np.zeros((len(selected_codes), len(selected_codes)))\n",
    "    \n",
    "    for codes in df['codes'].dropna():\n",
    "        codes_set = set(parse_string_list(codes)).intersection(selected_codes)\n",
    "        for i, code1 in enumerate(selected_codes):\n",
    "            for j in range(i + 1, len(selected_codes)):\n",
    "                code2 = selected_codes[j]\n",
    "                if code1 in codes_set and code2 in codes_set:\n",
    "                    cooc_matrix[i][j] += 1\n",
    "                    cooc_matrix[j][i] += 1\n",
    "\n",
    "    # Check if matrix is empty\n",
    "    if np.all(cooc_matrix == 0):\n",
    "        print(\"No co-occurrences found. Please check your input data.\")\n",
    "        return\n",
    "\n",
    "    heatmap_df = pd.DataFrame(cooc_matrix, index=selected_codes, columns=selected_codes)\n",
    "    plt.style.use('dark_background')\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    \n",
    "    if clustered:\n",
    "        # Convert co-occurrence matrix to proper distance format for linkage\n",
    "        row_linkage = hierarchy.linkage(pdist(heatmap_df), method='ward')\n",
    "        col_linkage = hierarchy.linkage(pdist(heatmap_df.T), method='ward')\n",
    "        \n",
    "        g = sns.clustermap(heatmap_df,\n",
    "                        annot=True,\n",
    "                        fmt='g',\n",
    "                        cmap='inferno',\n",
    "                        row_linkage=row_linkage,\n",
    "                        col_linkage=col_linkage,\n",
    "                        figsize=(12, 10),\n",
    "                        dendrogram_ratio=0.2,\n",
    "                        colors_ratio=0.03)\n",
    "        g.fig.patch.set_facecolor('black')\n",
    "        g.ax_heatmap.set_facecolor('black')\n",
    "\n",
    "        for item in [g.ax_row_dendrogram, g.ax_col_dendrogram]:\n",
    "            item.set_facecolor('black')\n",
    "            for c in item.collections:\n",
    "                c.set_color('white')\n",
    "\n",
    "        filename = f\"code_heatmap_clustered_{num_codes}.png\"\n",
    "        out_path = os.path.join(OUTPUT_DIR, filename)\n",
    "        g.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"âœ” [OK] Saved clustered heatmap: {out_path}\")\n",
    "    else:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        ax = sns.heatmap(\n",
    "            heatmap_df,\n",
    "            annot=True,\n",
    "            fmt='g',\n",
    "            cmap='inferno'\n",
    "        )\n",
    "        ax.set_title('Code Co-occurrence Matrix')\n",
    "\n",
    "        filename = f\"code_heatmap_plain_{num_codes}.png\"\n",
    "        out_path = os.path.join(OUTPUT_DIR, filename)\n",
    "        plt.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"âœ” [OK] Saved plain heatmap: {out_path}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37a75a9",
   "metadata": {},
   "source": [
    "# Data Overview "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1976323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Overview and Preparation \n",
    "\n",
    "# Path to your cleaned data\n",
    "DATA_PATH = CSV_PATH  # alias for clarity\n",
    "\n",
    "# --- Schema and Required Fields ---\n",
    "SCHEMA = {\n",
    "    \"project\": str, # List project \n",
    "    \"number\": str, # Position information\n",
    "    \"reference\": int, # Position information\n",
    "    \"text\": str, # Content, critical field: must not be empty\n",
    "    \"document\": str, # Data source, Critical field: must not be empty\n",
    "    \"old_codes\": list[str], # Optional: codings, must be a list of strings\n",
    "    \"start_position\": int, # Position information\n",
    "    \"end_position\": int, # Position information\n",
    "    \"data_group\": list[str], # Optional, to differentiate document sets: Must be a list of strings\n",
    "    \"text_length\": int, # Optional: NLP info\n",
    "    \"word_count\": int, # Optional: NLP info\n",
    "    \"doc_id\": str, # Optional: NLP info, unique paragrah level identifier\n",
    "    \"codes\": list[str] # critical for analyses with codes, Must be a list of strings\n",
    "}\n",
    "\n",
    "REQUIRED_FIELDS = [\"text\", \"document\", \"project\"]\n",
    "\n",
    "# --- Safe List Conversion Function ---\n",
    "def safe_convert_list(x):\n",
    "    \"\"\"Safe conversion of various formats to a list, handling edge cases.\"\"\"\n",
    "    try:\n",
    "        if isinstance(x, float) and np.isnan(x):\n",
    "            return []\n",
    "        if x in (None, \"\", \"nan\", \"NaN\"):\n",
    "            return []\n",
    "        if isinstance(x, list):\n",
    "            return [str(i).strip().strip(\"\\\"'[]\") for i in x if str(i).strip()]\n",
    "\n",
    "        x = str(x).strip()\n",
    "        if x == \"[]\":\n",
    "            return []\n",
    "\n",
    "        x = x.replace(\"â€™\", '\"').replace(\"][\", \",\").replace(\"],['\", '\",\"').replace('\"\"', '\"')\n",
    "\n",
    "        if not x.startswith(\"[\"):\n",
    "            x = \"[\" + x\n",
    "        if not x.endswith(\"]\"):\n",
    "            x = x + \"]\"\n",
    "\n",
    "        try:\n",
    "            items = json.loads(x)\n",
    "        except Exception:\n",
    "            items = x.strip(\"[]\").split(\",\")\n",
    "        return [str(i).strip().strip(\"\\\"'[]\") for i in items if str(i).strip()]\n",
    "\n",
    "    except Exception:\n",
    "        print(f\"Warning: Could not convert value: {x}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def parse_list_compat(val):\n",
    "    if pd.isna(val) or val in (None, \"\", \"nan\", \"NaN\"):\n",
    "        return []\n",
    "    if isinstance(val, list):\n",
    "        return val\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            if val.startswith(\"[\") and val.endswith(\"]\"):\n",
    "                return json.loads(val)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return [v.strip() for v in val.strip(\"[]\").replace(\"'\", \"\").replace('\"', \"\").split(\",\") if v.strip()]\n",
    "    return [val]\n",
    "\n",
    "# --- Load Data ---\n",
    "df_clean = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# --- Required Fields ---\n",
    "missing_required = [c for c in REQUIRED_FIELDS if c not in df_clean.columns]\n",
    "if missing_required:\n",
    "    print(f\"Missing required fields: {missing_required}\")\n",
    "\n",
    "for col in REQUIRED_FIELDS:\n",
    "    if col in df_clean.columns:\n",
    "        n_empty = df_clean[col].isna().sum() + (df_clean[col] == \"\").sum()\n",
    "        print(f\"Empty '{col}â€™: {n_empty}\")\n",
    "\n",
    "\n",
    "# --- Show First 5 Records ---\n",
    "print(\"\\nFirst 5 records:\")\n",
    "pprint.pprint(df_clean.head(5).to_dict(orient=\"records\"))\n",
    "\n",
    "# --- Schema Match ---\n",
    "print(\"\\nSchema match by column:\")\n",
    "for col, expected in SCHEMA.items():\n",
    "    if col not in df_clean.columns:\n",
    "        print(f\"{col}: MISSING\")\n",
    "    else:\n",
    "        dtype = (df_clean[col].dropna().map(type).mode()[0] if not df_clean[col].dropna().empty else None)\n",
    "        print(f\"{col}: expected {expected.__name__}, found {dtype.__name__ if dtype else 'None'}\")\n",
    "\n",
    "# --- Apply List Conversion to Specific Columns ---\n",
    "list_columns = ['old_codes', 'data_group', 'codes']\n",
    "for col in list_columns:\n",
    "    if col in df_clean.columns:\n",
    "        print(f\"\\nConverting and sanitizing {col}...\")\n",
    "        df_clean[col] = df_clean[col].apply(safe_convert_list)\n",
    "\n",
    "# --- Verify List Conversion ---\n",
    "print(\"\\nVerification of list columns:\")\n",
    "for col in list_columns:\n",
    "    if col in df_clean.columns:\n",
    "        invalid = df_clean[~df_clean[col].apply(lambda x: isinstance(x, list))].shape[0]\n",
    "        print(f\"{col}: {invalid} invalid entries\")\n",
    "        if invalid > 0:\n",
    "            print(\"Sample of first invalid entry:\")\n",
    "            print(df_clean[~df_clean[col].apply(lambda x: isinstance(x, list))][col].iloc[0])\n",
    "        else:\n",
    "            print(f\"Sample of cleaned {col}:\")\n",
    "            sample = df_clean[col].iloc[0] if len(df_clean) > 0 else []\n",
    "            print(f\"First entry: {sample}\")\n",
    "\n",
    "# --- Cleaning Summary ---\n",
    "print(\"\\nCleaning Summary:\")\n",
    "for col in list_columns:\n",
    "    if col in df_clean.columns:\n",
    "        empty = df_clean[col].apply(lambda x: len(x) == 0).sum()\n",
    "        print(f\"{col} â†’ empty: {empty}/{len(df_clean)}\")\n",
    "        \n",
    "# --- Add word_count column if needed ---\n",
    "if \"word_count\" not in df_clean.columns and \"text\" in df_clean.columns:\n",
    "    df_clean[\"word_count\"] = df_clean[\"text\"].str.split().str.len().fillna(0)\n",
    "\n",
    "print(\"\\nDataset overview:\")\n",
    "print(df_clean.info())\n",
    "\n",
    "# --- Codes column: parse and show top codes ---\n",
    "if 'codes' in df_clean.columns:\n",
    "    codes_flat = df_clean['codes'].dropna().apply(safe_convert_list).explode()\n",
    "    code_counts = codes_flat.value_counts().head(20)\n",
    "    print(\"\\nTop 20 codes by frequency:\")\n",
    "    print(code_counts)\n",
    "else:\n",
    "    print(\"No 'codes' column found in the dataframe\")\n",
    "\n",
    "# --- Count documents by project ---\n",
    "if 'project' in df_clean.columns:\n",
    "    if 'document' in df_clean.columns:\n",
    "        project_doc_counts = df_clean.groupby('project')['document'].nunique()\n",
    "    else:\n",
    "        project_doc_counts = df_clean['project'].value_counts()\n",
    "    print(\"\\nNumber of documents by project:\")\n",
    "    print(project_doc_counts)\n",
    "\n",
    "# --- Top values for data_group ---\n",
    "if 'data_group' in df_clean.columns:\n",
    "    dg_flat = df_clean['data_group'].dropna().apply(safe_convert_list).explode()\n",
    "    dg_counts = dg_flat.value_counts().head(10)\n",
    "    print(\"\\nTop 10 data_group values by frequency:\")\n",
    "    print(dg_counts)\n",
    "else:\n",
    "    print(\"\\nNo 'data_group' column found in the dataframe\")\n",
    "\n",
    "# --- Word Count ---\n",
    "if 'word_count' in df_clean.columns:\n",
    "    avg_word_count = df_clean['word_count'].mean()\n",
    "    print(f\"\\nAverage word count in text: {avg_word_count:.2f}\")\n",
    "elif 'text' in df_clean.columns:\n",
    "    df_clean['word_count'] = df_clean['text'].str.split().str.len().fillna(0)\n",
    "    avg_word_count = df_clean['word_count'].mean()\n",
    "    print(f\"\\nAverage word count in text: {avg_word_count:.2f}\")\n",
    "else:\n",
    "    print(\"No 'text' column found in the dataframe\")\n",
    "\n",
    "# --- Descriptive Statistics ---\n",
    "print(\"\\nMissing Values Count:\")\n",
    "print(df_clean.isnull().sum())\n",
    "\n",
    "numeric_cols = ['text_length', 'word_count']\n",
    "print(\"\\nDescriptive Statistics for Numeric Columns:\")\n",
    "for col in numeric_cols:\n",
    "    if col in df_clean.columns:\n",
    "        print(f\"\\nStats for {col}:\")\n",
    "        print(f\"Count: {df_clean[col].count()}\")\n",
    "        print(f\"Median: {df_clean[col].median():.2f}\")\n",
    "        print(f\"Standard Deviation: {df_clean[col].std():.2f}\")\n",
    "\n",
    "print(\"\\nDataset Overview:\")\n",
    "print(df_clean.info())\n",
    "\n",
    "# --- Unique Projects ---\n",
    "if 'project' in df_clean.columns:\n",
    "    projects = df_clean['project'].unique()\n",
    "    print(\"\\nUnique Projects:\")\n",
    "    for project in projects:\n",
    "        print(project)\n",
    "\n",
    "print(df_clean.head(5))\n",
    "\n",
    "\n",
    "# --- Load and Verify Saved File ---\n",
    "output_file_dropbox = os.path.join(DATA_DIR, '1_cleaned_data.csv')\n",
    "output_file_combined = os.path.join(BACKUP_DIR, '1_cleaned_data.csv')\n",
    "print(f\"\\nCleaned data saved to: {output_file_combined}\")\n",
    "df_clean.to_csv(output_file_dropbox, index=False)\n",
    "df_clean.to_csv(output_file_combined, index=False)\n",
    "print(f\"\\nCleaned data saved to: {output_file_combined}\")\n",
    "df = pd.read_csv(output_file_combined)\n",
    "if 'project' in df.columns:\n",
    "    projects_loaded = df['project'].unique()\n",
    "    print(\"\\nUnique Projects (from loaded file):\")\n",
    "    for project in projects_loaded:\n",
    "        print(project)\n",
    "\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da582a4",
   "metadata": {},
   "source": [
    "# Basic Analytics Tool "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3c3456",
   "metadata": {},
   "source": [
    "## 1. Wordcloud\n",
    "\n",
    "This section contains the wordcloud execution block. this produces a wordcloud from the data loaded, showing words that come up more frequent in larger size. The heatmap allows color coding by user defined categories, that represent concepts or themes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1df4027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter text based on project and terms\n",
    "filtered_df = df.copy()\n",
    "if 'project' in filtered_df.columns:\n",
    "    filtered_df = filtered_df[filtered_df['project'] == 'oral_history'] # Adjust as needed \n",
    "\n",
    "if 'text' in filtered_df.columns:\n",
    "    text_series = filtered_df['text'].fillna('').astype(str)\n",
    "    mask = text_series.str.lower().apply(lambda x: any(term in x for term in ['education', 'learning', 'teaching', 'student', 'school', 'classroom', 'curriculum', 'academic']))\n",
    "    text_series = text_series[mask]\n",
    "\n",
    "# Categories \n",
    "cmap = cm.get_cmap(\"mako\", 5)\n",
    "categories = {\n",
    "    \"People & Relations\": {\n",
    "        \"words\": {\n",
    "            \"people\", \"student\", \"students\", \"teacher\", \"teachers\", \"professor\", \"professors\",\n",
    "            \"advisor\", \"mentor\", \"classmates\", \"friends\", \"colleague\", \"colleagues\",\n",
    "            \"mother\", \"mom\", \"father\", \"dad\", \"family\", \"kids\", \"children\",\n",
    "            \"wife\", \"husband\", \"brother\", \"sister\"\n",
    "        },\n",
    "        \"color\": cmap(0)\n",
    "    },\n",
    "    \"Education & Career\": {\n",
    "        \"words\": {\n",
    "            \"school\", \"college\", \"university\", \"department\", \"campus\",\n",
    "            \"course\", \"courses\", \"class\", \"classes\", \"curriculum\", \"degree\",\n",
    "            \"major\", \"minor\", \"graduate\", \"graduated\", \"thesis\", \"exam\",\n",
    "            \"research\", \"lab\", \"laboratory\", \"project\", \"projects\",\n",
    "            \"engineering\", \"engineer\", \"physics\", \"mathematics\",\n",
    "            \"industry\", \"company\", \"career\", \"job\", \"internship\"\n",
    "        },\n",
    "        \"color\": cmap(1)\n",
    "    },\n",
    "    \"Emotions & Cognition\": {\n",
    "        \"words\": {\n",
    "            \"think\", \"thought\", \"know\", \"understand\", \"learn\", \"learning\",\n",
    "            \"decide\", \"decision\", \"believe\", \"remember\", \"idea\", \"ideas\",\n",
    "            \"feel\", \"feeling\", \"feelings\", \"excited\", \"interested\",\n",
    "            \"curious\", \"happy\", \"proud\", \"worried\", \"scared\", \"confused\"\n",
    "        },\n",
    "        \"color\": cmap(2)\n",
    "    },\n",
    "    \"Time / Duration\": {\n",
    "        \"words\": {\n",
    "            \"first\", \"second\", \"later\", \"since\", \"before\", \"after\",\n",
    "            \"started\", \"start\", \"begin\", \"early\", \"late\",\n",
    "            \"day\", \"days\", \"week\", \"weeks\", \"month\", \"months\",\n",
    "            \"year\", \"years\", \"semester\", \"summer\", \"winter\", \"spring\", \"fall\"\n",
    "        },\n",
    "        \"color\": cmap(3)\n",
    "    },\n",
    "    \"Daily Activities & Work\": {\n",
    "        \"words\": {\n",
    "            \"work\", \"working\", \"teach\", \"teaching\", \"study\", \"studying\",\n",
    "            \"read\", \"reading\", \"write\", \"writing\", \"talk\", \"talking\",\n",
    "            \"meet\", \"meeting\", \"present\", \"presentation\", \"travel\", \"move\",\n",
    "            \"build\", \"design\", \"make\", \"made\", \"fix\", \"support\", \"help\"\n",
    "        },\n",
    "        \"color\": cmap(4)\n",
    "    },\n",
    "}\n",
    "# Word cloud\n",
    "generate_wordcloud(\n",
    "    text_series=text_series,\n",
    "    stopwords_path=STOP_LIST_FILE,\n",
    "    title='Word Cloud',\n",
    "    out_dir=OUTPUT_DIR,  # change if needed\n",
    "    categories=categories\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9543eac8",
   "metadata": {},
   "source": [
    "# Intermediate Analytics Tools \n",
    "\n",
    "This section contains all heatmap and network execuion blocks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19757f9e",
   "metadata": {},
   "source": [
    "### User Configuration for Intermediate Visualization Tool\n",
    "\n",
    "Overall Embedding/Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd42eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------\n",
    "# Variables Preparation\n",
    "#---------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "OVERVIEW:\n",
    "----------------------\n",
    "This script generates semantic networks based on the specified clustering method,\n",
    "visualizing relationships between concepts in the text corpus. Heatmaps and t-SNE \n",
    "plots are also generated to help visualize the relationships.\n",
    "\n",
    "USAGE INSTRUCTIONS:\n",
    "----------------------\n",
    "1. Configure the parameters in the CONFIG section below\n",
    "2. Save this notebook, run each block\n",
    "3. Alternatively, export to a Python script\n",
    "4. Execute with: python semantic_network.py\n",
    "5. This is what a user would tweak, or be prompted to enter in an interactive version.\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # ========================= CONFIG =========================\n",
    "    # Paths & Stop-list\n",
    "    csv_path               = CSV_PATH # Your dataset path\n",
    "    stop_list_path         = STOP_LIST_FILE\n",
    "    use_custom_stoplist    = True\n",
    "\n",
    "    # Core Analysis\n",
    "    clustering_method = 3           # 1 = RoBERTa, 2 = Jaccard, 3 = PMI, 4 = TF-IDF\n",
    "    distance_metric   = \"cosine\"   \n",
    "    \n",
    "# Note on clustering method and distance metric:\n",
    "#\n",
    "# 1. RoBERTa (clustering_method = 1)\n",
    "#    - distance_metric is always \"default\" (ignored internally)\n",
    "#    - Uses contextual embeddings directly\n",
    "#\n",
    "# 2. Jaccard (clustering_method = 2)\n",
    "#    - \"default\": context-window overlap measured by Jaccard index\n",
    "#                 (set-based, binary overlap score)\n",
    "#    - \"cosine\" : context vectors built from co-occurrence counts,\n",
    "#                 compared using cosine similarity (frequency-sensitive)\n",
    "#\n",
    "# 3. PMI (clustering_method = 3)\n",
    "#    - \"default\": classic PMI co-occurrence scores\n",
    "#    - \"cosine\" : PMI-weighted context vectors compared with cosine similarity\n",
    "#                 (captures similarity of PMI distributions rather than raw scores)\n",
    "#\n",
    "# 4. TF-IDF (clustering_method = 4)\n",
    "#    - \"cosine\" : standard TF-IDF vectors compared with cosine similarity\n",
    "#                 (recommended; normalized similarity).\n",
    "#    - \"default\": experimental overlap-based method; shared context tokens are\n",
    "#                 weighted by their global TF-IDF scores (unnormalized)\n",
    "#                 â†’ Use with caution; included as an optional example\n",
    "\n",
    "    window_size            = 20 # Context window size for co-occurrence\n",
    "    num_words              = 25 # Max number of top frequent words to analyze\n",
    "    min_word_frequency     = 2 # Ignore words that appear fewer times\n",
    "    reuse_clusterings      = False # Whether to reuse saved clustering results if available\n",
    "\n",
    "\n",
    "    # Preprocessing Filters\n",
    "    cross_pos_normalize    = True # Normalize words across parts of speech (e.g., \"learn\", \"learning\", \"learned\" -> \"learn\")\n",
    "    projects               = [\"oral_history\"]     # Filter by project names \n",
    "    data_groups            = [\"interview\"]   # Filter by data_groups \n",
    "    codes                  = [\"background\"]        # Analyse specific codes\n",
    "    excluded_codes         = ['interviewer']  # Exclude these codes, removing 'interviewer' is important for NLP\n",
    "\n",
    "    # Visualisation\n",
    "    title                = \"Semantic Network (all interviews, contextual embeddings)\"\n",
    "    link_threshold       = 0.50          \n",
    "    link_color_threshold = 0.75   # set to 99 to remove black links       \n",
    "    custom_colors = True                 \n",
    "\n",
    "    # Seeds & Colours\n",
    "    seed_words = \"education: learning, teaching, student, school, classroom, curriculum, academic\"\n",
    "\n",
    "    # You can provide seed words in two formats:\n",
    "    # 1. Grouped format (for custom node colors and labels):\n",
    "    #       \"GroupName1: word1, word2; GroupName2: word3, word4; ...\"\n",
    "    #    Example: \n",
    "    #       \"Therapy: therapy, physical therapy; Caregivers: caregivers, caregiver; family\"\n",
    "    #    â†’ 'family' will be treated as its own group, using its own color.\n",
    "    #\n",
    "    # 2. Flat list format (no grouping, default color):\n",
    "    #       \"dementia, therapy, caregivers\"\n",
    "    #    â†’ All words will appear as individual highlighted nodes with default styling.\n",
    "    \n",
    "    # consider inductive readings, and analysis \n",
    "    cmap = cm.get_cmap(\"mako\", 5)\n",
    "    semantic_categories = {\n",
    "    \"People & Relations\": {\n",
    "        \"words\": {\n",
    "            \"people\", \"student\", \"students\", \"teacher\", \"teachers\", \"professor\", \"professors\",\n",
    "            \"advisor\", \"mentor\", \"classmates\", \"friends\", \"colleague\", \"colleagues\",\n",
    "            \"mother\", \"mom\", \"father\", \"dad\", \"family\", \"kids\", \"children\",\n",
    "            \"wife\", \"husband\", \"brother\", \"sister\"\n",
    "        },\n",
    "        \"color\": cmap(0)\n",
    "    },\n",
    "    \"Education & Career\": {\n",
    "        \"words\": {\n",
    "            \"school\", \"college\", \"university\", \"department\", \"campus\",\n",
    "            \"course\", \"courses\", \"class\", \"classes\", \"curriculum\", \"degree\",\n",
    "            \"major\", \"minor\", \"graduate\", \"graduated\", \"thesis\", \"exam\",\n",
    "            \"research\", \"lab\", \"laboratory\", \"project\", \"projects\",\n",
    "            \"engineering\", \"engineer\", \"physics\", \"mathematics\",\n",
    "            \"industry\", \"company\", \"career\", \"job\", \"internship\"\n",
    "        },\n",
    "        \"color\": cmap(1)\n",
    "    },\n",
    "    \"Emotions & Cognition\": {\n",
    "        \"words\": {\n",
    "            \"think\", \"thought\", \"know\", \"understand\", \"learn\", \"learning\",\n",
    "            \"decide\", \"decision\", \"believe\", \"remember\", \"idea\", \"ideas\",\n",
    "            \"feel\", \"feeling\", \"feelings\", \"excited\", \"interested\",\n",
    "            \"curious\", \"happy\", \"proud\", \"worried\", \"scared\", \"confused\",\n",
    "            \"reflect\", \"contemplate\", \"ponder\", \"analyze\", \"reason\"\n",
    "        },\n",
    "        \"color\": cmap(2)\n",
    "    },\n",
    "    \"Time / Duration\": {\n",
    "        \"words\": {\n",
    "            \"first\", \"second\", \"later\", \"since\", \"before\", \"after\",\n",
    "            \"started\", \"start\", \"begin\", \"early\", \"late\",\n",
    "            \"day\", \"days\", \"week\", \"weeks\", \"month\", \"months\",\n",
    "            \"year\", \"years\", \"semester\", \"summer\", \"winter\", \"spring\", \"fall\"\n",
    "        },\n",
    "        \"color\": cmap(3)\n",
    "    },\n",
    "    \"Daily Activities & Work\": {\n",
    "        \"words\": {\n",
    "            \"work\", \"working\", \"teach\", \"teaching\", \"study\", \"studying\",\n",
    "            \"read\", \"reading\", \"write\", \"writing\", \"talk\", \"talking\",\n",
    "            \"meet\", \"meeting\", \"present\", \"presentation\", \"travel\", \"move\",\n",
    "            \"build\", \"design\", \"make\", \"made\", \"fix\", \"support\", \"help\",\n",
    "            \"eat\", \"sleep\", \"walk\", \"exercise\", \"commute\", \"cook\", \"clean\",\n",
    "            \"shop\", \"plan\", \"organize\", \"schedule\", \"prepare\", \"attend\"\n",
    "        },\n",
    "        \"color\": cmap(4)\n",
    "    },\n",
    "}  \n",
    "    \n",
    "    # Network Layout Parameters\n",
    "    network_layout = \"kamada-kawai\"   # Options: \"spring\", \"kamada-kawai\", \"circular\", \"shell\"\n",
    "    \n",
    "    # Heatmap Parameters\n",
    "    num_codes    = 7              # Expand up to this many codes for heatmap\n",
    "    seed_codes   = [\"background\"] # Start from these seed codes (if None, top-N most frequent codes are used)\n",
    "    clustered    = True           # If True, apply hierarchical clustering to rows/columns\n",
    "\n",
    "    # Notes:\n",
    "    # - If seed_codes is given â†’ heatmap expands with top-N co-occurring codes. \n",
    "    # - If seed_codes is None   â†’ heatmap uses top-N most frequent codes in corpus. \n",
    "    # - num_codes defines N in both cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7cb401",
   "metadata": {},
   "source": [
    "### 2. Word-based heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d9e8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSemantic Network Analysis Tool\\n\" + \"=\" * 30)\n",
    "\n",
    "try:\n",
    "    # Build pydantic object\n",
    "    params = VisualsInput(\n",
    "        filepath            = csv_path,\n",
    "        stop_list           = stop_list_path if use_custom_stoplist else None,\n",
    "        num_words           = num_words,\n",
    "        clustering_method   = clustering_method,\n",
    "        distance_metric     = distance_metric,\n",
    "        window_size         = window_size,\n",
    "        min_word_frequency  = min_word_frequency,\n",
    "        projects            = projects,\n",
    "        data_groups         = data_groups,\n",
    "        codes               = codes,\n",
    "        cross_pos_normalize = cross_pos_normalize\n",
    "    )\n",
    "\n",
    "        \n",
    "\n",
    "    # Attach non-schema extras\n",
    "    setattr(params, \"reuse_clusterings\",    reuse_clusterings)\n",
    "    setattr(params, \"seed_words\",           seed_words)\n",
    "    setattr(params, \"custom_colors\",        custom_colors)\n",
    "    setattr(params, \"semantic_categories\",  semantic_categories)\n",
    "    setattr(params, \"link_threshold\",       link_threshold)\n",
    "    setattr(params, \"link_color_threshold\", link_color_threshold)\n",
    "    setattr(params, \"excluded_codes\",       excluded_codes)    \n",
    "\n",
    "\n",
    "    # Creates: (1) plain, (2) coloured, (3) coloured+seed-hidden\n",
    "    run_heatmap_pipeline(params)\n",
    "\n",
    "\n",
    "except ValidationError as ve:\n",
    "    print(\"\\nâš  Parameter error:\\n\", ve)\n",
    "except Exception as e:\n",
    "    print(f\"\\nâš  Unexpected error: {e}\")\n",
    "finally:\n",
    "    print(\"\\nAnalysis process completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4eb354",
   "metadata": {},
   "source": [
    "### 3. tSNE plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee8ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSemantic Network Analysis Tool\\n\" + \"=\" * 30)\n",
    "\n",
    "try:\n",
    "    # Build pydantic object\n",
    "    params = VisualsInput(\n",
    "        filepath            = csv_path,\n",
    "        stop_list           = stop_list_path if use_custom_stoplist else None,\n",
    "        num_words           = num_words,\n",
    "        clustering_method   = clustering_method,\n",
    "        distance_metric     = distance_metric,\n",
    "        window_size         = window_size,\n",
    "        min_word_frequency  = min_word_frequency,\n",
    "        projects            = projects,\n",
    "        data_groups         = data_groups,\n",
    "        codes               = codes,\n",
    "        cross_pos_normalize = cross_pos_normalize\n",
    "    )\n",
    "\n",
    "        \n",
    "\n",
    "    # Attach non-schema extras\n",
    "    setattr(params, \"reuse_clusterings\",    reuse_clusterings)\n",
    "    setattr(params, \"seed_words\",           seed_words)\n",
    "    setattr(params, \"custom_colors\",        custom_colors)\n",
    "    setattr(params, \"semantic_categories\",  semantic_categories)\n",
    "    setattr(params, \"link_threshold\",       link_threshold)\n",
    "    setattr(params, \"link_color_threshold\", link_color_threshold)\n",
    "    setattr(params, \"excluded_codes\",       excluded_codes)    \n",
    "\n",
    "\n",
    "    # Creates: (1) plain, (2) coloured, (3) coloured+seed-hidden\n",
    "    run_tsne_pipeline(params)\n",
    "\n",
    "\n",
    "except ValidationError as ve:\n",
    "    print(\"\\nâš  Parameter error:\\n\", ve)\n",
    "except Exception as e:\n",
    "    print(f\"\\nâš  Unexpected error: {e}\")\n",
    "finally:\n",
    "    print(\"\\nAnalysis process completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fda4de1",
   "metadata": {},
   "source": [
    "### 4. Word-based Heatmap and Social Network \n",
    "(no custom colors, just blue nodes and yellow seeds, no edge bundling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26994053",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSemantic Network Analysis Tool\\n\" + \"=\" * 30)\n",
    "\n",
    "try:\n",
    "    # Build pydantic object\n",
    "    params = VisualsInput(\n",
    "        filepath            = csv_path,\n",
    "        stop_list           = stop_list_path if use_custom_stoplist else None,\n",
    "        num_words           = num_words,\n",
    "        clustering_method   = clustering_method,\n",
    "        distance_metric     = distance_metric,\n",
    "        window_size         = window_size,\n",
    "        min_word_frequency  = min_word_frequency,\n",
    "        projects            = projects,\n",
    "        data_groups         = data_groups,\n",
    "        codes               = codes,\n",
    "        cross_pos_normalize = cross_pos_normalize\n",
    "    )\n",
    "\n",
    "        \n",
    "\n",
    "    # Attach non-schema extras\n",
    "    setattr(params, \"reuse_clusterings\",    reuse_clusterings)\n",
    "    setattr(params, \"seed_words\",           seed_words)\n",
    "    setattr(params, \"custom_colors\",        custom_colors)\n",
    "    setattr(params, \"semantic_categories\",  semantic_categories)\n",
    "    setattr(params, \"link_threshold\",       link_threshold)\n",
    "    setattr(params, \"link_color_threshold\", link_color_threshold)\n",
    "    setattr(params, \"excluded_codes\",       excluded_codes)    \n",
    "\n",
    "\n",
    "    # Creates: (1) plain, (2) coloured, (3) coloured+seed-hidden\n",
    "    run_heatmap_network_plain_pipeline(params)\n",
    "\n",
    "\n",
    "except ValidationError as ve:\n",
    "    print(\"\\nâš  Parameter error:\\n\", ve)\n",
    "except Exception as e:\n",
    "    print(f\"\\nâš  Unexpected error: {e}\")\n",
    "finally:\n",
    "    print(\"\\nAnalysis process completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8ba687",
   "metadata": {},
   "source": [
    "### 5. Word-based Heatmap and Social Network \n",
    "(custom colors, no edge bundling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d88201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSemantic Network Analysis Tool\\n\" + \"=\" * 30)\n",
    "\n",
    "try:\n",
    "    # Build pydantic object\n",
    "    params = VisualsInput(\n",
    "        filepath            = csv_path,\n",
    "        stop_list           = stop_list_path if use_custom_stoplist else None,\n",
    "        num_words           = num_words,\n",
    "        clustering_method   = clustering_method,\n",
    "        distance_metric     = distance_metric,\n",
    "        window_size         = window_size,\n",
    "        min_word_frequency  = min_word_frequency,\n",
    "        projects            = projects,\n",
    "        data_groups         = data_groups,\n",
    "        codes               = codes,\n",
    "        cross_pos_normalize = cross_pos_normalize\n",
    "    )\n",
    "\n",
    "        \n",
    "\n",
    "    # Attach non-schema extras\n",
    "    setattr(params, \"reuse_clusterings\",    reuse_clusterings)\n",
    "    setattr(params, \"seed_words\",           seed_words)\n",
    "    setattr(params, \"custom_colors\",        custom_colors)\n",
    "    setattr(params, \"semantic_categories\",  semantic_categories)\n",
    "    setattr(params, \"link_threshold\",       link_threshold)\n",
    "    setattr(params, \"link_color_threshold\", link_color_threshold)\n",
    "    setattr(params, \"excluded_codes\",       excluded_codes)    \n",
    "\n",
    "\n",
    "    # Creates: (1) plain, (2) coloured, (3) coloured+seed-hidden\n",
    "    run_heatmap_network_pipeline(params)\n",
    "\n",
    "\n",
    "except ValidationError as ve:\n",
    "    print(\"\\nâš  Parameter error:\\n\", ve)\n",
    "except Exception as e:\n",
    "    print(f\"\\nâš  Unexpected error: {e}\")\n",
    "finally:\n",
    "    print(\"\\nAnalysis process completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6b780e",
   "metadata": {},
   "source": [
    "### 6. Code-based Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d33b0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = HeatmapInput(\n",
    "    filepath    = CSV_PATH,\n",
    "    num_codes   = num_codes,\n",
    "    seed_codes  = seed_codes,\n",
    "    projects    = projects,\n",
    "    data_groups = data_groups,\n",
    "    clustered   = clustered\n",
    ")\n",
    "\n",
    "# Run the heatmap generator\n",
    "create_code_cooccurrence_heatmap(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd93f8f",
   "metadata": {},
   "source": [
    "#  Advanced Analytic Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed709f2f",
   "metadata": {},
   "source": [
    "### User Configuration for Advanced Visualization Tool\n",
    "\n",
    "This section contains advanced heatmap and network combined execution blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbc21be",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5161537",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------\n",
    "# Variables Preparation\n",
    "#---------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "OVERVIEW:\n",
    "----------------------\n",
    "This script generates semantic networks based on the specified clustering method,\n",
    "visualizing relationships between concepts in the text corpus. Heatmaps and t-SNE \n",
    "plots are also generated to help visualize the relationships.\n",
    "\n",
    "USAGE INSTRUCTIONS:\n",
    "----------------------\n",
    "1. Configure the parameters in the CONFIG section below\n",
    "2. Save this notebook, run each block\n",
    "3. Alternatively, export to a Python script\n",
    "4. Execute with: python semantic_network.py\n",
    "5. This is what a user would tweak, or be prompted to enter in an interactive version.\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # ========================= CONFIG =========================\n",
    "    # Paths & Stop-list\n",
    "    csv_path               = CSV_PATH # Your dataset path\n",
    "    stop_list_path         = STOP_LIST_FILE\n",
    "    use_custom_stoplist    = True\n",
    "\n",
    "    # Core Analysis\n",
    "    clustering_method = 2           # 1 = RoBERTa, 2 = Jaccard, 3 = PMI, 4 = TF-IDF\n",
    "    distance_metric   = \"cosine\"   \n",
    "    \n",
    "    # Note on clustering method and distance metric:\n",
    "    # - If clustering_method == 1 (RoBERTa), distance_metric is always \"default\" (ignored internally)\n",
    "    # - For clustering_method in [2, 3, 4], distance_metric can be:\n",
    "    #     \"default\" â†’ uses raw co-occurrence or weighted scores\n",
    "    #     \"cosine\"  â†’ uses context or TF-IDF vectors with cosine similarity\n",
    "\n",
    "    window_size            = 20 # Context window size for co-occurrence\n",
    "    num_words              = 25 # Max number of top frequent words to analyze\n",
    "    min_word_frequency     = 2 # Ignore words that appear fewer times\n",
    "    reuse_clusterings      = False # Whether to reuse saved clustering results if available\n",
    "\n",
    "\n",
    "    # Preprocessing Filters\n",
    "    cross_pos_normalize    = True # Normalize words across parts of speech (e.g., \"learn\", \"learning\", \"learned\" -> \"learn\")\n",
    "    projects               = [\"oral_history\"]     # Filter by project names \n",
    "    data_groups            = [\"interview\"]   # Filter by data_groups \n",
    "    codes                  = [\"background\"]        # Analyse specific codes\n",
    "    excluded_codes         = ['interviewer']  # Exclude these codes, removing 'interviewer' is important for NLP\n",
    "\n",
    "    # Visualisation\n",
    "    title                = \"Semantic Network (all interviews, contextual embeddings)\"\n",
    "    link_threshold       = 0.50          \n",
    "    link_color_threshold = 0.75          # set to 99 to remove black links\n",
    "    custom_colors = True                 \n",
    "\n",
    "    # Seeds & Colours\n",
    "    seed_words = \"education: learning, teaching, student, school, classroom, curriculum, academic\"\n",
    "\n",
    "    # You can provide seed words in two formats:\n",
    "    # 1. Grouped format (for custom node colors and labels):\n",
    "    #       \"GroupName1: word1, word2; GroupName2: word3, word4; ...\"\n",
    "    #    Example: \n",
    "    #       \"Therapy: therapy, physical therapy; Caregivers: caregivers, caregiver; family\"\n",
    "    #    â†’ 'family' will be treated as its own group, using its own color.\n",
    "    #\n",
    "    # 2. Flat list format (no grouping, default color):\n",
    "    #       \"dementia, therapy, caregivers\"\n",
    "    #    â†’ All words will appear as individual highlighted nodes with default styling.\n",
    "    \n",
    "    # consider inductive readings, and analysis \n",
    "    cmap = cm.get_cmap(\"mako\", 5)\n",
    "    semantic_categories = {\n",
    "    \"People & Relations\": {\n",
    "        \"words\": {\n",
    "            \"people\", \"student\", \"students\", \"teacher\", \"teachers\", \"professor\", \"professors\",\n",
    "            \"advisor\", \"mentor\", \"classmates\", \"friends\", \"colleague\", \"colleagues\",\n",
    "            \"mother\", \"mom\", \"father\", \"dad\", \"family\", \"kids\", \"children\",\n",
    "            \"wife\", \"husband\", \"brother\", \"sister\"\n",
    "        },\n",
    "        \"color\": cmap(0)\n",
    "    },\n",
    "    \"Education & Career\": {\n",
    "        \"words\": {\n",
    "            \"school\", \"college\", \"university\", \"department\", \"campus\",\n",
    "            \"course\", \"courses\", \"class\", \"classes\", \"curriculum\", \"degree\",\n",
    "            \"major\", \"minor\", \"graduate\", \"graduated\", \"thesis\", \"exam\",\n",
    "            \"research\", \"lab\", \"laboratory\", \"project\", \"projects\",\n",
    "            \"engineering\", \"engineer\", \"physics\", \"mathematics\",\n",
    "            \"industry\", \"company\", \"career\", \"job\", \"internship\"\n",
    "        },\n",
    "        \"color\": cmap(1)\n",
    "    },\n",
    "    \"Emotions & Cognition\": {\n",
    "        \"words\": {\n",
    "            \"think\", \"thought\", \"know\", \"understand\", \"learn\", \"learning\",\n",
    "            \"decide\", \"decision\", \"believe\", \"remember\", \"idea\", \"ideas\",\n",
    "            \"feel\", \"feeling\", \"feelings\", \"excited\", \"interested\",\n",
    "            \"curious\", \"happy\", \"proud\", \"worried\", \"scared\", \"confused\",\n",
    "            \"reflect\", \"contemplate\", \"ponder\", \"analyze\", \"reason\"\n",
    "        },\n",
    "        \"color\": cmap(2)\n",
    "    },\n",
    "    \"Time / Duration\": {\n",
    "        \"words\": {\n",
    "            \"first\", \"second\", \"later\", \"since\", \"before\", \"after\",\n",
    "            \"started\", \"start\", \"begin\", \"early\", \"late\",\n",
    "            \"day\", \"days\", \"week\", \"weeks\", \"month\", \"months\",\n",
    "            \"year\", \"years\", \"semester\", \"summer\", \"winter\", \"spring\", \"fall\"\n",
    "        },\n",
    "        \"color\": cmap(3)\n",
    "    },\n",
    "    \"Daily Activities & Work\": {\n",
    "        \"words\": {\n",
    "            \"work\", \"working\", \"teach\", \"teaching\", \"study\", \"studying\",\n",
    "            \"read\", \"reading\", \"write\", \"writing\", \"talk\", \"talking\",\n",
    "            \"meet\", \"meeting\", \"present\", \"presentation\", \"travel\", \"move\",\n",
    "            \"build\", \"design\", \"make\", \"made\", \"fix\", \"support\", \"help\",\n",
    "            \"eat\", \"sleep\", \"walk\", \"exercise\", \"commute\", \"cook\", \"clean\",\n",
    "            \"shop\", \"plan\", \"organize\", \"schedule\", \"prepare\", \"attend\"\n",
    "        },\n",
    "        \"color\": cmap(4)\n",
    "    },\n",
    "}\n",
    "     # Network Layout Parameters\n",
    "    network_layout = \"kamada-kawai\"   # Options: \"spring\", \"kamada-kawai\", \"circular\", \"shell\"\n",
    "    \n",
    "    # Heatmap Parameters\n",
    "    num_codes    = 7              # Expand up to this many codes for heatmap\n",
    "    seed_codes   = [\"background\"] # Start from these seed codes (if None, top-N most frequent codes are used)\n",
    "    clustered    = True           # If True, apply hierarchical clustering to rows/columns\n",
    "\n",
    "    # Notes:\n",
    "    # - If seed_codes is given â†’ heatmap expands with top-N co-occurring codes. \n",
    "    # - If seed_codes is None   â†’ heatmap uses top-N most frequent codes in corpus. \n",
    "    # - num_codes defines N in both cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace7f320",
   "metadata": {},
   "source": [
    "## Full Visuals Version\n",
    "\n",
    "Heatmap + tSNE + Social Semantic Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc57ad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSemantic Network Analysis Tool\\n\" + \"=\" * 30)\n",
    "\n",
    "try:\n",
    "    # Build pydantic object\n",
    "    params = VisualsInput(\n",
    "        filepath            = csv_path,\n",
    "        stop_list           = stop_list_path if use_custom_stoplist else None,\n",
    "        num_words           = num_words,\n",
    "        clustering_method   = clustering_method,\n",
    "        distance_metric     = distance_metric,\n",
    "        window_size         = window_size,\n",
    "        min_word_frequency  = min_word_frequency,\n",
    "        projects            = projects,\n",
    "        data_groups         = data_groups,\n",
    "        codes               = codes,\n",
    "        cross_pos_normalize = cross_pos_normalize\n",
    "    )\n",
    "\n",
    "        \n",
    "\n",
    "    # Attach non-schema extras\n",
    "    setattr(params, \"reuse_clusterings\",    reuse_clusterings)\n",
    "    setattr(params, \"seed_words\",           seed_words)\n",
    "    setattr(params, \"custom_colors\",        custom_colors)\n",
    "    setattr(params, \"semantic_categories\",  semantic_categories)\n",
    "    setattr(params, \"link_threshold\",       link_threshold)\n",
    "    setattr(params, \"link_color_threshold\", link_color_threshold)\n",
    "    setattr(params, \"excluded_codes\",       excluded_codes)    \n",
    "\n",
    "\n",
    "    # Creates: (1) plain, (2) coloured, (3) coloured+seed-hidden\n",
    "    run_visuals_pipeline(params)\n",
    "\n",
    "\n",
    "except ValidationError as ve:\n",
    "    print(\"\\nâš  Parameter error:\\n\", ve)\n",
    "except Exception as e:\n",
    "    print(f\"\\nâš  Unexpected error: {e}\")\n",
    "finally:\n",
    "    print(\"\\nAnalysis process completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f982c74",
   "metadata": {},
   "source": [
    "## Interactive Version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebb8936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "OVERVIEW:\n",
    "----------------------\n",
    "This tool allows you to generate a semantic network based on word embeddings \n",
    "or co-occurrence similarity. It supports multiple clustering methods (RoBERTa, \n",
    "Jaccard, PMI, TF-IDF), customizable filters, and dynamic seed word grouping.\n",
    "\n",
    "USAGE INSTRUCTIONS:\n",
    "----------------------\n",
    "\n",
    "1. CSV File Selection:\n",
    "   - On launch, the tool will ask whether to use the last CSV file you worked with.\n",
    "   - If not, you can provide a new path or use the default.\n",
    "\n",
    "2. Reuse Previous Parameters:\n",
    "   - Youâ€™ll be asked whether to reuse previous analysis settings.\n",
    "   - If 'y' and a config exists, it will skip all prompts and directly run using cached data.\n",
    "   - If 'n' or no config exists, you'll go through the full interactive setup.\n",
    "\n",
    "3. Stop Word List (Optional):\n",
    "   - You can load a custom stop words file (one word per line).\n",
    "   - If not provided, the default list will be used.\n",
    "\n",
    "4. Clustering Method & Parameters:\n",
    "   - Choose from: 1 = RoBERTa, 2 = Jaccard, 3 = PMI, 4 = TF-IDF.\n",
    "   - For methods 2â€“4, specify distance metric (default or cosine) and context window size.\n",
    "\n",
    "5. Analysis Filters:\n",
    "   - Specify the number of words to include, minimum frequency, and metadata filters\n",
    "     (projects, data groups, codes).\n",
    "\n",
    "6. Seed Words:\n",
    "   - You may enter structured seed groups (e.g. \"Grief: grief, sorrow; Memory: memory, recall\")\n",
    "     or leave blank to use the default.\n",
    "\n",
    "7. Output:\n",
    "   - The tool generates:\n",
    "     - A semantic network (with and without seed nodes)\n",
    "     - A similarity heatmap\n",
    "     - A t-SNE plot\n",
    "   - All visualizations are saved to the `OUTPUT_DIR` directory.\n",
    "\n",
    "Note:\n",
    "- The system caches results based on parameter combinations and data hash.\n",
    "- Settings are saved to 'last_run_config.json' and CSV path to 'last_csv_path.txt'.\n",
    "\n",
    "\"\"\"\n",
    "    try:\n",
    "        print(\"Semantic Network Analysis Tool\")\n",
    "        print(\"==============================\")\n",
    "\n",
    "        # --- Default configuration ---\n",
    "        default_csv_path           = CSV_PATH\n",
    "        default_stop_list_path     = STOP_LIST_FILE\n",
    "        default_window_size        = 20\n",
    "        default_num_words          = 25\n",
    "        default_min_word_frequency = 2\n",
    "        default_excluded_codes    = ['interviewer'] # Exclude these codes, removing 'interviewer' is important for NLP\n",
    "        default_seed_words         = \"education: learning, teaching, student, school, classroom, curriculum, academic\"\n",
    "        cmap = cm.get_cmap(\"mako\", 5)\n",
    "        default_semantic_categories = {\n",
    "    \"People & Relations\": {\n",
    "        \"words\": {\n",
    "            \"people\", \"student\", \"students\", \"teacher\", \"teachers\", \"professor\", \"professors\",\n",
    "            \"advisor\", \"mentor\", \"classmates\", \"friends\", \"colleague\", \"colleagues\",\n",
    "            \"mother\", \"mom\", \"father\", \"dad\", \"family\", \"kids\", \"children\",\n",
    "            \"wife\", \"husband\", \"brother\", \"sister\"\n",
    "        },\n",
    "        \"color\": cmap(0)\n",
    "    },\n",
    "    \"Education & Career\": {\n",
    "        \"words\": {\n",
    "            \"school\", \"college\", \"university\", \"department\", \"campus\",\n",
    "            \"course\", \"courses\", \"class\", \"classes\", \"curriculum\", \"degree\",\n",
    "            \"major\", \"minor\", \"graduate\", \"graduated\", \"thesis\", \"exam\",\n",
    "            \"research\", \"lab\", \"laboratory\", \"project\", \"projects\",\n",
    "            \"engineering\", \"engineer\", \"physics\", \"mathematics\",\n",
    "            \"industry\", \"company\", \"career\", \"job\", \"internship\"\n",
    "        },\n",
    "        \"color\": cmap(1)\n",
    "    },\n",
    "    \"Emotions & Cognition\": {\n",
    "        \"words\": {\n",
    "            \"think\", \"thought\", \"know\", \"understand\", \"learn\", \"learning\",\n",
    "            \"decide\", \"decision\", \"believe\", \"remember\", \"idea\", \"ideas\",\n",
    "            \"feel\", \"feeling\", \"feelings\", \"excited\", \"interested\",\n",
    "            \"curious\", \"happy\", \"proud\", \"worried\", \"scared\", \"confused\"\n",
    "        },\n",
    "        \"color\": cmap(2)\n",
    "    },\n",
    "    \"Time / Duration\": {\n",
    "        \"words\": {\n",
    "            \"first\", \"second\", \"later\", \"since\", \"before\", \"after\",\n",
    "            \"started\", \"start\", \"begin\", \"early\", \"late\",\n",
    "            \"day\", \"days\", \"week\", \"weeks\", \"month\", \"months\",\n",
    "            \"year\", \"years\", \"semester\", \"summer\", \"winter\", \"spring\", \"fall\"\n",
    "        },\n",
    "        \"color\": cmap(3)\n",
    "    },\n",
    "    \"Daily Activities & Work\": {\n",
    "        \"words\": {\n",
    "            \"work\", \"working\", \"teach\", \"teaching\", \"study\", \"studying\",\n",
    "            \"read\", \"reading\", \"write\", \"writing\", \"talk\", \"talking\",\n",
    "            \"meet\", \"meeting\", \"present\", \"presentation\", \"travel\", \"move\",\n",
    "            \"build\", \"design\", \"make\", \"made\", \"fix\", \"support\", \"help\"\n",
    "        },\n",
    "        \"color\": cmap(4)\n",
    "    },\n",
    "}\n",
    "\n",
    "        default_title  = \"Semantic Network: (contextual embeddings)\"\n",
    "        default_link_threshold     = 0.5\n",
    "        default_link_color_thresh  = 0.75\n",
    "        \n",
    "        # Check for previous CSV file\n",
    "        previous_csv_path = \"\"\n",
    "        if os.path.exists(LAST_CSV_PATH):\n",
    "            try:\n",
    "                with open(LAST_CSV_PATH, 'r') as f:\n",
    "                    previous_csv_path = f.read().strip()\n",
    "                if os.path.exists(previous_csv_path):\n",
    "                    use_last = ask_yes_no(f\"Use previous CSV file? ({previous_csv_path})\", default=False)\n",
    "                    if use_last:\n",
    "                        csv_path = previous_csv_path\n",
    "                    else:\n",
    "                        csv_path = ask_path(f\"Enter CSV file path\", default_csv_path)\n",
    "                else:\n",
    "                    print(f\"Previous CSV file not found: {previous_csv_path}\")\n",
    "                    csv_path = ask_path(f\"Enter CSV file path\", default_csv_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading previous CSV path: {e}\")\n",
    "                csv_path = ask_path(f\"Enter CSV file path\", default_csv_path)\n",
    "        else:\n",
    "            csv_path = ask_path(f\"Enter CSV file path\", default_csv_path)\n",
    "\n",
    "        \n",
    "        # Validate CSV path\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"Error: File {csv_path} not found\")\n",
    "            exit(1)\n",
    "\n",
    "        # Save the current CSV path for next time\n",
    "        try:\n",
    "            with open(LAST_CSV_PATH, 'w') as f:\n",
    "                f.write(csv_path)\n",
    "            print(f\"Saved current CSV path for future use: {csv_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save CSV path for future use: {e}\")\n",
    "\n",
    "        # Ask about stop words\n",
    "        use_stop_list = ask_yes_no(f\"Use a custom stop-words file? (default path shown: {default_stop_list_path})\", default=False)\n",
    "        stop_list_path = None\n",
    "        if use_stop_list:\n",
    "            stop_list_path = ask_path(\"Enter STOP words file path\", default_stop_list_path)\n",
    "            # Validate stop list path\n",
    "            if stop_list_path and not os.path.exists(stop_list_path):\n",
    "                print(f\"Warning: Stop words file {stop_list_path} not found. Using default stop words only.\")\n",
    "                stop_list_path = None\n",
    "            else:\n",
    "                # Debug stoplist\n",
    "                with open(stop_list_path, 'r') as f:\n",
    "                    custom_stopwords = [line.strip() for line in f if line.strip()]\n",
    "                print(f\"Loaded {len(custom_stopwords)} custom stopwords\")\n",
    "                print(f\"First 10 stopwords: {custom_stopwords[:10]}\")\n",
    "                print(f\"Stoplist file size: {os.path.getsize(stop_list_path)} bytes\")\n",
    "\n",
    "        reuse_clusterings = ask_yes_no(\"Reuse previous clusterings and parameters?\", default=True)\n",
    "\n",
    "        if reuse_clusterings: # reuse from the cache\n",
    "            if os.path.exists(LAST_CONFIG_PATH):\n",
    "                print(\"Loading previous parameters...\")\n",
    "                with open(LAST_CONFIG_PATH, 'r') as f:\n",
    "                    saved_params = json.load(f)\n",
    "                # Reconstruct VisualsInput from saved dictionary\n",
    "                params = VisualsInput(**saved_params)\n",
    "                setattr(params, \"custom_colors\", True)\n",
    "                setattr(params, \"link_threshold\", default_link_threshold)\n",
    "                setattr(params, \"link_color_threshold\", default_link_color_thresh)\n",
    "                run_visuals_pipeline(params)\n",
    "                return  # Exit early since pipeline is already run\n",
    "        else:\n",
    "            print(\"No saved parameter file found. Proceeding to manual input...\")\n",
    "            while True:\n",
    "                try:\n",
    "                    clustering_method = int(\n",
    "                        input(\"Choose clustering method 1=RoBERTa, 2=Jaccard, 3=PMI, 4=TF-IDF [Default 1]: \").strip() or \"1\"\n",
    "                        )\n",
    "                    if clustering_method in (1, 2, 3, 4):\n",
    "                        break\n",
    "                    print(\"Please enter 1-4\")\n",
    "                except ValueError:\n",
    "                    print(\"Please enter valid number\")\n",
    "\n",
    "            # distance metric & window\n",
    "            distance_metric = \"default\"\n",
    "            window_size = default_window_size\n",
    "            if clustering_method in (2, 3, 4):\n",
    "                metric_choice = input(\"Distance metric 1=Default, 2=Cosine [Default 1]: \").strip()\n",
    "                distance_metric = \"cosine\" if metric_choice == \"2\" else \"default\"\n",
    "                while True:\n",
    "                    try:\n",
    "                        window_size = int(\n",
    "                            ask_path(f\"Window size for co-occurrence\", default_window_size)\n",
    "                            )\n",
    "                        if window_size > 0:\n",
    "                            break\n",
    "                        print(\"Window size must be positive.\")\n",
    "                    except ValueError:\n",
    "                        print(\"Please enter valid number\")\n",
    "\n",
    "            # other analysis params\n",
    "            num_words = int(ask_path(f\"How many words to analyze?\", default_num_words))\n",
    "            min_word_frequency = int(ask_path(f\"Min word frequency\", default_min_word_frequency))\n",
    "            cross_pos_normalize = ask_yes_no(\"Normalize words across POS?\", default=True)\n",
    "            seed_input = ask_path(\"Seed words (blank for default): \", default_seed_words)\n",
    "\n",
    "            # filters\n",
    "            projects_input = input(f\"Projects comma-sep (Enter/space for skip): \").strip()\n",
    "            projects = [p.strip() for p in projects_input.split(\",\") if p.strip()] if projects_input else None\n",
    "            data_groups_input = input(\"Data groups comma-sep (Enter/space for skip): \").strip()\n",
    "            data_groups = [g.strip() for g in data_groups_input.split(\",\") if g.strip()] if data_groups_input else None\n",
    "            codes_input = input(\"Codes comma-sep (Enter/space for skip): \").strip()\n",
    "            codes = [c.strip() for c in codes_input.split(\",\") if c.strip()] if codes_input else None\n",
    "            excluded_codes_input = input(\"Codes you want to exclude from analysis comma-sep (Enter/space for skip): \").strip()\n",
    "            excluded_codes = [e.strip() for e in excluded_codes_input.split(\",\") if e.strip()] if excluded_codes_input else default_excluded_codes\n",
    "\n",
    "            # Consider inductive readings, and analysis \n",
    "            print(\n",
    "                \"\\nâ–¶ OPTIONAL â€” define semantic colour groups.\\n\"\n",
    "                \"  Format per group:  GroupName:{'color':'#HEX','words':[w1,w2]}\\n\"\n",
    "                \"  Separate multiple groups with a semicolon â€˜;â€™ on one line.\\n\"\n",
    "                \"  Example:\\n\"\n",
    "                \"    Health:{'color':'#F8961E','words':['health','clinician']};\\n\"\n",
    "                \"    Roles :{'color':'#43AA8B','words':['parent','caregiver']}\\n\"\n",
    "                \"  Press <Enter> to keep the default shown below.\\n\"\n",
    "                f\"  Default = {default_semantic_categories}\\n\"\n",
    "            )\n",
    "\n",
    "            raw_sc = input(\"semantic_categories â†’ \").strip()\n",
    "\n",
    "            if not raw_sc:                                   # user kept the defaults\n",
    "                semantic_categories = default_semantic_categories.copy()\n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    semantic_categories = {}\n",
    "                    # allow several groups separated by ';'\n",
    "                    for block in filter(None, map(str.strip, raw_sc.split(';'))):\n",
    "                        if ':' not in block:\n",
    "                            raise ValueError(f\"missing ':' in Â«{block}Â»\")\n",
    "\n",
    "                        label, dict_part = map(str.strip, block.split(':', 1))\n",
    "\n",
    "                        # try JSON first, then Python literal\n",
    "                        try:\n",
    "                            cat_dict = json.loads(dict_part)\n",
    "                        except json.JSONDecodeError:\n",
    "                            cat_dict = ast.literal_eval(dict_part)\n",
    "\n",
    "                        # minimal validation\n",
    "                        if not isinstance(cat_dict, dict) or \\\n",
    "                        'color' not in cat_dict or 'words' not in cat_dict:\n",
    "                            raise ValueError(\n",
    "                                f\"Group â€˜{label}â€™ must contain 'color' and 'words' keys\"\n",
    "                            )\n",
    "\n",
    "                        # normalise words to lower-case for matching\n",
    "                        cat_dict['words'] = [w.lower() for w in cat_dict['words']]\n",
    "                        semantic_categories[label] = cat_dict\n",
    "\n",
    "                    if not semantic_categories:\n",
    "                        raise ValueError(\"no valid groups parsed\")\n",
    "\n",
    "                except Exception as err:\n",
    "                    print(\"âš   Could not parse your custom groups. \"\n",
    "                        \"Falling back to the default set.\\n\", err)\n",
    "        semantic_categories = default_semantic_categories.copy()\n",
    "\n",
    "        print(\"\\nRunning analysis...\\n\")\n",
    "\n",
    "        params = VisualsInput(\n",
    "            filepath=csv_path,\n",
    "            stop_list=stop_list_path,\n",
    "            num_words=num_words,\n",
    "            clustering_method=clustering_method,\n",
    "            distance_metric=distance_metric,\n",
    "            window_size=window_size,\n",
    "            min_word_frequency=min_word_frequency,\n",
    "            projects=projects,\n",
    "            data_groups= data_groups,\n",
    "            codes=codes,\n",
    "            cross_pos_normalize=cross_pos_normalize\n",
    "        )\n",
    "\n",
    "        setattr(params, \"reuse_clusterings\", reuse_clusterings)\n",
    "        setattr(params, \"seed_words\", seed_input)\n",
    "        setattr(params, \"custom_colors\", True)\n",
    "        setattr(params, \"semantic_categories\", semantic_categories)\n",
    "        setattr(params, \"link_threshold\", default_link_threshold)\n",
    "        setattr(params, \"link_color_threshold\", default_link_color_thresh)\n",
    "        setattr(params, \"excluded_codes\", excluded_codes)\n",
    "\n",
    "        try:\n",
    "            with open(LAST_CONFIG_PATH, 'w') as f:\n",
    "                json.dump(params.__dict__, f, indent=2)\n",
    "            print(f\"Parameters saved to {LAST_CONFIG_PATH}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to save parameters: {e}\")\n",
    "\n",
    "        run_visuals_pipeline(params)\n",
    "\n",
    "    except ValidationError as ve:\n",
    "        print(\"\\nâš  Parameter validation error:\\n\", ve)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâš  Unexpected error: {e}\")\n",
    "    finally:\n",
    "        print(\"\\nAnalysis process completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536568e4-3b86-4507-852a-9bee92d6d18e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddb3204-ce24-4895-8c23-ed640eb6e407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d443a3f1-86ae-4d39-bd87-9781538cd388",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmap_visualization_toolkit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
