{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d42a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b250ddb3",
   "metadata": {},
   "source": [
    "# Visualization Toolkit Usage Guide\n",
    "\n",
    "This notebook provides a suite of semantic analysis and visualization tools to explore word-level patterns and relationships in qualitative text data. It supports multiple clustering and embedding strategies for t-SNE, semantic networks, and word clouds.\n",
    "\n",
    "### What You Can Do with This Toolkit\n",
    "\n",
    "- **Generate Word Clouds**  \n",
    "  Visualize the most frequent and salient terms across your dataset or within filtered subsets based on keywords or project name.\n",
    "\n",
    "- **Plot t-SNE Semantic Maps**  \n",
    "  Reduce high-dimensional similarity or co-occurrence matrices into 2D using t-SNE to highlight semantic proximity between words. Seed words are emphasized to anchor interpretation.\n",
    "\n",
    "- **Create Word-Based Heatmaps and Semantic Networks**  \n",
    "  Explore how terms relate to each other in both visual space and co-occurrence structure:\n",
    "  \n",
    "  - **Basic Heatmap**: Highlights semantically clustered keywords based on selected embeddings or similarity matrices.\n",
    "  - **Heatmap + Network (Black & White)**: Adds a basic network graph on top of the heatmap using default node/edge colors (no bundling, no category styling).\n",
    "  - **Heatmap + Network (Colored Nodes & Edges)**: Fully stylized version including colored clusters, semantic links, and optional edge styling.\n",
    "\n",
    "- **Visualize Code Co-Occurrence Heatmaps**  \n",
    "  Plot the frequency with which qualitative codes appear together in the same entries. \n",
    "\n",
    "These tools help you visually investigate language patterns, conceptual clustering, and topic proximity—whether you’re doing grounded theory, thematic analysis, or exploratory semantic mapping.\n",
    "\n",
    "### How to Use\n",
    "1. **Prepare Your Dataset**  \n",
    "   Make sure your dataset is a `.csv` or DataFrame with at least:\n",
    "   - A `text` column (raw text)\n",
    "   - Optionally, a `project` column (for subsetting)\n",
    "   - optionally a `codes' column` (for subsetting, and analysis)\n",
    "   - Please see READ.md for the complete schema and more possibilities.\n",
    "\n",
    "2. **Set Key Parameters**  \n",
    "   Most functions accept:\n",
    "   - `stopwords_path`: path to extra stopwords (optional)\n",
    "   - `clustering_method`: 1 = RoBERTa, 2 = Jaccard, 3 = PMI, 4 = TF-IDF\n",
    "   - `distance_metric`: \"cosine\" or \"default\" (used for similarity matrix choice)\n",
    "\n",
    "3. **Run Visualizations**\n",
    "You can explore and generate a variety of visual outputs using the execution blocks provided in the **Analytics Tools** and **Advanced Analytics Tools** sections.\n",
    "\n",
    "These include:\n",
    "- **Word Clouds** for highlighting high-frequency terms in selected texts or projects.\n",
    "- **t-SNE Semantic Maps** to project word relationships into 2D space for visualizing proximity and clusters.\n",
    "- **Word-Based Heatmaps** to show how frequently words co-occur.\n",
    "- **Semantic Network Graphs** (with optional heatmaps), including:\n",
    "  - show relationships in text\n",
    "  - basic  node/seed networks,\n",
    "  - networks with customized node colors,\n",
    "  - and networks with or without edge bundling.\n",
    "- **Code-based Heatmap** to visualize co-occurrence patterns among qualitative codes\n",
    "\n",
    "📍 Simply scroll down to the relevant execution cells to run and customize these visualizations based on your dataset.\n",
    "\n",
    "### File Structure Notes\n",
    "- Define constants like `DATA_DIR`, `OUTPUT_DIR`, and stopword files before running.\n",
    "- Ensure required files and embeddings are preloaded or generated using prior pipeline steps.\n",
    "\n",
    "> Tip: Start by testing on one project or topic before scaling to all data.\n",
    "\n",
    "# TESTING, PLEASE DO NOT SHARE. CITE WITHOUT WRITTEN PERMISSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1316a766",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294f11c0",
   "metadata": {},
   "source": [
    "### Packages Loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee1d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python built-ins\n",
    "# Using Python 3.11.13\n",
    "import os\n",
    "import urllib.request\n",
    "from functools import lru_cache\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import ast\n",
    "import sys\n",
    "import platform\n",
    "import importlib\n",
    "\n",
    "# Data loading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Natural Language Processing (NLP)\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure required NLTK resources are available\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt_tab\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt_tab\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"corpora/stopwords\")\n",
    "except LookupError:\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"corpora/wordnet\")\n",
    "except LookupError:\n",
    "    nltk.download(\"wordnet\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"taggers/averaged_perceptron_tagger\")\n",
    "except LookupError:\n",
    "    nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "# Sentence / transformer embeddings\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Machine Learning / Math\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "import scipy.cluster.hierarchy as hierarchy\n",
    "import torch\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import matplotlib.cm as cm\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image, ImageDraw\n",
    "import pprint\n",
    "\n",
    "# Validation & helpers\n",
    "from pydantic import BaseModel, FilePath, ValidationError, field_validator\n",
    "from typing import List, Optional, Union, Any\n",
    "import nbimporter  # import from notebook\n",
    "# Project-Local Modules\n",
    "# ⚠️ Environment Warning\n",
    "# To successfully import local Python modules (e.g., `vis_tool_core.py`)\n",
    "# into this Jupyter notebook, ensure that the `.py` file is located in the **same directory**\n",
    "# as this notebook.\n",
    "import warnings \n",
    "\n",
    "from function import vis_tool_core\n",
    "# Force reload the module to apply any code changes\n",
    "importlib.reload(vis_tool_core)\n",
    "from function.vis_tool_core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66067b34",
   "metadata": {},
   "source": [
    "#### Version Check\n",
    "\n",
    "This section checks current version of packages loaded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d97b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version_check.py \n",
    "import platform, importlib\n",
    "from packaging.version import Version\n",
    "from packaging.specifiers import SpecifierSet\n",
    "\n",
    "SUPPORTED = {\n",
    "    \"python\": \">=3.10,<3.13.2\",\n",
    "    \"numpy\": \">=1.24,<2.2\",\n",
    "    \"pandas\": \">=1.5,<2.3\",\n",
    "    \"nltk\": \">=3.8,<4\",\n",
    "    \"gensim\": \">=4.3,<5\",\n",
    "    \"sklearn\": \">=1.2,<1.5\",\n",
    "    \"sentence_transformers\": \">=2.2,<3.6\",\n",
    "    \"transformers\": \">=4.36,<5.1\",\n",
    "    \"torch\": \">=2.1,<2.8\",\n",
    "    \"matplotlib\": \">=3.7,<4\",\n",
    "    \"seaborn\": \">=0.12,<1\",\n",
    "    \"networkx\": \">=3,<4\",\n",
    "    \"wordcloud\": \">=1.9,<2\",\n",
    "    \"dash\": \">=2.10,<3\",\n",
    "    \"plotly\": \">=5.0,<6\",  \n",
    "    \"tqdm\": \">=4.65,<5\",\n",
    "    \"joblib\": \">=1.2,<2\",     \n",
    "    \"dill\": \">=0.3.6,<0.4\",\n",
    "    \"python-dotenv\": \">=0.15,<2\",\n",
    "    \"pydantic\": \">=1.10,<3\",\n",
    "}\n",
    "print(f\"\\n Environment Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "if not check_versions(SUPPORTED):\n",
    "    print(\"Please align your environment to the required versions above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e794c1a",
   "metadata": {},
   "source": [
    "### Environment Configuration\n",
    "\n",
    "In this section, we define the core directory structure used throughout the replication project. These paths help organize:\n",
    "\n",
    "- **raw data**,  \n",
    "- **trained models**,  \n",
    "- **clustering results**, and  \n",
    "- **final outputs**.\n",
    "\n",
    "This setup also makes it easier for users to customize where intermediate results and final outputs will be saved. For example, by changing these directory names, users can **create their own versions of model runs or clustering outputs without overwriting previous results.**\n",
    "\n",
    "All directories will be automatically created if they don't already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2546e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"input\", exist_ok=True)\n",
    "\n",
    "# Define base directory\n",
    "BASE_DIR = os.getcwd()\n",
    "\n",
    "# Define project directories\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "INPUT_DIR = os.path.join(BASE_DIR, \"input\")\n",
    "BACKUP_DIR = os.path.join(BASE_DIR, \"backup\")\n",
    "MODEL_DIR = os.path.join(DATA_DIR, \"models\", \"auto_model\")\n",
    "CLUSTERING_DIR = os.path.join(DATA_DIR, \"models\", \"clusterings\")\n",
    "OUTPUT_DIR = os.path.join(DATA_DIR, \"outputs\")\n",
    "LAST_CSV_PATH = os.path.join(OUTPUT_DIR, \"last_csv_path.txt\")\n",
    "\n",
    "# Paths\n",
    "CSV_PATH = os.path.join(DATA_DIR, \"data.csv\")\n",
    "STOP_LIST_FILE = os.path.join(INPUT_DIR, \"additional_stops.txt\")\n",
    "\n",
    "# Cache \n",
    "LAST_CONFIG_PATH = \"last_run_config.json\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for d in (BACKUP_DIR, MODEL_DIR, CLUSTERING_DIR, OUTPUT_DIR):\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# Initialize NLTK components\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Initialize lemmatizer (only once)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Torch Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# HuggingFace token for private models\n",
    "load_dotenv()\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\") or \"\" #this is free to get, but this might be packageable with one of the BERT models. Currently using roBERTa, but the distilled version is doable.\n",
    "\n",
    "# Enable GPU acceleration if available\n",
    "USE_GPU_ACCELERATION = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\") #not optimized for MPS on mac, but should access.\n",
    "\n",
    "# Define and load the preferred model\n",
    "# Use 'roberta-base' for tests (smaller, faster), and switch back to 'all-roberta-large-v1' for full runs\n",
    "MODEL_NAME = \"roberta-base\"  \n",
    "\n",
    "print(f\"Loading '{MODEL_NAME}'...\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# Load HuggingFace model\n",
    "# ⚠️ Note on add_pooling_layer:\n",
    "# - Plain RoBERTa checkpoints (e.g., \"roberta-base\", \"roberta-large\") do NOT include a pooling layer.\n",
    "#   If you load them with the default settings, HuggingFace will create a random pooler and issue a warning.\n",
    "#   To avoid this, we explicitly set add_pooling_layer=False for these cases.\n",
    "# - For Sentence-Transformers models (e.g., \"all-roberta-large-v1\"), KEEP the default (pooling layer included).\n",
    "#   These models rely on pooling for producing sentence embeddings.\n",
    "#\n",
    "# So: \n",
    "#   MODEL = AutoModel.from_pretrained(MODEL_NAME, add_pooling_layer=False)  # for roberta-base / roberta-large\n",
    "#   MODEL = AutoModel.from_pretrained(MODEL_NAME)                          # for embedding models like all-roberta-large-v1\n",
    "\n",
    "MODEL = AutoModel.from_pretrained(MODEL_NAME, add_pooling_layer=False)\n",
    "MAX_TOKENS = TOKENIZER.model_max_length\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"Some weights of RobertaModel were not initialized\"\n",
    ")\n",
    "\n",
    "# Move model to the appropriate device\n",
    "MODEL.to(device)\n",
    "\n",
    "# Optional: Enable CUDA optimizations for better performance on NVIDIA GPUs\n",
    "if device.type == 'cuda':\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "    print(f\"CUDA memory reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "\n",
    "print(f\"Loaded '{MODEL_NAME}' on {device}\")\n",
    "print(f\"Maximum length of tokens is {MAX_TOKENS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc74655",
   "metadata": {},
   "source": [
    "### Stopword Expansion and Semantic Word Family Definitions\n",
    "\n",
    "This section defines a comprehensive list of stopwords, extending NLTK’s default stopword set with:\n",
    "- **punctuation**, \n",
    "- **contractions**, \n",
    "- **common filler words**, and \n",
    "- **project-specific conversational terms** that are semantically uninformative in analysis.\n",
    "\n",
    "We also build a custom `WORD_FAMILIES` dictionary, which groups related words into unified concepts (e.g., \"death\", \"caregiver\", \"memory\"). This allows the model to:\n",
    "- **compress synonyms and variations** into semantically meaningful units,\n",
    "- **reduce noise** in the embedding space,\n",
    "- and **support cultural/qualitative interpretation** of the results.\n",
    "\n",
    "This section also includes validation checks to:\n",
    "- Ensure **no accidental overlaps** between stopwords and key analytical terms,\n",
    "- Detect **redundant words across families**, \n",
    "- And print summaries for user verification.\n",
    "\n",
    "These definitions are critical for interpretability in downstream visualization and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536db12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base stop words from NLTK and punctuation\n",
    "default_stop_words = set(stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "# Common contractions and special characters\n",
    "common_special_chars = {'...', \"''\", '\"\"', \"``\", \"--\", \"n't\", \"'s\", '|'}\n",
    "default_stop_words.update(common_special_chars)\n",
    "\n",
    "# Additional stop words for conversation analysis\n",
    "additional_stops = {\n",
    "    # Common verbs that don't add semantic value\n",
    "    'would', 'could', 'may', 'also', 'one', 'like', 'get', 'well', \n",
    "    'many', 'much', 'even', 'said', 'say', 'says', 'see', 'seen',\n",
    "    'use', 'used', 'using', 'way', 'ways', 'make', 'makes', 'made',\n",
    "    'take', 'takes', 'took', 'taken', 'go', 'goes', 'going', 'went',\n",
    "    'come', 'comes', 'coming', 'came', 'try', 'tries', 'tried',\n",
    "    \n",
    "    # General placeholders\n",
    "    'thing', 'things', 'something', 'anything', 'everything',\n",
    "    'someone', 'anyone', 'everyone', 'somebody', 'anybody', 'everybody',\n",
    "    \n",
    "    # Filler words and conversational markers\n",
    "    'um', 'uh', 'hmm', 'oh', 'huh', 'uhhuh', 'yeah', 'okay',\n",
    "    'sorta', 'kinda', 'basically', 'literally', 'honestly', 'anyway',\n",
    "    'whatever', 'actually', 'really', 'just', 'pretty', 'right',\n",
    "    \n",
    "    # Common pronouns and contractions\n",
    "    'im', 'youre', 'shes', 'hes', 'theyre', 'ive', 'dont', 'cant',\n",
    "    'doesnt', 'didnt', 'thats', 'theres', 'heres', 'couldnt', 'shouldnt', \n",
    "    'wouldnt', 'lets', 'youve', 'weve', 'theyve', 'whats', 'whos', 'hows', \n",
    "    'wheres', 'gotta', 'gonna', 'wanna', 'aint', 'alot', 'isnt', 'wont',\n",
    "    \n",
    "    # Enhanced common words to exclude from auto-selection\n",
    "    'tell', 'told', 'right', 'lot', 'way', 'kind', 'bit', 'maybe', \n",
    "    'still', 'stuff', 'sure', 'getting', 'gets', 'goes', 'gone',\n",
    "    'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten',\n",
    "    'first', 'second', 'third', 'fourth', 'fifth', 'last', 'next',\n",
    "    'should', 'might', 'must', 'may', 'can', 'cannot',\n",
    "    'ah', 'wow', 'yes', 'no', 'nope', 'ok',\n",
    "    'hey', 'hi', 'hello', 'bye', 'goodbye', 'etc', 'etc.',\n",
    "    \n",
    "    # Single letters and 2 letter pairs\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "    'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "    'aa', 'ab', 'ac', 'ad', 'ae', 'af', 'ag', 'ah', 'ai', 'aj', 'ak', 'al', 'am',\n",
    "    'an', 'ao', 'ap', 'aq', 'ar', 'as', 'at', 'au', 'av', 'aw', 'ax', 'ay', 'az',\n",
    "    'ba', 'bb', 'bc', 'bd', 'be', 'bf', 'bg', 'bh', 'bi', 'bj', 'bk', 'bl', 'bm',\n",
    "    'bn', 'bo', 'bp', 'bq', 'br', 'bs', 'bt', 'bu', 'bv', 'bw', 'bx', 'by', 'bz',\n",
    "    'ca', 'cb', 'cc', 'cd', 'ce', 'cf', 'cg', 'ch', 'ci', 'cj', 'ck', 'cl', 'cm',\n",
    "    'cn', 'co', 'cp', 'cq', 'cr', 'cs', 'ct', 'cu', 'cv', 'cw', 'cx', 'cy', 'cz',\n",
    "    'da', 'db', 'dc', 'dd', 'de', 'df', 'dg', 'dh', 'di', 'dj', 'dk', 'dl', 'dm',\n",
    "    'dn', 'do', 'dp', 'dq', 'dr', 'ds', 'dt', 'du', 'dv', 'dw', 'dx', 'dy', 'dz',\n",
    "    'ea', 'eb', 'ec', 'ed', 'ee', 'ef', 'eg', 'eh', 'ei', 'ej', 'ek', 'el', 'em',\n",
    "    'en', 'eo', 'ep', 'eq', 'er', 'es', 'et', 'eu', 'ev', 'ew', 'ex', 'ey', 'ez',\n",
    "    'fa', 'fb', 'fc', 'fd', 'fe', 'ff', 'fg', 'fh', 'fi', 'fj', 'fk', 'fl', 'fm',\n",
    "    'fn', 'fo', 'fp', 'fq', 'fr', 'fs', 'ft', 'fu', 'fv', 'fw', 'fx', 'fy', 'fz',\n",
    "    'ga', 'gb', 'gc', 'gd', 'ge', 'gf', 'gg', 'gh', 'gi', 'gj', 'gk', 'gl', 'gm',\n",
    "    'gn', 'go', 'gp', 'gq', 'gr', 'gs', 'gt', 'gu', 'gv', 'gw', 'gx', 'gy', 'gz',\n",
    "    'ha', 'hb', 'hc', 'hd', 'he', 'hf', 'hg', 'hh', 'hi', 'hj', 'hk', 'hl', 'hm',\n",
    "    'hn', 'ho', 'hp', 'hq', 'hr', 'hs', 'ht', 'hu', 'hv', 'hw', 'hx', 'hy', 'hz',\n",
    "    'ia', 'ib', 'ic', 'id', 'ie', 'if', 'ig', 'ih', 'ii', 'ij', 'ik', 'il', 'im',\n",
    "    \n",
    "    # Project-specific stopwords from file\n",
    "    'a', 'um', 'an', 'the', 'have', 'dont', 'get', 'know', 'there', 'org', 'happen', 'find'\n",
    "}\n",
    "\n",
    "# Remove 'few', 'many', and 'more' from stop words as they're needed in word families\n",
    "# if 'few' in additional_stops:\n",
    "#     additional_stops.remove('few')\n",
    "# if 'many' in additional_stops:\n",
    "#     additional_stops.remove('many')\n",
    "# if 'more' in additional_stops:\n",
    "#     additional_stops.remove('more')\n",
    "# if 'few' in default_stop_words:\n",
    "#     default_stop_words.remove('few')\n",
    "# if 'many' in default_stop_words:\n",
    "#     default_stop_words.remove('many')\n",
    "# if 'more' in default_stop_words:\n",
    "#     default_stop_words.remove('more')\n",
    "\n",
    "# Update the default stop words with our additional list\n",
    "default_stop_words.update(additional_stops)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Normalized Word-Family Map\n",
    "# ------------------------------------------------\n",
    "\n",
    "\n",
    "# word families allow compression to move from embedding space (semantic), to meaning space (cultural); Basically, the idea is to use to get at more meaningful concepts and minimize false negatives for use with qual analyses (see Li and Abramson 2021; Abramson 2011)\n",
    "WORD_FAMILIES = {\n",
    "    \"education\": [\"college\", \"schooling\", \"graduate school\", \"university\"],\n",
    "    \"people\": [\"person\", \"student\", \"teacher\"],\n",
    "    \"dementia\": [\"dementia\"]\n",
    "}\n",
    "\n",
    "# Remove empty word families\n",
    "WORD_FAMILIES = {k: v for k, v in WORD_FAMILIES.items() if v}\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Summary and checks\n",
    "# ------------------------------------------------\n",
    "\n",
    "# Print summary information\n",
    "print(f\"Total stop words: {len(default_stop_words)}\")\n",
    "print(f\"Total word family compressions: {len(WORD_FAMILIES)}\")\n",
    "\n",
    "# Check for redundancies in stop words\n",
    "duplicate_stops = [item for item in additional_stops if item in default_stop_words and item not in additional_stops]\n",
    "if duplicate_stops:\n",
    "    print(f\"Warning: Found {len(duplicate_stops)} redundant stop words\")\n",
    "else:\n",
    "    print(\"No redundant stop words found\")\n",
    "\n",
    "# Check for redundancies in word families\n",
    "all_words = []\n",
    "word_to_families = {}\n",
    "for family, words in WORD_FAMILIES.items():\n",
    "    for word in words:\n",
    "        all_words.append(word)\n",
    "        if word not in word_to_families:\n",
    "            word_to_families[word] = []\n",
    "        word_to_families[word].append(family)\n",
    "    \n",
    "duplicate_words = [word for word in all_words if all_words.count(word) > 1]\n",
    "if duplicate_words:\n",
    "    print(f\"Warning: Found {len(set(duplicate_words))} words appearing in multiple word families:\")\n",
    "    for word in sorted(set(duplicate_words)):\n",
    "        families = word_to_families[word]\n",
    "        print(f\"  '{word}' appears in: {', '.join(families)}\")\n",
    "else:\n",
    "    print(\"No words appear in multiple word families\")\n",
    "\n",
    "# Check for overlap between word families and stop words\n",
    "stop_words_set = set(default_stop_words) | set(additional_stops)\n",
    "overlap_words = []\n",
    "overlap_by_family = {}\n",
    "\n",
    "for family, words in WORD_FAMILIES.items():\n",
    "    family_overlaps = [word for word in words if word in stop_words_set]\n",
    "    if family_overlaps:\n",
    "        overlap_by_family[family] = family_overlaps\n",
    "        overlap_words.extend(family_overlaps)\n",
    "\n",
    "if overlap_words:\n",
    "    print(f\"\\nWarning: Found {len(set(overlap_words))} words that appear in both word families and stop words:\")\n",
    "    for family, words in sorted(overlap_by_family.items()):\n",
    "        print(f\"  Family '{family}' has {len(words)} stop words: {', '.join(sorted(words))}\")\n",
    "else:\n",
    "    print(\"\\nNo overlap between word families and stop words\")\n",
    "\n",
    "# Verify no words in word families are in stop words after fixing\n",
    "stop_words_set = set(default_stop_words)\n",
    "all_family_words = [word for family_words in WORD_FAMILIES.values() for word in family_words]\n",
    "remaining_overlaps = [word for word in all_family_words if word in stop_words_set]\n",
    "print(f\"\\nAfter fixes, remaining overlaps between word families and stop words: {len(remaining_overlaps)}\")\n",
    "if remaining_overlaps:\n",
    "    print(f\"Remaining overlapping words: {', '.join(sorted(remaining_overlaps))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6833a002",
   "metadata": {},
   "source": [
    "### Global Variables Updates\n",
    "\n",
    "Sync updated variables from notebook into vis_tool_core module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d0b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word family mapping\n",
    "vis_tool_core.WORD_FAMILIES = WORD_FAMILIES  \n",
    "\n",
    "# Stopword lists\n",
    "vis_tool_core.additional_stops = additional_stops  \n",
    "vis_tool_core.default_stop_words = default_stop_words  \n",
    "\n",
    "# NLP tools\n",
    "vis_tool_core.TOKENIZER = TOKENIZER  \n",
    "vis_tool_core.lemmatizer = lemmatizer  \n",
    "vis_tool_core.MODEL = MODEL\n",
    "vis_tool_core.MAX_TOKENS = MAX_TOKENS\n",
    "\n",
    "# Output and cache directories\n",
    "vis_tool_core.OUTPUT_DIR = OUTPUT_DIR  \n",
    "vis_tool_core.CLUSTERING_DIR = CLUSTERING_DIR  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f08c41",
   "metadata": {},
   "source": [
    "### Validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d885044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress Pydantic deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pydantic\")\n",
    "\n",
    "# Temporarily using compatibility mode\n",
    "\n",
    "class VisualsInput(BaseModel):\n",
    "    filepath: str\n",
    "    stop_list: Optional[str] = None\n",
    "    num_words: int = 10\n",
    "    clustering_method: int = 1\n",
    "    distance_metric: str = \"default\" # \"default\" | \"cosine\"\n",
    "    reuse_clusterings: bool = False\n",
    "    window_size: int = 5\n",
    "    min_word_frequency: int = 2\n",
    "    cross_pos_normalize: bool = False\n",
    "    projects: Optional[List[str]] = None\n",
    "    data_groups: Optional[List[str]] = None\n",
    "    codes: Optional[List[str]] = None\n",
    "    seed_words: Optional[str] = None\n",
    "\n",
    "    # ---------- validators ----------\n",
    "    @field_validator(\"num_words\")\n",
    "    def validate_num_words(cls, v):\n",
    "        if v <= 0:\n",
    "            raise ValueError(\"num_words must be greater than 0\")\n",
    "        return v\n",
    "\n",
    "    @field_validator(\"clustering_method\")\n",
    "    def validate_clustering_method(cls, v):\n",
    "        if v not in [1, 2, 3, 4]:\n",
    "            raise ValueError(\"clustering_method must be 1-4\")\n",
    "        return v\n",
    "\n",
    "    @field_validator(\"window_size\")\n",
    "    def validate_window_size(cls, v):\n",
    "        if v <= 0:\n",
    "            raise ValueError(\"window_size must be greater than 0\")\n",
    "        return v\n",
    "\n",
    "    @field_validator(\"min_word_frequency\")\n",
    "    def validate_min_word_frequency(cls, v):\n",
    "        if v <= 0:\n",
    "            raise ValueError(\"min_word_frequency must be greater than 0\")\n",
    "        return v\n",
    "\n",
    "    # ---------- Configuration ----------\n",
    "    model_config = ConfigDict(extra=\"allow\", validate_assignment=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bb964b",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3b5947",
   "metadata": {},
   "source": [
    "### 1. Wordcloud \n",
    "\n",
    "This plot shows the most-frequent, non-trivial words in the selected texts—bigger words = higher frequency—so you can spot dominant topics at a glance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85d19d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud Function\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def make_circular_mask(diam: int = 1600, border: int = 5) -> np.ndarray:\n",
    "    img = Image.new(\"L\", (diam, diam), 0)\n",
    "    ImageDraw.Draw(img).ellipse([(border, border), (diam - border, diam - border)], fill=255)\n",
    "    return 255 - np.array(img)  # WordCloud expects black = non-fillable\n",
    "\n",
    "\n",
    "def generate_wordcloud(\n",
    "    text_series,\n",
    "    stopwords_path=None,\n",
    "    title=\"Wordcloud\",\n",
    "    out_dir=OUTPUT_DIR,\n",
    "    categories=None\n",
    "):\n",
    "    print(\"\\n✔ [OK] Building word-cloud…\")\n",
    "\n",
    "    # Stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    if stopwords_path and os.path.exists(stopwords_path):\n",
    "        with open(stopwords_path, 'r') as f:\n",
    "            stop_words.update(f.read().splitlines())\n",
    "\n",
    "    # Tokenize and filter\n",
    "    combined_text = ' '.join(text_series.dropna().astype(str))\n",
    "    tokens = word_tokenize(combined_text.lower())\n",
    "    filtered_tokens = [w for w in tokens if w.isalnum() and w not in stop_words and len(w) > 2]\n",
    "    paragraph_count = len(text_series)\n",
    "\n",
    "    # Mask\n",
    "    mask = make_circular_mask()\n",
    "    print(f\"✔ [OK] Mask ready {mask.shape}\")\n",
    "\n",
    "    # Frequencies\n",
    "    word_freq = Counter(filtered_tokens)\n",
    "    print(f\"✔ [OK] {len(word_freq):,} unique tokens\")\n",
    "\n",
    "    # Categories\n",
    "    if categories is None:\n",
    "        categories = {}\n",
    "    \n",
    "    word2cat = {w: cat for cat, info in categories.items() for w in info[\"words\"]}\n",
    "\n",
    "    # Color function\n",
    "    def colour_for_word(word, **_):\n",
    "        cat = word2cat.get(word.lower())\n",
    "        if cat:\n",
    "            r, g, b, _ = categories[cat][\"color\"]\n",
    "            return f\"#{int(r * 255):02x}{int(g * 255):02x}{int(b * 255):02x}\"\n",
    "        return \"#bcbcbc\"\n",
    "\n",
    "    # Generate WordCloud\n",
    "    wc = (WordCloud(\n",
    "        width=1600, height=1600, mask=mask, background_color=\"white\",\n",
    "        max_words=600, min_font_size=5, max_font_size=160, font_step=1,\n",
    "        margin=1, prefer_horizontal=0.3, random_state=42,\n",
    "        collocations=False, repeat=True, mode=\"RGBA\"\n",
    "    )\n",
    "        .generate_from_frequencies(word_freq)\n",
    "        .recolor(color_func=colour_for_word, random_state=42))\n",
    "\n",
    "    print(\"✔ [OK] WordCloud generated\")\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 12), dpi=300)\n",
    "    fig.patch.set_facecolor(\"white\")\n",
    "    ax.imshow(wc.to_array(), interpolation=\"bilinear\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    fig.suptitle(title, fontsize=36, fontweight=\"bold\", y=1.05, color=\"black\")\n",
    "    fig.text(0.5, 0.975, f\"Analysis of {paragraph_count:,} Paragraphs of Text\",\n",
    "             ha=\"center\", va=\"top\", fontsize=18, style=\"italic\", color=\"#333333\")\n",
    "\n",
    "    handles = [Patch(color=info[\"color\"], label=cat) for cat, info in categories.items()]\n",
    "    legend = ax.legend(handles=handles,\n",
    "                       loc=\"lower center\", bbox_to_anchor=(0.5, -0.085),\n",
    "                       ncol=3, frameon=False, fontsize=14)\n",
    "    for txt in legend.get_texts():\n",
    "        txt.set_color(\"#333333\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "\n",
    "    # Save\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    base = \"wordcloud_latest\"\n",
    "\n",
    "    fig.savefig(os.path.join(out_dir, f\"{base}.png\"),\n",
    "                dpi=300, bbox_inches=\"tight\", format=\"png\")\n",
    "    print(f\"✔ [OK] Saved {base}.png\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d705bf3",
   "metadata": {},
   "source": [
    "### 2. Word-based Heatmap \n",
    "\n",
    "This plot produces a word-by-speaker heatmap—columns clustered by cosine similarity or co-occurrence, so you can quickly see which keywords co-occur across interviews and how they group into thematic clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45872ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_heatmap_pipeline(input_data):\n",
    "    \"\"\"\n",
    "    Main function to run the heatmap pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_data : run_heatmap_pipeline\n",
    "        Object containing all input parameters\n",
    "    \"\"\"\n",
    "    # Build reverse mapping for word families\n",
    "    word_to_base = {}\n",
    "    for base_word, variants in WORD_FAMILIES.items():\n",
    "        for variant in variants:\n",
    "            word_to_base[variant.lower()] = base_word\n",
    "\n",
    "    seed_groups, seed_words, use_group_label = {}, [], False\n",
    "\n",
    "    df = pd.read_csv(input_data.filepath)\n",
    "\n",
    "    # Normalize alternative column names if needed\n",
    "    if 'text' not in df.columns:\n",
    "        alternatives = [col for col in df.columns if 'text' in col.lower() or 'content' in col.lower() or 'body' in col.lower()]\n",
    "        if alternatives:\n",
    "            print(f\"'text' column not found, using '{alternatives[0]}' instead.\")\n",
    "            df.rename(columns={alternatives[0]: 'text'}, inplace=True)\n",
    "        else:\n",
    "            print(\"Error: No suitable text column found.\")\n",
    "            return\n",
    "    \n",
    "    # We'll use the stop list instead of hardcoding additional common words\n",
    "    additional_common_words = set()\n",
    "            \n",
    "    # Use the pre-loaded stopwords if available\n",
    "    if hasattr(input_data, 'custom_stopwords') and input_data.custom_stopwords:\n",
    "        stop_words = input_data.custom_stopwords\n",
    "    else:\n",
    "        # Load stop words the old way if not pre-loaded\n",
    "        stop_words = manage_stop_list(input_data.stop_list, default_stop_words)\n",
    "        # Add our additional common words to the stop list\n",
    "        stop_words = stop_words.union(additional_common_words)\n",
    "\n",
    "    \n",
    "    # Fix list columns that may be stored as strings\n",
    "    for col in ['data_group', 'codes']:\n",
    "        if col in df.columns:\n",
    "            # Check if first non-null value is a string that looks like a list\n",
    "            sample = df[col].dropna().iloc[0] if not df[col].dropna().empty else None\n",
    "            if isinstance(sample, str) and (sample.startswith('[') or ',' in sample):\n",
    "                df[col] = df[col].apply(lambda x: eval(x) if isinstance(x, str) and x.strip() else \n",
    "                                       ([] if pd.isna(x) else [x]))\n",
    "    \n",
    "    # Apply Metadata Filters\n",
    "    if input_data.projects and 'project' in df.columns:\n",
    "        before = len(df)\n",
    "        df = df[df['project'].isin(input_data.projects)]\n",
    "        print(f\"Project filter: {before} → {len(df)} rows\")\n",
    "    \n",
    "    if input_data.data_groups and 'data_group' in df.columns:\n",
    "        before = len(df)\n",
    "        try:\n",
    "            # Handle both list and non-list data_group columns\n",
    "            if df['data_group'].apply(lambda x: isinstance(x, list)).any():\n",
    "                mask = df['data_group'].apply(lambda x: \n",
    "                    isinstance(x, list) and any(item in input_data.data_groups for item in x))\n",
    "            else:\n",
    "                mask = df['data_group'].isin(input_data.data_groups)\n",
    "            df = df[mask]\n",
    "            print(f\"Data group filter: {before} → {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in data_group filtering: {e}\")\n",
    "            print(f\"Sample data_group values: {df['data_group'].head()}\")\n",
    "    \n",
    "    if input_data.codes and 'codes' in df.columns:\n",
    "        before = len(df)\n",
    "        try:\n",
    "            # Handle both list and non-list codes columns\n",
    "            if df['codes'].apply(lambda x: isinstance(x, list)).any():\n",
    "                mask = df['codes'].apply(lambda x: \n",
    "                    isinstance(x, list) and any(item in input_data.codes for item in x))\n",
    "            else:\n",
    "                mask = df['codes'].isin(input_data.codes)\n",
    "            df = df[mask]\n",
    "            print(f\"Codes filter: {before} → {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in codes filtering: {e}\")\n",
    "            print(f\"Sample codes values: {df['codes'].head()}\")\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"Error: All rows were filtered out. Please check your filter criteria.\")\n",
    "        return\n",
    "    \n",
    "    sentences = df['text'].dropna().tolist()\n",
    "\n",
    "    # Add filtered sentences tracking here\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if not isinstance(sentence, str):\n",
    "            print(f\"Not a string: type={type(sentence)}, value={sentence}\")\n",
    "        tokens = tokenize_and_filter([sentence], stop_list=stop_words, \n",
    "                                   lemmatize=True, \n",
    "                                   cross_pos_normalize=input_data.cross_pos_normalize)\n",
    "        if tokens:  # Only keep sentences that have tokens after filtering\n",
    "            filtered_sentences.append(sentence)\n",
    "    print(f\"After filtering: {len(filtered_sentences)} valid text segments\")\n",
    "\n",
    "    # Filter out excluded codes if specified\n",
    "    if hasattr(input_data, 'excluded_codes') and input_data.excluded_codes and 'codes' in df.columns:\n",
    "        before = len(df)\n",
    "        try:\n",
    "            # Handle both list and non-list codes columns\n",
    "            if df['codes'].apply(lambda x: isinstance(x, list)).any():\n",
    "                # Keep rows where NONE of the excluded codes are present\n",
    "                mask = df['codes'].apply(lambda x: \n",
    "                    isinstance(x, list) and not any(item in input_data.excluded_codes for item in x))\n",
    "            else:\n",
    "                # Keep rows where the code is not in excluded_codes\n",
    "                mask = ~df['codes'].isin(input_data.excluded_codes)\n",
    "            df = df[mask]\n",
    "            print(f\"Excluded codes filter: {before} → {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in excluded_codes filtering: {e}\")\n",
    "    \n",
    "        # Define a custom word filter function\n",
    "    def custom_word_filter(word):\n",
    "        # First normalize with word families\n",
    "        word_lower = word.lower()\n",
    "        if word_lower in word_to_base:\n",
    "            word = word_to_base[word_lower]\n",
    "        \n",
    "        # Manual exclusion of common words that should be filtered\n",
    "        manual_exclusions = {'got', 'get', 'just', 'like', 'many', 'much', 'very', 'really', 'make'}\n",
    "        \n",
    "        return (word.lower() not in stop_words and\n",
    "                word.lower() not in manual_exclusions and\n",
    "                len(word) > 2 and  # Exclude very short words\n",
    "                not any(c.isdigit() for c in word) and  # Exclude words with numbers\n",
    "                re.match(r'^[a-z]+$', word.lower()))  # Only pure alphabetic words\n",
    "                \n",
    "    # Check if seed words were provided in the input\n",
    "    if hasattr(input_data, 'seed_words') and input_data.seed_words and input_data.seed_words.strip().lower() != \"none\":\n",
    "        seed_input = input_data.seed_words.strip()\n",
    "\n",
    "        if \":\" in seed_input:\n",
    "            use_group_label = True\n",
    "            for part in seed_input.split(\";\"):\n",
    "                part = part.strip()\n",
    "                if \":\" in part:\n",
    "                    group_label, word_str = part.split(\":\", 1)\n",
    "                    group_label = group_label.strip().lower()\n",
    "                    words = [w.strip().lower() for w in word_str.split(\",\") if w.strip()]\n",
    "                    seed_groups[group_label] = set(words)\n",
    "                    seed_words.append(group_label)\n",
    "                    print(f\"Group mode: all {words} will be treated as '{group_label}'\")\n",
    "                else:\n",
    "                    individuals = [w.strip().lower() for w in part.split(\",\") if w.strip()]\n",
    "                    seed_words.extend(individuals)\n",
    "                    print(f\"Individual mode: adding {individuals}\")\n",
    "        else:\n",
    "            seed_words = [w.strip().lower() for w in seed_input.split(\",\") if w.strip()]\n",
    "            print(f\"Pure individual word mode: using {seed_words}\")\n",
    "        if use_group_label:\n",
    "                sentences = [replace_group_words(text, seed_groups) for text in sentences]\n",
    "    else:\n",
    "        # Process sentences to get word frequencies for auto-selection of top words\n",
    "        print(\"WARNING: No seed words provided or 'NONE' specified. Using top frequent words as seeds... \")\n",
    "        excluded_words = stop_words.union(set(WORD_FAMILIES.keys()))\n",
    "        all_tokens = []\n",
    "        for sentence in sentences:\n",
    "            tokens = tokenize_and_filter([sentence], stop_list=stop_words,\n",
    "                                        lemmatize=True,\n",
    "                                        cross_pos_normalize=input_data.cross_pos_normalize)\n",
    "            filtered_tokens = [token.lower() for token in tokens if custom_word_filter(token)]\n",
    "            all_tokens.extend(filtered_tokens)\n",
    "        word_counts = Counter(all_tokens)\n",
    "        top_words = [word for word, _ in word_counts.most_common(30)\n",
    "                    if word.lower() not in excluded_words][:min(10, len(word_counts))]\n",
    "        seed_words = top_words\n",
    "        print(f\"Top frequent words as seeds: {seed_words}\")\n",
    "       \n",
    "    start = time.time()\n",
    "\n",
    "    # Clean and normalize seed words, but they're already preprocessed with word families\n",
    "    print(f\"Original seed words before cleaning: {seed_words}\")\n",
    "    clean_seed_words = clean_words(seed_words)\n",
    "    print(f\"Seed words after cleaning: {clean_seed_words}\")\n",
    "    clean_seeds = clean_words(seed_words)\n",
    "    seed_words = normalize_words(clean_seeds, stop_words, lemmatize=True, cross_pos_normalize=input_data.cross_pos_normalize)\n",
    "    seed_words = list(set(seed_words))\n",
    "    print(\"Final normalized seed words:\", seed_words)\n",
    "\n",
    "\n",
    "    # choose context source: keep stop-words only for RoBERTa\n",
    "    if input_data.clustering_method == 1:          # 1 = RoBERTa\n",
    "        sentences_for_embedding = sentences        # full context\n",
    "    else:                                          # 2-4 = Jaccard/PMI/TF-IDF\n",
    "        sentences_for_embedding = filtered_sentences\n",
    "\n",
    "    word_embeddings, similarity_matrix, co_occurrence_matrix = train_embedding(\n",
    "        sentences_for_embedding,\n",
    "        context_window = input_data.window_size,\n",
    "        stop_list  = stop_words, \n",
    "        seed_words = seed_words, \n",
    "        clustering_method  = input_data.clustering_method,\n",
    "        num_words = input_data.num_words, \n",
    "        lemmatize = True, \n",
    "        min_word_frequency = input_data.min_word_frequency,\n",
    "        reuse_clusterings  = input_data.reuse_clusterings,\n",
    "        cross_pos_normalize= getattr(input_data, 'cross_pos_normalize', False),\n",
    "        distance_metric = getattr(input_data, 'distance_metric', 'default'),\n",
    "        custom_word_filter = custom_word_filter\n",
    "    )\n",
    "\n",
    "    elapsed_time = time.time() - start\n",
    "    print(f\"Embedding generation completed in {elapsed_time:.1f} seconds\")\n",
    "\n",
    "    if word_embeddings is None:\n",
    "        print(\"Error: Failed to generate embeddings. Please check your input data and parameters.\")\n",
    "        return\n",
    "    # Plot Similarity Heatmap\n",
    "    \n",
    "    print(\"Plotting similarity heatmap...\")\n",
    "    fig = plot_heatmap(\n",
    "        input_data.clustering_method, word_embeddings, \n",
    "        similarity_matrix, co_occurrence_matrix, input_data.distance_metric\n",
    "    )\n",
    "\n",
    "    filename = f\"ONLY_heatmap_{input_data.clustering_method}_{input_data.distance_metric}.png\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    out_path = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "    fig.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"✔ [OK] Saved {out_path}\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    print(\"\\nAnalysis complete\")\n",
    "    print(f\"\\nNetwork visualization method: {input_data.clustering_method}\")\n",
    "    \n",
    "    if input_data.clustering_method == 1:\n",
    "        print(\"Method: RoBERTa – Shows semantic relationships based on contextual embeddings\")\n",
    "    elif input_data.clustering_method == 2:\n",
    "        if input_data.distance_metric == \"cosine\":\n",
    "            print(\"Method: Jaccard (cosine) – Uses context window vectors to compute cosine-based similarity between word usage patterns\")\n",
    "        elif input_data.distance_metric == \"default\":\n",
    "            print(\"Method: Jaccard (default) – Uses binary co-occurrence counts within a context window to capture word overlap\")\n",
    "    elif input_data.clustering_method == 3:\n",
    "        print(\"Method: PMI – Highlights statistically significant word associations based on pointwise mutual information\")\n",
    "    elif input_data.clustering_method == 4:\n",
    "        if input_data.distance_metric == \"cosine\":\n",
    "            print(\"Method: TF-IDF (cosine) – Uses TF-IDF-weighted context vectors to compute cosine similarity between words\")\n",
    "        elif input_data.distance_metric == \"default\":\n",
    "            print(\"Method: TF-IDF (default) – Uses raw TF-IDF-weighted co-occurrence scores for word associations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbe32fa",
   "metadata": {},
   "source": [
    "### 3. tSNE \n",
    "\n",
    "This function creates a 2-D t-SNE map of your vocabulary—using either the similarity matrix (embeddings) or the co-occurrence matrix—so that spatial distance ≈ lexical / semantic closeness; seed words are drawn larger and in red to spotlight how neighbouring terms cluster around them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238caf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tsne_pipeline(input_data):\n",
    "    \"\"\"\n",
    "    Main function to run the semantic network analysis pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_data : run_heatmap_pipeline\n",
    "        Object containing all input parameters\n",
    "    \"\"\"\n",
    "    # Build reverse mapping for word families\n",
    "    word_to_base = {}\n",
    "    for base_word, variants in WORD_FAMILIES.items():\n",
    "        for variant in variants:\n",
    "            word_to_base[variant.lower()] = base_word\n",
    "\n",
    "    seed_groups, seed_words, use_group_label = {}, [], False\n",
    "    auto_selected_seeds = False  # Flag to track if seeds were auto-selected\n",
    "\n",
    "    df = pd.read_csv(input_data.filepath)\n",
    "    \n",
    "    # Normalize alternative column names if needed\n",
    "    if 'text' not in df.columns:\n",
    "        alternatives = [col for col in df.columns if 'text' in col.lower() or 'content' in col.lower() or 'body' in col.lower()]\n",
    "        if alternatives:\n",
    "            print(f\"'text' column not found, using '{alternatives[0]}' instead.\")\n",
    "            df.rename(columns={alternatives[0]: 'text'}, inplace=True)\n",
    "        else:\n",
    "            print(\"Error: No suitable text column found.\")\n",
    "            return\n",
    "    \n",
    "    # We'll use the stop list instead of hardcoding additional common words\n",
    "    additional_common_words = set()\n",
    "            \n",
    "    # Use the pre-loaded stopwords if available\n",
    "    if hasattr(input_data, 'custom_stopwords') and input_data.custom_stopwords:\n",
    "        stop_words = input_data.custom_stopwords\n",
    "    else:\n",
    "        # Load stop words the old way if not pre-loaded\n",
    "        stop_words = manage_stop_list(input_data.stop_list, default_stop_words)\n",
    "        # Add our additional common words to the stop list\n",
    "        stop_words = stop_words.union(additional_common_words)\n",
    "\n",
    "    \n",
    "    # Fix list columns that may be stored as strings\n",
    "    for col in ['data_group', 'codes']:\n",
    "        if col in df.columns:\n",
    "            # Check if first non-null value is a string that looks like a list\n",
    "            sample = df[col].dropna().iloc[0] if not df[col].dropna().empty else None\n",
    "            if isinstance(sample, str) and (sample.startswith('[') or ',' in sample):\n",
    "                df[col] = df[col].apply(lambda x: eval(x) if isinstance(x, str) and x.strip() else \n",
    "                                       ([] if pd.isna(x) else [x]))\n",
    "    \n",
    "    # Apply Metadata Filters\n",
    "    if input_data.projects and 'project' in df.columns:\n",
    "        before = len(df)\n",
    "        df = df[df['project'].isin(input_data.projects)]\n",
    "        print(f\"Project filter: {before} → {len(df)} rows\")\n",
    "    \n",
    "    if input_data.data_groups and 'data_group' in df.columns:\n",
    "        before = len(df)\n",
    "        try:\n",
    "            # Handle both list and non-list data_group columns\n",
    "            if df['data_group'].apply(lambda x: isinstance(x, list)).any():\n",
    "                mask = df['data_group'].apply(lambda x: \n",
    "                    isinstance(x, list) and any(item in input_data.data_groups for item in x))\n",
    "            else:\n",
    "                mask = df['data_group'].isin(input_data.data_groups)\n",
    "            df = df[mask]\n",
    "            print(f\"Data group filter: {before} → {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in data_group filtering: {e}\")\n",
    "            print(f\"Sample data_group values: {df['data_group'].head()}\")\n",
    "    \n",
    "    if input_data.codes and 'codes' in df.columns:\n",
    "        before = len(df)\n",
    "        try:\n",
    "            # Handle both list and non-list codes columns\n",
    "            if df['codes'].apply(lambda x: isinstance(x, list)).any():\n",
    "                mask = df['codes'].apply(lambda x: \n",
    "                    isinstance(x, list) and any(item in input_data.codes for item in x))\n",
    "            else:\n",
    "                mask = df['codes'].isin(input_data.codes)\n",
    "            df = df[mask]\n",
    "            print(f\"Codes filter: {before} → {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in codes filtering: {e}\")\n",
    "            print(f\"Sample codes values: {df['codes'].head()}\")\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"Error: All rows were filtered out. Please check your filter criteria.\")\n",
    "        return\n",
    "    \n",
    "    sentences = df['text'].dropna().tolist()\n",
    "    print(f\"Final dataset: {len(sentences)} text segments ready for processing\")\n",
    "\n",
    "    # Add filtered sentences tracking here\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if not isinstance(sentence, str):\n",
    "            print(f\"Not a string: type={type(sentence)}, value={sentence}\")\n",
    "        tokens = tokenize_and_filter([sentence], stop_list=stop_words, \n",
    "                                   lemmatize=True, \n",
    "                                   cross_pos_normalize=input_data.cross_pos_normalize)\n",
    "        if tokens:  # Only keep sentences that have tokens after filtering\n",
    "            filtered_sentences.append(sentence)\n",
    "    print(f\"After filtering: {len(filtered_sentences)} valid text segments\")\n",
    "    \n",
    "\n",
    "    # Filter out excluded codes if specified\n",
    "    if hasattr(input_data, 'excluded_codes') and input_data.excluded_codes and 'codes' in df.columns:\n",
    "        before = len(df)\n",
    "        try:\n",
    "            # Handle both list and non-list codes columns\n",
    "            if df['codes'].apply(lambda x: isinstance(x, list)).any():\n",
    "                # Keep rows where NONE of the excluded codes are present\n",
    "                mask = df['codes'].apply(lambda x: \n",
    "                    isinstance(x, list) and not any(item in input_data.excluded_codes for item in x))\n",
    "            else:\n",
    "                # Keep rows where the code is not in excluded_codes\n",
    "                mask = ~df['codes'].isin(input_data.excluded_codes)\n",
    "            df = df[mask]\n",
    "            print(f\"Excluded codes filter: {before} → {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in excluded_codes filtering: {e}\")\n",
    "    \n",
    "        # Define a custom word filter function\n",
    "    def custom_word_filter(word):\n",
    "        # First normalize with word families\n",
    "        word_lower = word.lower()\n",
    "        if word_lower in word_to_base:\n",
    "            word = word_to_base[word_lower]\n",
    "        \n",
    "        # Manual exclusion of common words that should be filtered\n",
    "        manual_exclusions = {'got', 'get', 'just', 'like', 'many', 'much', 'very', 'really', 'make'}\n",
    "        \n",
    "        return (word.lower() not in stop_words and\n",
    "                word.lower() not in manual_exclusions and\n",
    "                len(word) > 2 and  # Exclude very short words\n",
    "                not any(c.isdigit() for c in word) and  # Exclude words with numbers\n",
    "                re.match(r'^[a-z]+$', word.lower()))  # Only pure alphabetic words\n",
    "                \n",
    "    # Check if seed words were provided in the input\n",
    "    if hasattr(input_data, 'seed_words') and input_data.seed_words and input_data.seed_words.strip().lower() != \"none\":\n",
    "        seed_input = input_data.seed_words.strip()\n",
    "\n",
    "        if \":\" in seed_input:\n",
    "            use_group_label = True\n",
    "            for part in seed_input.split(\";\"):\n",
    "                part = part.strip()\n",
    "                if \":\" in part:\n",
    "                    group_label, word_str = part.split(\":\", 1)\n",
    "                    group_label = group_label.strip().lower()\n",
    "                    words = [w.strip().lower() for w in word_str.split(\",\") if w.strip()]\n",
    "                    seed_groups[group_label] = set(words)\n",
    "                    seed_words.append(group_label)\n",
    "                    print(f\"Group mode: all {words} will be treated as '{group_label}'\")\n",
    "                else:\n",
    "                    individuals = [w.strip().lower() for w in part.split(\",\") if w.strip()]\n",
    "                    seed_words.extend(individuals)\n",
    "                    print(f\"Individual mode: adding {individuals}\")\n",
    "        else:\n",
    "            seed_words = [w.strip().lower() for w in seed_input.split(\",\") if w.strip()]\n",
    "            print(f\"Pure individual word mode: using {seed_words}\")\n",
    "        if use_group_label:\n",
    "                sentences = [replace_group_words(text, seed_groups) for text in sentences]\n",
    "    else:\n",
    "        # Process sentences to get word frequencies for auto-selection of top words\n",
    "        print(\"WARNING: No seed words provided or 'NONE' specified. Using top frequent words as seeds... \")\n",
    "        excluded_words = stop_words.union(set(WORD_FAMILIES.keys()))\n",
    "        all_tokens = []\n",
    "        for sentence in sentences:\n",
    "            tokens = tokenize_and_filter([sentence], stop_list=stop_words,\n",
    "                                        lemmatize=True,\n",
    "                                        cross_pos_normalize=input_data.cross_pos_normalize)\n",
    "            filtered_tokens = [token.lower() for token in tokens if custom_word_filter(token)]\n",
    "            all_tokens.extend(filtered_tokens)\n",
    "        word_counts = Counter(all_tokens)\n",
    "        top_words = [word for word, _ in word_counts.most_common(30)\n",
    "                    if word.lower() not in excluded_words][:min(10, len(word_counts))]\n",
    "        seed_words = top_words\n",
    "        print(f\"Top frequent words as seeds: {seed_words}\")\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Clean and normalize seed words, but they're already preprocessed with word families\n",
    "    print(f\"Original seed words before cleaning: {seed_words}\")\n",
    "    clean_seed_words = clean_words(seed_words)\n",
    "    print(f\"Seed words after cleaning: {clean_seed_words}\")\n",
    "    clean_seeds = clean_words(seed_words)\n",
    "    seed_words = normalize_words(clean_seeds, stop_words, lemmatize=True, cross_pos_normalize=input_data.cross_pos_normalize)\n",
    "    seed_words = list(set(seed_words))\n",
    "    print(\"Final normalized seed words:\", seed_words)\n",
    "\n",
    "    # choose context source: keep stop-words only for RoBERTa\n",
    "    if input_data.clustering_method == 1:          # 1 = RoBERTa\n",
    "        sentences_for_embedding = sentences        # full context\n",
    "    else:                                          # 2-4 = Jaccard/PMI/TF-IDF\n",
    "        sentences_for_embedding = filtered_sentences\n",
    "\n",
    "    word_embeddings, similarity_matrix, co_occurrence_matrix = train_embedding(\n",
    "        sentences_for_embedding,\n",
    "        context_window = input_data.window_size,\n",
    "        stop_list  = stop_words, \n",
    "        seed_words = seed_words, \n",
    "        clustering_method  = input_data.clustering_method,\n",
    "        num_words = input_data.num_words, \n",
    "        lemmatize = True, \n",
    "        min_word_frequency = input_data.min_word_frequency,\n",
    "        reuse_clusterings  = input_data.reuse_clusterings,\n",
    "        cross_pos_normalize= getattr(input_data, 'cross_pos_normalize', False),\n",
    "        distance_metric = getattr(input_data, 'distance_metric', 'default'),\n",
    "        custom_word_filter = custom_word_filter\n",
    "    )\n",
    "\n",
    "    elapsed_time = time.time() - start\n",
    "    print(f\"Embedding generation completed in {elapsed_time:.1f} seconds\")\n",
    "\n",
    "    if word_embeddings is None:\n",
    "        print(\"Error: Failed to generate embeddings. Please check your input data and parameters.\")\n",
    "        return\n",
    "\n",
    "    # Generate t-SNE Dimensional Reduction Plot\n",
    "    print(\"-----------------------------------------------\")\n",
    "    print(\"Generating t-SNE dimensional reduction plot...\")\n",
    "    try:\n",
    "        plot_tsne_dimensional_reduction(\n",
    "            word_embeddings=word_embeddings,\n",
    "            similarity_matrix=similarity_matrix,\n",
    "            co_occurrence_matrix=co_occurrence_matrix,\n",
    "            clustering_method=input_data.clustering_method,\n",
    "            seed_words=seed_words, \n",
    "            distance_metric=getattr(input_data, 'distance_metric', 'default')\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"t-SNE plot error: {e}\")\n",
    "    \n",
    "    print(\"\\nAnalysis complete\")\n",
    "    print(f\"\\nNetwork visualization method: {input_data.clustering_method}\")\n",
    "    \n",
    "    if input_data.clustering_method == 1:\n",
    "        print(\"Method: RoBERTa – Shows semantic relationships based on contextual embeddings\")\n",
    "    elif input_data.clustering_method == 2:\n",
    "        if input_data.distance_metric == \"cosine\":\n",
    "            print(\"Method: Jaccard (cosine) – Uses context window vectors to compute cosine-based similarity between word usage patterns\")\n",
    "        elif input_data.distance_metric == \"default\":\n",
    "            print(\"Method: Jaccard (default) – Uses binary co-occurrence counts within a context window to capture word overlap\")\n",
    "    elif input_data.clustering_method == 3:\n",
    "        print(\"Method: PMI – Highlights statistically significant word associations based on pointwise mutual information\")\n",
    "    elif input_data.clustering_method == 4:\n",
    "        if input_data.distance_metric == \"cosine\":\n",
    "            print(\"Method: TF-IDF (cosine) – Uses TF-IDF-weighted context vectors to compute cosine similarity between words\")\n",
    "        elif input_data.distance_metric == \"default\":\n",
    "            print(\"Method: TF-IDF (default) – Uses raw TF-IDF-weighted co-occurrence scores for word associations\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e11b66",
   "metadata": {},
   "source": [
    "### 4. Word-based Heatmp and Semantic Network (no custom colors, no edge bundling)\n",
    "\n",
    "This function generates a frequency-by-speaker heat-map plus a plain semantic-network graph—default blue nodes, gray edges, no category colours and no edge-bundling—so you get a quick, unbiased view of word overlap and overall connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551315ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Run the Pipeline\n",
    "def run_heatmap_network_plain_pipeline(input_data):\n",
    "    \"\"\"\n",
    "    Main function to run the semantic network analysis pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_data : VisualsInput\n",
    "        Object containing all input parameters\n",
    "    \"\"\"\n",
    "    # Build reverse mapping for word families\n",
    "    word_to_base = {}\n",
    "    for base_word, variants in WORD_FAMILIES.items():\n",
    "        for variant in variants:\n",
    "            word_to_base[variant.lower()] = base_word\n",
    "\n",
    "    seed_groups, seed_words, use_group_label = {}, [], False\n",
    "    auto_selected_seeds = False  # Flag to track if seeds were auto-selected\n",
    "\n",
    "    df = pd.read_csv(input_data.filepath)\n",
    "  \n",
    "    # Normalize alternative column names if needed\n",
    "    if 'text' not in df.columns:\n",
    "        alternatives = [col for col in df.columns if 'text' in col.lower() or 'content' in col.lower() or 'body' in col.lower()]\n",
    "        if alternatives:\n",
    "            print(f\"'text' column not found, using '{alternatives[0]}' instead.\")\n",
    "            df.rename(columns={alternatives[0]: 'text'}, inplace=True)\n",
    "        else:\n",
    "            print(\"Error: No suitable text column found.\")\n",
    "            return\n",
    "    \n",
    "    # We'll use the stop list instead of hardcoding additional common words\n",
    "    additional_common_words = set()\n",
    "            \n",
    "    # Use the pre-loaded stopwords if available\n",
    "    if hasattr(input_data, 'custom_stopwords') and input_data.custom_stopwords:\n",
    "        stop_words = input_data.custom_stopwords\n",
    "    else:\n",
    "        # Load stop words the old way if not pre-loaded\n",
    "        stop_words = manage_stop_list(input_data.stop_list, default_stop_words)\n",
    "        # Add our additional common words to the stop list\n",
    "        stop_words = stop_words.union(additional_common_words)\n",
    "\n",
    "    \n",
    "    # Fix list columns that may be stored as strings\n",
    "    for col in ['data_group', 'codes']:\n",
    "        if col in df.columns:\n",
    "            # Check if first non-null value is a string that looks like a list\n",
    "            sample = df[col].dropna().iloc[0] if not df[col].dropna().empty else None\n",
    "            if isinstance(sample, str) and (sample.startswith('[') or ',' in sample):\n",
    "                df[col] = df[col].apply(lambda x: eval(x) if isinstance(x, str) and x.strip() else \n",
    "                                       ([] if pd.isna(x) else [x]))\n",
    "    \n",
    "    # Apply Metadata Filters\n",
    "    if input_data.projects and 'project' in df.columns:\n",
    "        before = len(df)\n",
    "        df = df[df['project'].isin(input_data.projects)]\n",
    "        print(f\"Project filter: {before} → {len(df)} rows\")\n",
    "    \n",
    "    if input_data.data_groups and 'data_group' in df.columns:\n",
    "        before = len(df)\n",
    "        try:\n",
    "            # Handle both list and non-list data_group columns\n",
    "            if df['data_group'].apply(lambda x: isinstance(x, list)).any():\n",
    "                mask = df['data_group'].apply(lambda x: \n",
    "                    isinstance(x, list) and any(item in input_data.data_groups for item in x))\n",
    "            else:\n",
    "                mask = df['data_group'].isin(input_data.data_groups)\n",
    "            df = df[mask]\n",
    "            print(f\"Data group filter: {before} → {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in data_group filtering: {e}\")\n",
    "            print(f\"Sample data_group values: {df['data_group'].head()}\")\n",
    "    \n",
    "    if input_data.codes and 'codes' in df.columns:\n",
    "        before = len(df)\n",
    "        try:\n",
    "            # Handle both list and non-list codes columns\n",
    "            if df['codes'].apply(lambda x: isinstance(x, list)).any():\n",
    "                mask = df['codes'].apply(lambda x: \n",
    "                    isinstance(x, list) and any(item in input_data.codes for item in x))\n",
    "            else:\n",
    "                mask = df['codes'].isin(input_data.codes)\n",
    "            df = df[mask]\n",
    "            print(f\"Codes filter: {before} → {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in codes filtering: {e}\")\n",
    "            print(f\"Sample codes values: {df['codes'].head()}\")\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"Error: All rows were filtered out. Please check your filter criteria.\")\n",
    "        return\n",
    "    \n",
    "    sentences = df['text'].dropna().tolist()\n",
    "    print(f\"Final dataset: {len(sentences)} text segments ready for processing\")\n",
    "\n",
    "    # Add filtered sentences tracking here\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if not isinstance(sentence, str):\n",
    "            print(f\"Not a string: type={type(sentence)}, value={sentence}\")\n",
    "        tokens = tokenize_and_filter([sentence],stop_list=stop_words, \n",
    "                                   lemmatize=True, \n",
    "                                   cross_pos_normalize=input_data.cross_pos_normalize)\n",
    "        if tokens:  # Only keep sentences that have tokens after filtering\n",
    "            filtered_sentences.append(sentence)\n",
    "    print(f\"After filtering: {len(filtered_sentences)} valid text segments\")\n",
    "    \n",
    "\n",
    "    # Filter out excluded codes if specified\n",
    "    if hasattr(input_data, 'excluded_codes') and input_data.excluded_codes and 'codes' in df.columns:\n",
    "        before = len(df)\n",
    "        try:\n",
    "            # Handle both list and non-list codes columns\n",
    "            if df['codes'].apply(lambda x: isinstance(x, list)).any():\n",
    "                # Keep rows where NONE of the excluded codes are present\n",
    "                mask = df['codes'].apply(lambda x: \n",
    "                    isinstance(x, list) and not any(item in input_data.excluded_codes for item in x))\n",
    "            else:\n",
    "                # Keep rows where the code is not in excluded_codes\n",
    "                mask = ~df['codes'].isin(input_data.excluded_codes)\n",
    "            df = df[mask]\n",
    "            print(f\"Excluded codes filter: {before} → {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in excluded_codes filtering: {e}\")\n",
    "    \n",
    "        # Define a custom word filter function\n",
    "    def custom_word_filter(word):\n",
    "        # First normalize with word families\n",
    "        word_lower = word.lower()\n",
    "        if word_lower in word_to_base:\n",
    "            word = word_to_base[word_lower]\n",
    "        \n",
    "        # Manual exclusion of common words that should be filtered\n",
    "        manual_exclusions = {'got', 'get', 'just', 'like', 'many', 'much', 'very', 'really', 'make'}\n",
    "        \n",
    "        return (word.lower() not in stop_words and\n",
    "                word.lower() not in manual_exclusions and\n",
    "                len(word) > 2 and  # Exclude very short words\n",
    "                not any(c.isdigit() for c in word) and  # Exclude words with numbers\n",
    "                re.match(r'^[a-z]+$', word.lower()))  # Only pure alphabetic words\n",
    "                \n",
    "    # Check if seed words were provided in the input\n",
    "    if hasattr(input_data, 'seed_words') and input_data.seed_words and input_data.seed_words.strip().lower() != \"none\":\n",
    "        seed_input = input_data.seed_words.strip()\n",
    "\n",
    "        if \":\" in seed_input:\n",
    "            use_group_label = True\n",
    "            for part in seed_input.split(\";\"):\n",
    "                part = part.strip()\n",
    "                if \":\" in part:\n",
    "                    group_label, word_str = part.split(\":\", 1)\n",
    "                    group_label = group_label.strip().lower()\n",
    "                    words = [w.strip().lower() for w in word_str.split(\",\") if w.strip()]\n",
    "                    seed_groups[group_label] = set(words)\n",
    "                    seed_words.append(group_label)\n",
    "                    print(f\"Group mode: all {words} will be treated as '{group_label}'\")\n",
    "                else:\n",
    "                    individuals = [w.strip().lower() for w in part.split(\",\") if w.strip()]\n",
    "                    seed_words.extend(individuals)\n",
    "                    print(f\"Individual mode: adding {individuals}\")\n",
    "        else:\n",
    "            seed_words = [w.strip().lower() for w in seed_input.split(\",\") if w.strip()]\n",
    "            print(f\"Pure individual word mode: using {seed_words}\")\n",
    "        if use_group_label:\n",
    "                sentences = [replace_group_words(text, seed_groups) for text in sentences]\n",
    "    else:\n",
    "        # Process sentences to get word frequencies for auto-selection of top words\n",
    "        print(\"WARNING: No seed words provided or 'NONE' specified. Using top frequent words as seeds... \")\n",
    "        excluded_words = stop_words.union(set(WORD_FAMILIES.keys()))\n",
    "        all_tokens = []\n",
    "        for sentence in sentences:\n",
    "            tokens = tokenize_and_filter([sentence], stop_list=stop_words,\n",
    "                                        lemmatize=True,\n",
    "                                        cross_pos_normalize=input_data.cross_pos_normalize)\n",
    "            filtered_tokens = [token.lower() for token in tokens if custom_word_filter(token)]\n",
    "            all_tokens.extend(filtered_tokens)\n",
    "        word_counts = Counter(all_tokens)\n",
    "        top_words = [word for word, _ in word_counts.most_common(30)\n",
    "                    if word.lower() not in excluded_words][:min(10, len(word_counts))]\n",
    "        seed_words = top_words\n",
    "        print(f\"Top frequent words as seeds: {seed_words}\")\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    # Clean and normalize seed words, but they're already preprocessed with word families\n",
    "    print(f\"Original seed words before cleaning: {seed_words}\")\n",
    "    clean_seed_words = clean_words(seed_words)\n",
    "    print(f\"Seed words after cleaning: {clean_seed_words}\")\n",
    "    clean_seeds = clean_words(seed_words)\n",
    "    seed_words = normalize_words(clean_seeds, stop_words, lemmatize=True, cross_pos_normalize=input_data.cross_pos_normalize)\n",
    "    seed_words = list(set(seed_words))\n",
    "    print(\"Final normalized seed words:\", seed_words)\n",
    "\n",
    "\n",
    "    # choose context source: keep stop-words only for RoBERTa\n",
    "    if input_data.clustering_method == 1:          # 1 = RoBERTa\n",
    "        sentences_for_embedding = sentences        # full context\n",
    "    else:                                          # 2-4 = Jaccard/PMI/TF-IDF\n",
    "        sentences_for_embedding = filtered_sentences\n",
    "\n",
    "    word_embeddings, similarity_matrix, co_occurrence_matrix = train_embedding(\n",
    "        sentences_for_embedding,\n",
    "        context_window = input_data.window_size, \n",
    "        stop_list  = stop_words, \n",
    "        seed_words = seed_words, \n",
    "        clustering_method  = input_data.clustering_method,\n",
    "        num_words = input_data.num_words, \n",
    "        lemmatize = True, \n",
    "        min_word_frequency = input_data.min_word_frequency,\n",
    "        reuse_clusterings  = input_data.reuse_clusterings,\n",
    "        cross_pos_normalize= getattr(input_data, 'cross_pos_normalize', False),\n",
    "        distance_metric = getattr(input_data, 'distance_metric', 'default'),\n",
    "        custom_word_filter = custom_word_filter\n",
    "    )\n",
    "\n",
    "    elapsed_time = time.time() - start\n",
    "    print(f\"Embedding generation completed in {elapsed_time:.1f} seconds\")\n",
    "\n",
    "    if word_embeddings is None:\n",
    "        print(\"Error: Failed to generate embeddings. Please check your input data and parameters.\")\n",
    "        return\n",
    "    # Plot Similarity Heatmap\n",
    "    \n",
    "    print(\"Plotting similarity heatmap...\")\n",
    "    fig_heat = plot_heatmap(\n",
    "        input_data.clustering_method, word_embeddings, \n",
    "        similarity_matrix, co_occurrence_matrix, input_data.distance_metric\n",
    "    )\n",
    "\n",
    "    filename = f\"heatmap_plain_m{input_data.clustering_method}_{input_data.distance_metric}.png\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    out_path = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "    fig_heat.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"✔ [OK] Saved {out_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(\"-----------------------------------------------\")\n",
    "    print(\"Plotting semantic network (plain, no categories)...\")\n",
    "    fig_sn1 = plot_semantic_network(\n",
    "        word_embeddings,\n",
    "        [] if auto_selected_seeds else seed_words,\n",
    "        input_data.clustering_method,\n",
    "        similarity_matrix, co_occurrence_matrix,\n",
    "        semantic_categories=None,               \n",
    "        link_threshold = input_data.link_threshold,\n",
    "        link_color_threshold= input_data.link_color_threshold,\n",
    "        distance_metric=getattr(input_data, 'distance_metric', 'default'))\n",
    "    filename = f\"semantic_network_plain_m{input_data.clustering_method}_{input_data.distance_metric}.png\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    out_path = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "    fig_sn1.suptitle(\"Semantic Network (Plain)\", fontsize=30, y=0.98, fontweight='bold')\n",
    "    fig_sn1.subplots_adjust(top=0.95)\n",
    "\n",
    "    fig_sn1.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"✔ [OK] Saved {out_path}\")\n",
    "\n",
    "    plt.show()\n",
    "        # Add a second network visualization that removes seed nodes\n",
    "    print(\"\\nGenerating secondary network with seed nodes hidden...\")\n",
    "    \n",
    "    # Filter out seed words from word embeddings and matrices\n",
    "    non_seed_words = [word for word in word_embeddings.keys() if word not in seed_words]\n",
    "    \n",
    "    if len(non_seed_words) > 5:  # Only proceed if we have enough nodes to make a meaningful network\n",
    "        non_seed_embeddings = {word: word_embeddings[word] for word in non_seed_words}\n",
    "        \n",
    "        # Create filtered similarity/co-occurrence matrices\n",
    "        if similarity_matrix is not None:\n",
    "            words = list(word_embeddings.keys())\n",
    "            indices = [words.index(word) for word in non_seed_words]\n",
    "            filtered_similarity = similarity_matrix[np.ix_(indices, indices)]\n",
    "        else:\n",
    "            filtered_similarity = None\n",
    "            \n",
    "        if co_occurrence_matrix is not None:\n",
    "            words = list(word_embeddings.keys())\n",
    "            indices = [words.index(word) for word in non_seed_words]\n",
    "            filtered_co_occurrence = co_occurrence_matrix[np.ix_(indices, indices)]\n",
    "        else:\n",
    "            filtered_co_occurrence = None\n",
    "        \n",
    "        # Plot the filtered network\n",
    "        print(f\"Plotting secondary network with {len(non_seed_words)} nodes (seeds hidden)...\")\n",
    "        fig_sn2 = plot_semantic_network(\n",
    "            non_seed_embeddings, [], \n",
    "            input_data.clustering_method, \n",
    "            filtered_similarity, filtered_co_occurrence, \n",
    "            semantic_categories=None,\n",
    "            link_threshold=input_data.link_threshold,\n",
    "            link_color_threshold=input_data.link_color_threshold,\n",
    "            distance_metric=getattr(input_data, 'distance_metric', 'default')\n",
    "        )\n",
    "        filename = f\"semantic_network_noseeds_m{input_data.clustering_method}_{input_data.distance_metric}.png\"\n",
    "        out_path = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "        fig_sn2.suptitle(\"Semantic Network (Seeds Hidden)\", fontsize=30, y=0.98, fontweight='bold')\n",
    "        fig_sn2.subplots_adjust(top=0.95)\n",
    "\n",
    "        fig_sn2.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"✔ [OK] Saved {out_path}\")\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "   \n",
    "    # If custom coloring was used in the first visualization, also do it for the second\n",
    "    if hasattr(input_data, 'custom_colors') and input_data.custom_colors and hasattr(input_data, 'semantic_categories'):\n",
    "        semantic_categories = input_data.semantic_categories\n",
    "    else:\n",
    "        print(\"Not enough non-seed nodes to generate a meaningful secondary network.\")\n",
    "    \n",
    "    # Print the list of actual nodes used in the network (excluding seeds)\n",
    "    print(\"\\nFinal nodes used in network (excluding seeds):\")\n",
    "    if 'non_seed_words' in locals() and len(non_seed_words) > 0:\n",
    "        # Get frequencies for each node and sort by frequency (highest first)\n",
    "        print(f\"Total non-seed words: {len(non_seed_words)}\")\n",
    "        \n",
    "        # Check if word_frequencies is defined, otherwise use the word_counts from earlier\n",
    "        if 'word_frequencies' not in locals() and 'word_counts' in locals():\n",
    "            word_frequencies = word_counts\n",
    "        elif 'word_frequencies' not in locals():\n",
    "            print(\"Warning: Word frequency information not available\")\n",
    "            # Just print the words without frequencies\n",
    "            for word in sorted(non_seed_words):\n",
    "                print(f\"- {word}\")\n",
    "        else:\n",
    "            # Create list of (word, frequency) pairs and sort by frequency\n",
    "            freq_sorted_words = [(word, word_frequencies[word]) for word in non_seed_words if word in word_frequencies]\n",
    "            freq_sorted_words.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            for word, freq in freq_sorted_words:\n",
    "                print(f\"- {word}: {freq}\")\n",
    "    else:\n",
    "        print(\"No non-seed nodes were used in the network.uelse\")\n",
    "    \n",
    "    # Print the number of filtered sentences used\n",
    "    if 'filtered_sentences' in locals():\n",
    "        print(f\"\\nTotal number of filtered sentences used: {len(filtered_sentences)}\")\n",
    "        if 'seed_words' in locals() and seed_words:\n",
    "            # Enhanced seed word detection using word families\n",
    "            seed_containing_sentences = 0\n",
    "            for sentence in filtered_sentences:\n",
    "                sentence_lower = sentence.lower()\n",
    "                contains_seed = False\n",
    "                for seed in seed_words:\n",
    "                    # Check direct match\n",
    "                    if seed.lower() in sentence_lower:\n",
    "                        contains_seed = True\n",
    "                        break\n",
    "                    # Check word family variants\n",
    "                    for family, variants in WORD_FAMILIES.items():\n",
    "                        if seed.lower() == family.lower() or seed.lower() in [v.lower() for v in variants]:\n",
    "                            if any(variant.lower() in sentence_lower for variant in variants):\n",
    "                                contains_seed = True\n",
    "                                break\n",
    "                    if contains_seed:\n",
    "                        break\n",
    "                if contains_seed:\n",
    "                    seed_containing_sentences += 1\n",
    "            print(f\"Number of filtered sentences containing seed words: {seed_containing_sentences}\")\n",
    "    \n",
    "    print(\"\\nAnalysis complete\")\n",
    "    print(f\"\\nNetwork visualization method: {input_data.clustering_method}\")\n",
    "    \n",
    "    if input_data.clustering_method == 1:\n",
    "        print(\"Method: RoBERTa – Shows semantic relationships based on contextual embeddings\")\n",
    "    elif input_data.clustering_method == 2:\n",
    "        if input_data.distance_metric == \"cosine\":\n",
    "            print(\"Method: Jaccard (cosine) – Uses context window vectors to compute cosine-based similarity between word usage patterns\")\n",
    "        elif input_data.distance_metric == \"default\":\n",
    "            print(\"Method: Jaccard (default) – Uses binary co-occurrence counts within a context window to capture word overlap\")\n",
    "    elif input_data.clustering_method == 3:\n",
    "        print(\"Method: PMI – Highlights statistically significant word associations based on pointwise mutual information\")\n",
    "    elif input_data.clustering_method == 4:\n",
    "        if input_data.distance_metric == \"cosine\":\n",
    "            print(\"Method: TF-IDF (cosine) – Uses TF-IDF-weighted context vectors to compute cosine similarity between words\")\n",
    "        elif input_data.distance_metric == \"default\":\n",
    "            print(\"Method: TF-IDF (default) – Uses raw TF-IDF-weighted co-occurrence scores for word associations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a877f58",
   "metadata": {},
   "source": [
    "### 5. Word-based Heatmp and Semantic Network (custom colors, no edge bundling)\n",
    "\n",
    "This function generates a per-speaker word-frequency heat-map + a semantic-network graph that uses your custom color palette for node groups (so themes pop out) but keeps simple straight/gray edges—no bundling—giving a quick colored overview of concept clusters without extra visual wiring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd6f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to Run the Pipeline\n",
    "def run_heatmap_network_pipeline(input_data):\n",
    "    \"\"\"\n",
    "    Main function to run the semantic network analysis pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_data : SemanticNetworkInput\n",
    "        Object containing all input parameters\n",
    "    \"\"\"\n",
    "    # Build reverse mapping for word families\n",
    "    word_to_base = {}\n",
    "    for base_word, variants in WORD_FAMILIES.items():\n",
    "        for variant in variants:\n",
    "            word_to_base[variant.lower()] = base_word\n",
    "\n",
    "    seed_groups, seed_words, use_group_label = {}, [], False\n",
    "    auto_selected_seeds = False  # Flag to track if seeds were auto-selected\n",
    "\n",
    "    df = pd.read_csv(input_data.filepath)\n",
    "    \n",
    "    original_row_count = len(df)\n",
    "    \n",
    "    # Normalize alternative column names if needed\n",
    "    if 'text' not in df.columns:\n",
    "        alternatives = [col for col in df.columns if 'text' in col.lower() or 'content' in col.lower() or 'body' in col.lower()]\n",
    "        if alternatives:\n",
    "            print(f\"'text' column not found, using '{alternatives[0]}' instead.\")\n",
    "            df.rename(columns={alternatives[0]: 'text'}, inplace=True)\n",
    "        else:\n",
    "            print(\"Error: No suitable text column found.\")\n",
    "            return\n",
    "    \n",
    "    # We'll use the stop list instead of hardcoding additional common words\n",
    "    additional_common_words = set()\n",
    "            \n",
    "    # Use the pre-loaded stopwords if available\n",
    "    if hasattr(input_data, 'custom_stopwords') and input_data.custom_stopwords:\n",
    "        stop_words = input_data.custom_stopwords\n",
    "    else:\n",
    "        # Load stop words the old way if not pre-loaded\n",
    "        stop_words = manage_stop_list(input_data.stop_list, default_stop_words)\n",
    "        # Add our additional common words to the stop list\n",
    "        stop_words = stop_words.union(additional_common_words)\n",
    "    \n",
    "    # Fix list columns that may be stored as strings\n",
    "    for col in ['data_group', 'codes']:\n",
    "        if col in df.columns:\n",
    "            # Check if first non-null value is a string that looks like a list\n",
    "            sample = df[col].dropna().iloc[0] if not df[col].dropna().empty else None\n",
    "            if isinstance(sample, str) and (sample.startswith('[') or ',' in sample):\n",
    "                df[col] = df[col].apply(lambda x: eval(x) if isinstance(x, str) and x.strip() else \n",
    "                                       ([] if pd.isna(x) else [x]))\n",
    "    \n",
    "    # Apply Metadata Filters\n",
    "    if input_data.projects and 'project' in df.columns:\n",
    "        before = len(df)\n",
    "        df = df[df['project'].isin(input_data.projects)]\n",
    "        print(f\"Project filter: {before} → {len(df)} rows\")\n",
    "    \n",
    "    if input_data.data_groups and 'data_group' in df.columns:\n",
    "        before = len(df)\n",
    "        try:\n",
    "            # Handle both list and non-list data_group columns\n",
    "            if df['data_group'].apply(lambda x: isinstance(x, list)).any():\n",
    "                mask = df['data_group'].apply(lambda x: \n",
    "                    isinstance(x, list) and any(item in input_data.data_groups for item in x))\n",
    "            else:\n",
    "                mask = df['data_group'].isin(input_data.data_groups)\n",
    "            df = df[mask]\n",
    "            print(f\"Data group filter: {before} → {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in data_group filtering: {e}\")\n",
    "            print(f\"Sample data_group values: {df['data_group'].head()}\")\n",
    "    \n",
    "    if input_data.codes and 'codes' in df.columns:\n",
    "        before = len(df)\n",
    "        try:\n",
    "            # Handle both list and non-list codes columns\n",
    "            if df['codes'].apply(lambda x: isinstance(x, list)).any():\n",
    "                mask = df['codes'].apply(lambda x: \n",
    "                    isinstance(x, list) and any(item in input_data.codes for item in x))\n",
    "            else:\n",
    "                mask = df['codes'].isin(input_data.codes)\n",
    "            df = df[mask]\n",
    "            print(f\"Codes filter: {before} → {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in codes filtering: {e}\")\n",
    "            print(f\"Sample codes values: {df['codes'].head()}\")\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"Error: All rows were filtered out. Please check your filter criteria.\")\n",
    "        return\n",
    "    \n",
    "    sentences = df['text'].dropna().tolist()\n",
    "    print(f\"Final dataset: {len(sentences)} text segments ready for processing\")\n",
    "\n",
    "    # Add filtered sentences tracking here\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if not isinstance(sentence, str):\n",
    "            print(f\"Not a string: type={type(sentence)}, value={sentence}\")\n",
    "        tokens = tokenize_and_filter([sentence],stop_list=stop_words, \n",
    "                                   lemmatize=True, \n",
    "                                   cross_pos_normalize=input_data.cross_pos_normalize)\n",
    "        if tokens:  # Only keep sentences that have tokens after filtering\n",
    "            filtered_sentences.append(sentence)\n",
    "    print(f\"After filtering: {len(filtered_sentences)} valid text segments\")\n",
    "    \n",
    "\n",
    "    # Filter out excluded codes if specified\n",
    "    if hasattr(input_data, 'excluded_codes') and input_data.excluded_codes and 'codes' in df.columns:\n",
    "        before = len(df)\n",
    "        try:\n",
    "            # Handle both list and non-list codes columns\n",
    "            if df['codes'].apply(lambda x: isinstance(x, list)).any():\n",
    "                # Keep rows where NONE of the excluded codes are present\n",
    "                mask = df['codes'].apply(lambda x: \n",
    "                    isinstance(x, list) and not any(item in input_data.excluded_codes for item in x))\n",
    "            else:\n",
    "                # Keep rows where the code is not in excluded_codes\n",
    "                mask = ~df['codes'].isin(input_data.excluded_codes)\n",
    "            df = df[mask]\n",
    "            print(f\"Excluded codes filter: {before} → {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in excluded_codes filtering: {e}\")\n",
    "    \n",
    "        # Define a custom word filter function\n",
    "    def custom_word_filter(word):\n",
    "        # First normalize with word families\n",
    "        word_lower = word.lower()\n",
    "        if word_lower in word_to_base:\n",
    "            word = word_to_base[word_lower]\n",
    "        \n",
    "        # Manual exclusion of common words that should be filtered\n",
    "        manual_exclusions = {'got', 'get', 'just', 'like', 'many', 'much', 'very', 'really', 'make'}\n",
    "        \n",
    "        return (word.lower() not in stop_words and\n",
    "                word.lower() not in manual_exclusions and\n",
    "                len(word) > 2 and  # Exclude very short words\n",
    "                not any(c.isdigit() for c in word) and  # Exclude words with numbers\n",
    "                re.match(r'^[a-z]+$', word.lower()))  # Only pure alphabetic words\n",
    "                \n",
    "    # Check if seed words were provided in the input\n",
    "    if hasattr(input_data, 'seed_words') and input_data.seed_words and input_data.seed_words.strip().lower() != \"none\":\n",
    "        seed_input = input_data.seed_words.strip()\n",
    "\n",
    "        if \":\" in seed_input:\n",
    "            use_group_label = True\n",
    "            for part in seed_input.split(\";\"):\n",
    "                part = part.strip()\n",
    "                if \":\" in part:\n",
    "                    group_label, word_str = part.split(\":\", 1)\n",
    "                    group_label = group_label.strip().lower()\n",
    "                    words = [w.strip().lower() for w in word_str.split(\",\") if w.strip()]\n",
    "                    seed_groups[group_label] = set(words)\n",
    "                    seed_words.append(group_label)\n",
    "                    print(f\"Group mode: all {words} will be treated as '{group_label}'\")\n",
    "                else:\n",
    "                    individuals = [w.strip().lower() for w in part.split(\",\") if w.strip()]\n",
    "                    seed_words.extend(individuals)\n",
    "                    print(f\"Individual mode: adding {individuals}\")\n",
    "        else:\n",
    "            seed_words = [w.strip().lower() for w in seed_input.split(\",\") if w.strip()]\n",
    "            print(f\"Pure individual word mode: using {seed_words}\")\n",
    "        if use_group_label:\n",
    "                sentences = [replace_group_words(text, seed_groups) for text in sentences]\n",
    "    else:\n",
    "        # Process sentences to get word frequencies for auto-selection of top words\n",
    "        print(\"WARNING: No seed words provided or 'NONE' specified. Using top frequent words as seeds... \")\n",
    "        excluded_words = stop_words.union(set(WORD_FAMILIES.keys()))\n",
    "        all_tokens = []\n",
    "        for sentence in sentences:\n",
    "            tokens = tokenize_and_filter([sentence], stop_list=stop_words,\n",
    "                                        lemmatize=True,\n",
    "                                        cross_pos_normalize=input_data.cross_pos_normalize)\n",
    "            filtered_tokens = [token.lower() for token in tokens if custom_word_filter(token)]\n",
    "            all_tokens.extend(filtered_tokens)\n",
    "        word_counts = Counter(all_tokens)\n",
    "        top_words = [word for word, _ in word_counts.most_common(30)\n",
    "                    if word.lower() not in excluded_words][:min(10, len(word_counts))]\n",
    "        seed_words = top_words\n",
    "        print(f\"Top frequent words as seeds: {seed_words}\")\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Clean and normalize seed words, but they're already preprocessed with word families\n",
    "    print(f\"Original seed words before cleaning: {seed_words}\")\n",
    "    clean_seed_words = clean_words(seed_words)\n",
    "    print(f\"Seed words after cleaning: {clean_seed_words}\")\n",
    "    clean_seeds = clean_words(seed_words)\n",
    "    seed_words = normalize_words(clean_seeds, stop_words, lemmatize=True, cross_pos_normalize=input_data.cross_pos_normalize)\n",
    "    seed_words = list(set(seed_words))\n",
    "    print(\"Final normalized seed words:\", seed_words)\n",
    "    \n",
    "    \n",
    "    # choose context source: keep stop-words only for RoBERTa\n",
    "    if input_data.clustering_method == 1:          # 1 = RoBERTa\n",
    "        sentences_for_embedding = sentences        # full context\n",
    "    else:                                          # 2-4 = Jaccard/PMI/TF-IDF\n",
    "        sentences_for_embedding = filtered_sentences\n",
    "\n",
    "    word_embeddings, similarity_matrix, co_occurrence_matrix = train_embedding(\n",
    "        sentences_for_embedding,\n",
    "        context_window = input_data.window_size, \n",
    "        stop_list  = stop_words, \n",
    "        seed_words = seed_words, \n",
    "        clustering_method  = input_data.clustering_method,\n",
    "        num_words = input_data.num_words, \n",
    "        lemmatize = True, \n",
    "        min_word_frequency = input_data.min_word_frequency,\n",
    "        reuse_clusterings  = input_data.reuse_clusterings,\n",
    "        cross_pos_normalize= getattr(input_data, 'cross_pos_normalize', False),\n",
    "        distance_metric = getattr(input_data, 'distance_metric', 'default'),\n",
    "        custom_word_filter = custom_word_filter\n",
    "    )\n",
    "\n",
    "    elapsed_time = time.time() - start\n",
    "    print(f\"Embedding generation completed in {elapsed_time:.1f} seconds\")\n",
    "\n",
    "    if word_embeddings is None:\n",
    "        print(\"Error: Failed to generate embeddings. Please check your input data and parameters.\")\n",
    "        return\n",
    "    # Plot Similarity Heatmap\n",
    "    \n",
    "    print(\"Plotting similarity heatmap...\")\n",
    "    fig_heat = plot_heatmap(\n",
    "        input_data.clustering_method, word_embeddings, \n",
    "        similarity_matrix, co_occurrence_matrix, input_data.distance_metric\n",
    "    )\n",
    "\n",
    "\n",
    "    print(\"-----------------------------------------------\")\n",
    "    print(\"Plotting semantic network (plain, no categories)...\")\n",
    "    fig_sn1 = plot_semantic_network(\n",
    "        word_embeddings,\n",
    "        [] if auto_selected_seeds else seed_words,\n",
    "        input_data.clustering_method,\n",
    "        similarity_matrix, co_occurrence_matrix,\n",
    "        semantic_categories=None,               \n",
    "        link_threshold = input_data.link_threshold,\n",
    "        link_color_threshold= input_data.link_color_threshold,\n",
    "        distance_metric=getattr(input_data, 'distance_metric', 'default'))\n",
    "    \n",
    "    filename = f\"semantic_network_plain_m{input_data.clustering_method}_{input_data.distance_metric}.png\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    out_path = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "    fig_sn1.suptitle(\"Semantic Network (Plain)\", fontsize=30, y=0.98, fontweight='bold')\n",
    "    fig_sn1.subplots_adjust(top=0.95)\n",
    "\n",
    "    fig_sn1.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"✔ [OK] Saved {out_path}\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    \n",
    "    # Coloured visualisation (only if custom colors are enabled) \n",
    "    if getattr(input_data, 'custom_colors', False):\n",
    "        # Print the actual list of words used in the network so users can match for coloring\n",
    "        print(\"\\n Word list available for coloring:\")\n",
    "        print(sorted(list(word_embeddings.keys())))\n",
    "        print(\"\\nUsing predefined semantic categories for custom grouping\")\n",
    "        # Use seed_words instead of empty list for the colored visualization\n",
    "        fig_sn2 = plot_semantic_network(\n",
    "            word_embeddings,\n",
    "            seed_words,\n",
    "            input_data.clustering_method,\n",
    "            similarity_matrix, co_occurrence_matrix,\n",
    "            semantic_categories = input_data.semantic_categories,\n",
    "            link_threshold     = input_data.link_threshold,\n",
    "            link_color_threshold = input_data.link_color_threshold,\n",
    "            distance_metric=getattr(input_data, 'distance_metric', 'default')\n",
    "        )\n",
    "        filename = f\"semantic_network_customcolor_m{input_data.clustering_method}_{input_data.distance_metric}.png\"\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        out_path = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "        fig_sn2.suptitle(\"Semantic Network (Custom Colors)\", fontsize=30, y=0.98, fontweight='bold')\n",
    "        fig_sn2.subplots_adjust(top=0.95)\n",
    "\n",
    "        fig_sn2.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"✔ [OK] Saved {out_path}\")\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "    # Add a second network visualization that removes seed nodes\n",
    "    print(\"\\nGenerating secondary network with seed nodes hidden...\")\n",
    "    \n",
    "    # Filter out seed words from word embeddings and matrices\n",
    "    non_seed_words = [word for word in word_embeddings.keys() if word not in seed_words]\n",
    "    \n",
    "    if len(non_seed_words) > 5:  # Only proceed if we have enough nodes to make a meaningful network\n",
    "        non_seed_embeddings = {word: word_embeddings[word] for word in non_seed_words}\n",
    "        \n",
    "        # Create filtered similarity/co-occurrence matrices\n",
    "        if similarity_matrix is not None:\n",
    "            words = list(word_embeddings.keys())\n",
    "            indices = [words.index(word) for word in non_seed_words]\n",
    "            filtered_similarity = similarity_matrix[np.ix_(indices, indices)]\n",
    "        else:\n",
    "            filtered_similarity = None\n",
    "            \n",
    "        if co_occurrence_matrix is not None:\n",
    "            words = list(word_embeddings.keys())\n",
    "            indices = [words.index(word) for word in non_seed_words]\n",
    "            filtered_co_occurrence = co_occurrence_matrix[np.ix_(indices, indices)]\n",
    "        else:\n",
    "            filtered_co_occurrence = None\n",
    "        \n",
    "        # Plot the filtered network\n",
    "        print(f\"Plotting secondary network with {len(non_seed_words)} nodes (seeds hidden)...\")\n",
    "        plt.figure(figsize=(16, 12))\n",
    "        fig_sn3 = plot_semantic_network(\n",
    "            non_seed_embeddings, [], \n",
    "            input_data.clustering_method, \n",
    "            filtered_similarity, filtered_co_occurrence, \n",
    "            semantic_categories=None,\n",
    "            link_threshold=input_data.link_threshold,\n",
    "            link_color_threshold=input_data.link_color_threshold,\n",
    "            distance_metric=getattr(input_data, 'distance_metric', 'default')\n",
    "        )\n",
    "        filename = f\"semantic_network_noseeds_m{input_data.clustering_method}_{input_data.distance_metric}.png\"\n",
    "        out_path = os.path.join(OUTPUT_DIR, filename)\n",
    "        fig_sn3.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"✔ [OK] Saved {out_path}\")\n",
    "\n",
    "        fig_sn3.suptitle(\"Semantic Network (Seeds Hidden)\", fontsize=30, y=0.98, fontweight='bold')\n",
    "        fig_sn3.subplots_adjust(top=0.95)\n",
    "        plt.show()\n",
    "        \n",
    "        # If custom coloring was used in the first visualization, also do it for the second\n",
    "        if hasattr(input_data, 'custom_colors') and input_data.custom_colors and hasattr(input_data, 'semantic_categories'):\n",
    "            semantic_categories = input_data.semantic_categories\n",
    "            print(\"\\nGenerating secondary network with custom grouping (seeds hidden)...\")\n",
    "            fig_sn4 = plot_semantic_network(\n",
    "                non_seed_embeddings, [], \n",
    "                input_data.clustering_method, \n",
    "                filtered_similarity, filtered_co_occurrence, \n",
    "                semantic_categories=semantic_categories,\n",
    "                link_threshold=input_data.link_threshold,\n",
    "                link_color_threshold=input_data.link_color_threshold,\n",
    "                distance_metric=getattr(input_data, 'distance_metric', 'default')\n",
    "            )\n",
    "            filename = f\"semantic_network_noseeds_customcolor_m{input_data.clustering_method}_{input_data.distance_metric}.png\"\n",
    "            out_path = os.path.join(OUTPUT_DIR, filename)\n",
    "            fig_sn4.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "            print(f\"✔ [OK] Saved {out_path}\")\n",
    "\n",
    "            fig_sn4.suptitle(\"Semantic Network with Custom Grouping (Seeds Hidden)\", fontsize=30, y=0.98, fontweight='bold')\n",
    "            fig_sn4.subplots_adjust(top=0.95)\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"Not enough non-seed nodes to generate a meaningful secondary network.\")\n",
    "    \n",
    "    # Print the list of actual nodes used in the network (excluding seeds)\n",
    "    print(\"\\nFinal nodes used in network (excluding seeds):\")\n",
    "    if 'non_seed_words' in locals() and len(non_seed_words) > 0:\n",
    "        # Get frequencies for each node and sort by frequency (highest first)\n",
    "        print(f\"Total non-seed words: {len(non_seed_words)}\")\n",
    "        \n",
    "        # Check if word_frequencies is defined, otherwise use the word_counts from earlier\n",
    "        if 'word_frequencies' not in locals() and 'word_counts' in locals():\n",
    "            word_frequencies = word_counts\n",
    "        elif 'word_frequencies' not in locals():\n",
    "            print(\"Warning: Word frequency information not available\")\n",
    "            # Just print the words without frequencies\n",
    "            for word in sorted(non_seed_words):\n",
    "                print(f\"- {word}\")\n",
    "        else:\n",
    "            # Create list of (word, frequency) pairs and sort by frequency\n",
    "            freq_sorted_words = [(word, word_frequencies[word]) for word in non_seed_words if word in word_frequencies]\n",
    "            freq_sorted_words.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            for word, freq in freq_sorted_words:\n",
    "                print(f\"- {word}: {freq}\")\n",
    "    else:\n",
    "        print(\"No non-seed nodes were used in the network.uelse\")\n",
    "    \n",
    "    # Print the number of filtered sentences used\n",
    "    if 'filtered_sentences' in locals():\n",
    "        print(f\"\\nTotal number of filtered sentences used: {len(filtered_sentences)}\")\n",
    "        if 'seed_words' in locals() and seed_words:\n",
    "            # Enhanced seed word detection using word families\n",
    "            seed_containing_sentences = 0\n",
    "            for sentence in filtered_sentences:\n",
    "                sentence_lower = sentence.lower()\n",
    "                contains_seed = False\n",
    "                for seed in seed_words:\n",
    "                    # Check direct match\n",
    "                    if seed.lower() in sentence_lower:\n",
    "                        contains_seed = True\n",
    "                        break\n",
    "                    # Check word family variants\n",
    "                    for family, variants in WORD_FAMILIES.items():\n",
    "                        if seed.lower() == family.lower() or seed.lower() in [v.lower() for v in variants]:\n",
    "                            if any(variant.lower() in sentence_lower for variant in variants):\n",
    "                                contains_seed = True\n",
    "                                break\n",
    "                    if contains_seed:\n",
    "                        break\n",
    "                if contains_seed:\n",
    "                    seed_containing_sentences += 1\n",
    "            print(f\"Number of filtered sentences containing seed words: {seed_containing_sentences}\")\n",
    "    \n",
    "    print(\"\\nAnalysis complete\")\n",
    "    print(f\"\\nNetwork visualization method: {input_data.clustering_method}\")\n",
    "    \n",
    "    if input_data.clustering_method == 1:\n",
    "        print(\"Method: RoBERTa – Shows semantic relationships based on contextual embeddings\")\n",
    "    elif input_data.clustering_method == 2:\n",
    "        if input_data.distance_metric == \"cosine\":\n",
    "            print(\"Method: Jaccard (cosine) – Uses context window vectors to compute cosine-based similarity between word usage patterns\")\n",
    "        elif input_data.distance_metric == \"default\":\n",
    "            print(\"Method: Jaccard (default) – Uses binary co-occurrence counts within a context window to capture word overlap\")\n",
    "    elif input_data.clustering_method == 3:\n",
    "        print(\"Method: PMI – Highlights statistically significant word associations based on pointwise mutual information\")\n",
    "    elif input_data.clustering_method == 4:\n",
    "        if input_data.distance_metric == \"cosine\":\n",
    "            print(\"Method: TF-IDF (cosine) – Uses TF-IDF-weighted context vectors to compute cosine similarity between words\")\n",
    "        elif input_data.distance_metric == \"default\":\n",
    "            print(\"Method: TF-IDF (default) – Uses raw TF-IDF-weighted co-occurrence scores for word associations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c35e797",
   "metadata": {},
   "source": [
    "### 6. Code-based Heatmap \n",
    "\n",
    "This function builds & displays a heat-map of how often the top N interview “codes” appear together across transcripts—optionally runs hierarchical clustering so related codes sit next to each other—giving a quick visual of thematic overlap in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55402bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeatmapInput(BaseModel):\n",
    "    filepath: FilePath \n",
    "    num_codes: int = 10\n",
    "    seed_codes: Optional[List[str]] = None  # ← New\n",
    "    projects: Optional[List[str]] = None \n",
    "    data_groups: Optional[List[str]] = None\n",
    "    clustered: bool = True\n",
    "\n",
    "    @field_validator(\"num_codes\")\n",
    "    def validate_num_codes(cls, v):\n",
    "        if v <= 0:\n",
    "            raise ValueError(\"num_codes must be greater than 0\")\n",
    "        return v\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=10000)\n",
    "def parse_string_list(value: Union[str, list, None]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Performance-optimized string-formatted list parser for code lists.\n",
    "    \n",
    "    Args:\n",
    "        value: String, list or None containing codes\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: Cleaned and parsed list of codes\n",
    "    \"\"\"\n",
    "    if pd.isna(value) or value == \"\" or value is None:\n",
    "        return []\n",
    "    \n",
    "    if isinstance(value, list):\n",
    "        return [str(item).lower().strip() for item in value if item]\n",
    "        \n",
    "    if isinstance(value, str):\n",
    "        value = value.strip()\n",
    "        \n",
    "        if value in [\"[]\", \"['']\", '[\"\"]', \"nan\", \"NaN\"]:\n",
    "            return []\n",
    "            \n",
    "        try:\n",
    "            if value.startswith(\"[\") and value.endswith(\"]\"):\n",
    "                parsed = ast.literal_eval(value) \n",
    "                if isinstance(parsed, list):\n",
    "                    return [str(item).lower().strip() for item in parsed if item and str(item).strip()]\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass\n",
    "            \n",
    "        try:\n",
    "            cleaned = value.strip(\"[]\").replace(\"'\", \"\").replace('\"', \"\")\n",
    "            if cleaned:\n",
    "                items = [item.strip().lower() for item in cleaned.split(\",\")]\n",
    "                return [item for item in items if item]\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    return []\n",
    "\n",
    "\n",
    "def create_code_cooccurrence_heatmap(input_data: HeatmapInput):\n",
    "    filepath = input_data.filepath\n",
    "    num_codes = input_data.num_codes\n",
    "    projects = input_data.projects\n",
    "    data_groups = input_data.data_groups\n",
    "    clustered = input_data.clustered\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Filter by projects \n",
    "    if projects:\n",
    "        df = df[df['project'].isin(projects)]\n",
    "\n",
    "    # Filter by data groups\n",
    "    if data_groups:\n",
    "        df = df[df['data_group'].apply(lambda x: any(g in parse_string_list(x) for g in data_groups))]\n",
    "\n",
    "    # Vectorized code parsing\n",
    "    all_codes = []\n",
    "    for codes in df['codes'].dropna():\n",
    "        all_codes.extend(parse_string_list(codes))\n",
    "\n",
    "    # Get top N most frequent codes\n",
    "    if input_data.seed_codes:\n",
    "        seed_set = set(code.lower().strip() for code in input_data.seed_codes)\n",
    "        cooccurrence_counter = Counter()\n",
    "\n",
    "        # Count co-occurring codes with seeds\n",
    "        for codes in df['codes'].dropna():\n",
    "            code_list = set(parse_string_list(codes))\n",
    "            if seed_set & code_list:  # If any seed is in the list\n",
    "                overlapping = code_list - seed_set\n",
    "                cooccurrence_counter.update(overlapping)\n",
    "\n",
    "        # Add top-N co-occurring codes\n",
    "        top_overlap = [code for code, _ in cooccurrence_counter.most_common(num_codes)]\n",
    "        selected_codes = list(seed_set) + top_overlap\n",
    "        print(f\"Selected codes: {selected_codes}\")\n",
    "    else:\n",
    "        # Use top-N most frequent codes in corpus\n",
    "        selected_codes = [code for code, _ in Counter(all_codes).most_common(num_codes)]\n",
    "        print(f\"Top {num_codes} most frequent codes: {selected_codes}\")\n",
    "\n",
    "    # Check if we have any codes to analyze\n",
    "    if not selected_codes:\n",
    "        print(\"No codes found to analyze. Please check your input data.\")\n",
    "        return\n",
    "\n",
    "    # Co-occurrence matrix using vectorized operations\n",
    "    cooc_matrix = np.zeros((len(selected_codes), len(selected_codes)))\n",
    "    \n",
    "    for codes in df['codes'].dropna():\n",
    "        codes_set = set(parse_string_list(codes)).intersection(selected_codes)\n",
    "        for i, code1 in enumerate(selected_codes):\n",
    "            for j in range(i + 1, len(selected_codes)):\n",
    "                code2 = selected_codes[j]\n",
    "                if code1 in codes_set and code2 in codes_set:\n",
    "                    cooc_matrix[i][j] += 1\n",
    "                    cooc_matrix[j][i] += 1\n",
    "\n",
    "    # Check if matrix is empty\n",
    "    if np.all(cooc_matrix == 0):\n",
    "        print(\"No co-occurrences found. Please check your input data.\")\n",
    "        return\n",
    "\n",
    "    heatmap_df = pd.DataFrame(cooc_matrix, index=selected_codes, columns=selected_codes)\n",
    "    plt.style.use('dark_background')\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    \n",
    "    if clustered:\n",
    "        # Convert co-occurrence matrix to proper distance format for linkage\n",
    "        row_linkage = hierarchy.linkage(pdist(heatmap_df), method='ward')\n",
    "        col_linkage = hierarchy.linkage(pdist(heatmap_df.T), method='ward')\n",
    "        \n",
    "        g = sns.clustermap(heatmap_df,\n",
    "                        annot=True,\n",
    "                        fmt='g',\n",
    "                        cmap='inferno',\n",
    "                        row_linkage=row_linkage,\n",
    "                        col_linkage=col_linkage,\n",
    "                        figsize=(12, 10),\n",
    "                        dendrogram_ratio=0.2,\n",
    "                        colors_ratio=0.03)\n",
    "        g.fig.patch.set_facecolor('black')\n",
    "        g.ax_heatmap.set_facecolor('black')\n",
    "\n",
    "        for item in [g.ax_row_dendrogram, g.ax_col_dendrogram]:\n",
    "            item.set_facecolor('black')\n",
    "            for c in item.collections:\n",
    "                c.set_color('white')\n",
    "\n",
    "        filename = f\"code_heatmap_clustered_{num_codes}.png\"\n",
    "        out_path = os.path.join(OUTPUT_DIR, filename)\n",
    "        g.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"✔ [OK] Saved clustered heatmap: {out_path}\")\n",
    "    else:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        ax = sns.heatmap(\n",
    "            heatmap_df,\n",
    "            annot=True,\n",
    "            fmt='g',\n",
    "            cmap='inferno'\n",
    "        )\n",
    "        ax.set_title('Code Co-occurrence Matrix')\n",
    "\n",
    "        filename = f\"code_heatmap_plain_{num_codes}.png\"\n",
    "        out_path = os.path.join(OUTPUT_DIR, filename)\n",
    "        plt.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"✔ [OK] Saved plain heatmap: {out_path}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37a75a9",
   "metadata": {},
   "source": [
    "# Data Overview "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1976323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Overview and Preparation \n",
    "\n",
    "# Path to your cleaned data\n",
    "DATA_PATH = CSV_PATH  # alias for clarity\n",
    "\n",
    "# --- Schema and Required Fields ---\n",
    "SCHEMA = {\n",
    "    \"project\": str, # List project \n",
    "    \"number\": str, # Position information\n",
    "    \"reference\": int, # Position information\n",
    "    \"text\": str, # Content, critical field: must not be empty\n",
    "    \"document\": str, # Data source, Critical field: must not be empty\n",
    "    \"old_codes\": list[str], # Optional: codings, must be a list of strings\n",
    "    \"start_position\": int, # Position information\n",
    "    \"end_position\": int, # Position information\n",
    "    \"data_group\": list[str], # Optional, to differentiate document sets: Must be a list of strings\n",
    "    \"text_length\": int, # Optional: NLP info\n",
    "    \"word_count\": int, # Optional: NLP info\n",
    "    \"doc_id\": str, # Optional: NLP info, unique paragrah level identifier\n",
    "    \"codes\": list[str] # critical for analyses with codes, Must be a list of strings\n",
    "}\n",
    "\n",
    "REQUIRED_FIELDS = [\"text\", \"document\", \"project\"]\n",
    "\n",
    "# --- Safe List Conversion Function ---\n",
    "def safe_convert_list(x):\n",
    "    \"\"\"Safe conversion of various formats to a list, handling edge cases.\"\"\"\n",
    "    try:\n",
    "        if isinstance(x, float) and np.isnan(x):\n",
    "            return []\n",
    "        if x in (None, \"\", \"nan\", \"NaN\"):\n",
    "            return []\n",
    "        if isinstance(x, list):\n",
    "            return [str(i).strip().strip(\"\\\"'[]\") for i in x if str(i).strip()]\n",
    "\n",
    "        x = str(x).strip()\n",
    "        if x == \"[]\":\n",
    "            return []\n",
    "\n",
    "        x = x.replace(\"’\", '\"').replace(\"][\", \",\").replace(\"],['\", '\",\"').replace('\"\"', '\"')\n",
    "\n",
    "        if not x.startswith(\"[\"):\n",
    "            x = \"[\" + x\n",
    "        if not x.endswith(\"]\"):\n",
    "            x = x + \"]\"\n",
    "\n",
    "        try:\n",
    "            items = json.loads(x)\n",
    "        except Exception:\n",
    "            items = x.strip(\"[]\").split(\",\")\n",
    "        return [str(i).strip().strip(\"\\\"'[]\") for i in items if str(i).strip()]\n",
    "\n",
    "    except Exception:\n",
    "        print(f\"Warning: Could not convert value: {x}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def parse_list_compat(val):\n",
    "    if pd.isna(val) or val in (None, \"\", \"nan\", \"NaN\"):\n",
    "        return []\n",
    "    if isinstance(val, list):\n",
    "        return val\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            if val.startswith(\"[\") and val.endswith(\"]\"):\n",
    "                return json.loads(val)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return [v.strip() for v in val.strip(\"[]\").replace(\"'\", \"\").replace('\"', \"\").split(\",\") if v.strip()]\n",
    "    return [val]\n",
    "\n",
    "# --- Load Data ---\n",
    "df_clean = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# --- Required Fields ---\n",
    "missing_required = [c for c in REQUIRED_FIELDS if c not in df_clean.columns]\n",
    "if missing_required:\n",
    "    print(f\"Missing required fields: {missing_required}\")\n",
    "\n",
    "for col in REQUIRED_FIELDS:\n",
    "    if col in df_clean.columns:\n",
    "        n_empty = df_clean[col].isna().sum() + (df_clean[col] == \"\").sum()\n",
    "        print(f\"Empty '{col}’: {n_empty}\")\n",
    "\n",
    "\n",
    "# --- Show First 5 Records ---\n",
    "print(\"\\nFirst 5 records:\")\n",
    "pprint.pprint(df_clean.head(5).to_dict(orient=\"records\"))\n",
    "\n",
    "# --- Schema Match ---\n",
    "print(\"\\nSchema match by column:\")\n",
    "for col, expected in SCHEMA.items():\n",
    "    if col not in df_clean.columns:\n",
    "        print(f\"{col}: MISSING\")\n",
    "    else:\n",
    "        dtype = (df_clean[col].dropna().map(type).mode()[0] if not df_clean[col].dropna().empty else None)\n",
    "        print(f\"{col}: expected {expected.__name__}, found {dtype.__name__ if dtype else 'None'}\")\n",
    "\n",
    "# --- Apply List Conversion to Specific Columns ---\n",
    "list_columns = ['old_codes', 'data_group', 'codes']\n",
    "for col in list_columns:\n",
    "    if col in df_clean.columns:\n",
    "        print(f\"\\nConverting and sanitizing {col}...\")\n",
    "        df_clean[col] = df_clean[col].apply(safe_convert_list)\n",
    "\n",
    "# --- Verify List Conversion ---\n",
    "print(\"\\nVerification of list columns:\")\n",
    "for col in list_columns:\n",
    "    if col in df_clean.columns:\n",
    "        invalid = df_clean[~df_clean[col].apply(lambda x: isinstance(x, list))].shape[0]\n",
    "        print(f\"{col}: {invalid} invalid entries\")\n",
    "        if invalid > 0:\n",
    "            print(\"Sample of first invalid entry:\")\n",
    "            print(df_clean[~df_clean[col].apply(lambda x: isinstance(x, list))][col].iloc[0])\n",
    "        else:\n",
    "            print(f\"Sample of cleaned {col}:\")\n",
    "            sample = df_clean[col].iloc[0] if len(df_clean) > 0 else []\n",
    "            print(f\"First entry: {sample}\")\n",
    "\n",
    "# --- Cleaning Summary ---\n",
    "print(\"\\nCleaning Summary:\")\n",
    "for col in list_columns:\n",
    "    if col in df_clean.columns:\n",
    "        empty = df_clean[col].apply(lambda x: len(x) == 0).sum()\n",
    "        print(f\"{col} → empty: {empty}/{len(df_clean)}\")\n",
    "        \n",
    "# --- Add word_count column if needed ---\n",
    "if \"word_count\" not in df_clean.columns and \"text\" in df_clean.columns:\n",
    "    df_clean[\"word_count\"] = df_clean[\"text\"].str.split().str.len().fillna(0)\n",
    "\n",
    "print(\"\\nDataset overview:\")\n",
    "print(df_clean.info())\n",
    "\n",
    "# --- Codes column: parse and show top codes ---\n",
    "if 'codes' in df_clean.columns:\n",
    "    codes_flat = df_clean['codes'].dropna().apply(safe_convert_list).explode()\n",
    "    code_counts = codes_flat.value_counts().head(20)\n",
    "    print(\"\\nTop 20 codes by frequency:\")\n",
    "    print(code_counts)\n",
    "else:\n",
    "    print(\"No 'codes' column found in the dataframe\")\n",
    "\n",
    "# --- Count documents by project ---\n",
    "if 'project' in df_clean.columns:\n",
    "    if 'document' in df_clean.columns:\n",
    "        project_doc_counts = df_clean.groupby('project')['document'].nunique()\n",
    "    else:\n",
    "        project_doc_counts = df_clean['project'].value_counts()\n",
    "    print(\"\\nNumber of documents by project:\")\n",
    "    print(project_doc_counts)\n",
    "\n",
    "# --- Top values for data_group ---\n",
    "if 'data_group' in df_clean.columns:\n",
    "    dg_flat = df_clean['data_group'].dropna().apply(safe_convert_list).explode()\n",
    "    dg_counts = dg_flat.value_counts().head(10)\n",
    "    print(\"\\nTop 10 data_group values by frequency:\")\n",
    "    print(dg_counts)\n",
    "else:\n",
    "    print(\"\\nNo 'data_group' column found in the dataframe\")\n",
    "\n",
    "# --- Word Count ---\n",
    "if 'word_count' in df_clean.columns:\n",
    "    avg_word_count = df_clean['word_count'].mean()\n",
    "    print(f\"\\nAverage word count in text: {avg_word_count:.2f}\")\n",
    "elif 'text' in df_clean.columns:\n",
    "    df_clean['word_count'] = df_clean['text'].str.split().str.len().fillna(0)\n",
    "    avg_word_count = df_clean['word_count'].mean()\n",
    "    print(f\"\\nAverage word count in text: {avg_word_count:.2f}\")\n",
    "else:\n",
    "    print(\"No 'text' column found in the dataframe\")\n",
    "\n",
    "# --- Descriptive Statistics ---\n",
    "print(\"\\nMissing Values Count:\")\n",
    "print(df_clean.isnull().sum())\n",
    "\n",
    "numeric_cols = ['text_length', 'word_count']\n",
    "print(\"\\nDescriptive Statistics for Numeric Columns:\")\n",
    "for col in numeric_cols:\n",
    "    if col in df_clean.columns:\n",
    "        print(f\"\\nStats for {col}:\")\n",
    "        print(f\"Count: {df_clean[col].count()}\")\n",
    "        print(f\"Median: {df_clean[col].median():.2f}\")\n",
    "        print(f\"Standard Deviation: {df_clean[col].std():.2f}\")\n",
    "\n",
    "print(\"\\nDataset Overview:\")\n",
    "print(df_clean.info())\n",
    "\n",
    "# --- Unique Projects ---\n",
    "if 'project' in df_clean.columns:\n",
    "    projects = df_clean['project'].unique()\n",
    "    print(\"\\nUnique Projects:\")\n",
    "    for project in projects:\n",
    "        print(project)\n",
    "\n",
    "print(df_clean.head(5))\n",
    "\n",
    "\n",
    "# --- Load and Verify Saved File ---\n",
    "output_file_dropbox = os.path.join(DATA_DIR, '1_cleaned_data.csv')\n",
    "output_file_combined = os.path.join(BACKUP_DIR, '1_cleaned_data.csv')\n",
    "print(f\"\\nCleaned data saved to: {output_file_combined}\")\n",
    "df_clean.to_csv(output_file_dropbox, index=False)\n",
    "df_clean.to_csv(output_file_combined, index=False)\n",
    "print(f\"\\nCleaned data saved to: {output_file_combined}\")\n",
    "df = pd.read_csv(output_file_combined)\n",
    "if 'project' in df.columns:\n",
    "    projects_loaded = df['project'].unique()\n",
    "    print(\"\\nUnique Projects (from loaded file):\")\n",
    "    for project in projects_loaded:\n",
    "        print(project)\n",
    "\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da582a4",
   "metadata": {},
   "source": [
    "# Basic Analytics Tool "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3c3456",
   "metadata": {},
   "source": [
    "## 1. Wordcloud\n",
    "\n",
    "This section contains the wordcloud execution block. this produces a wordcloud from the data loaded, showing words that come up more frequent in larger size. The heatmap allows color coding by user defined categories, that represent concepts or themes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1df4027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter text based on project and terms\n",
    "filtered_df = df.copy()\n",
    "if 'project' in filtered_df.columns:\n",
    "    filtered_df = filtered_df[filtered_df['project'] == 'oral_history'] # Adjust as needed \n",
    "\n",
    "if 'text' in filtered_df.columns:\n",
    "    text_series = filtered_df['text'].fillna('').astype(str)\n",
    "    mask = text_series.str.lower().apply(lambda x: any(term in x for term in ['education', 'learning', 'teaching', 'student', 'school', 'classroom', 'curriculum', 'academic']))\n",
    "    text_series = text_series[mask]\n",
    "\n",
    "# Categories \n",
    "cmap = cm.get_cmap(\"mako\", 5)\n",
    "categories = {\n",
    "    \"People & Relations\": {\n",
    "        \"words\": {\n",
    "            \"people\", \"student\", \"students\", \"teacher\", \"teachers\", \"professor\", \"professors\",\n",
    "            \"advisor\", \"mentor\", \"classmates\", \"friends\", \"colleague\", \"colleagues\",\n",
    "            \"mother\", \"mom\", \"father\", \"dad\", \"family\", \"kids\", \"children\",\n",
    "            \"wife\", \"husband\", \"brother\", \"sister\"\n",
    "        },\n",
    "        \"color\": cmap(0)\n",
    "    },\n",
    "    \"Education & Career\": {\n",
    "        \"words\": {\n",
    "            \"school\", \"college\", \"university\", \"department\", \"campus\",\n",
    "            \"course\", \"courses\", \"class\", \"classes\", \"curriculum\", \"degree\",\n",
    "            \"major\", \"minor\", \"graduate\", \"graduated\", \"thesis\", \"exam\",\n",
    "            \"research\", \"lab\", \"laboratory\", \"project\", \"projects\",\n",
    "            \"engineering\", \"engineer\", \"physics\", \"mathematics\",\n",
    "            \"industry\", \"company\", \"career\", \"job\", \"internship\"\n",
    "        },\n",
    "        \"color\": cmap(1)\n",
    "    },\n",
    "    \"Emotions & Cognition\": {\n",
    "        \"words\": {\n",
    "            \"think\", \"thought\", \"know\", \"understand\", \"learn\", \"learning\",\n",
    "            \"decide\", \"decision\", \"believe\", \"remember\", \"idea\", \"ideas\",\n",
    "            \"feel\", \"feeling\", \"feelings\", \"excited\", \"interested\",\n",
    "            \"curious\", \"happy\", \"proud\", \"worried\", \"scared\", \"confused\"\n",
    "        },\n",
    "        \"color\": cmap(2)\n",
    "    },\n",
    "    \"Time / Duration\": {\n",
    "        \"words\": {\n",
    "            \"first\", \"second\", \"later\", \"since\", \"before\", \"after\",\n",
    "            \"started\", \"start\", \"begin\", \"early\", \"late\",\n",
    "            \"day\", \"days\", \"week\", \"weeks\", \"month\", \"months\",\n",
    "            \"year\", \"years\", \"semester\", \"summer\", \"winter\", \"spring\", \"fall\"\n",
    "        },\n",
    "        \"color\": cmap(3)\n",
    "    },\n",
    "    \"Daily Activities & Work\": {\n",
    "        \"words\": {\n",
    "            \"work\", \"working\", \"teach\", \"teaching\", \"study\", \"studying\",\n",
    "            \"read\", \"reading\", \"write\", \"writing\", \"talk\", \"talking\",\n",
    "            \"meet\", \"meeting\", \"present\", \"presentation\", \"travel\", \"move\",\n",
    "            \"build\", \"design\", \"make\", \"made\", \"fix\", \"support\", \"help\"\n",
    "        },\n",
    "        \"color\": cmap(4)\n",
    "    },\n",
    "}\n",
    "# Word cloud\n",
    "generate_wordcloud(\n",
    "    text_series=text_series,\n",
    "    stopwords_path=STOP_LIST_FILE,\n",
    "    title='Word Cloud',\n",
    "    out_dir=OUTPUT_DIR,  # change if needed\n",
    "    categories=categories\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9543eac8",
   "metadata": {},
   "source": [
    "# Intermediate Analytics Tools \n",
    "\n",
    "This section contains all heatmap and network execuion blocks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19757f9e",
   "metadata": {},
   "source": [
    "### User Configuration for Intermediate Visualization Tool\n",
    "\n",
    "Overall Embedding/Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd42eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------\n",
    "# Variables Preparation\n",
    "#---------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "OVERVIEW:\n",
    "----------------------\n",
    "This script generates semantic networks based on the specified clustering method,\n",
    "visualizing relationships between concepts in the text corpus. Heatmaps and t-SNE \n",
    "plots are also generated to help visualize the relationships.\n",
    "\n",
    "USAGE INSTRUCTIONS:\n",
    "----------------------\n",
    "1. Configure the parameters in the CONFIG section below\n",
    "2. Save this notebook, run each block\n",
    "3. Alternatively, export to a Python script\n",
    "4. Execute with: python semantic_network.py\n",
    "5. This is what a user would tweak, or be prompted to enter in an interactive version.\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # ========================= CONFIG =========================\n",
    "    # Paths & Stop-list\n",
    "    csv_path               = CSV_PATH # Your dataset path\n",
    "    stop_list_path         = STOP_LIST_FILE\n",
    "    use_custom_stoplist    = True\n",
    "\n",
    "    # Core Analysis\n",
    "    clustering_method = 3           # 1 = RoBERTa, 2 = Jaccard, 3 = PMI, 4 = TF-IDF\n",
    "    distance_metric   = \"cosine\"   \n",
    "    \n",
    "# Note on clustering method and distance metric:\n",
    "#\n",
    "# 1. RoBERTa (clustering_method = 1)\n",
    "#    - distance_metric is always \"default\" (ignored internally)\n",
    "#    - Uses contextual embeddings directly\n",
    "#\n",
    "# 2. Jaccard (clustering_method = 2)\n",
    "#    - \"default\": context-window overlap measured by Jaccard index\n",
    "#                 (set-based, binary overlap score)\n",
    "#    - \"cosine\" : context vectors built from co-occurrence counts,\n",
    "#                 compared using cosine similarity (frequency-sensitive)\n",
    "#\n",
    "# 3. PMI (clustering_method = 3)\n",
    "#    - \"default\": classic PMI co-occurrence scores\n",
    "#    - \"cosine\" : PMI-weighted context vectors compared with cosine similarity\n",
    "#                 (captures similarity of PMI distributions rather than raw scores)\n",
    "#\n",
    "# 4. TF-IDF (clustering_method = 4)\n",
    "#    - \"cosine\" : standard TF-IDF vectors compared with cosine similarity\n",
    "#                 (recommended; normalized similarity).\n",
    "#    - \"default\": experimental overlap-based method; shared context tokens are\n",
    "#                 weighted by their global TF-IDF scores (unnormalized)\n",
    "#                 → Use with caution; included as an optional example\n",
    "\n",
    "    window_size            = 20 # Context window size for co-occurrence\n",
    "    num_words              = 25 # Max number of top frequent words to analyze\n",
    "    min_word_frequency     = 2 # Ignore words that appear fewer times\n",
    "    reuse_clusterings      = False # Whether to reuse saved clustering results if available\n",
    "\n",
    "\n",
    "    # Preprocessing Filters\n",
    "    cross_pos_normalize    = True # Normalize words across parts of speech (e.g., \"learn\", \"learning\", \"learned\" -> \"learn\")\n",
    "    projects               = [\"oral_history\"]     # Filter by project names \n",
    "    data_groups            = [\"interview\"]   # Filter by data_groups \n",
    "    codes                  = [\"background\"]        # Analyse specific codes\n",
    "    excluded_codes         = ['interviewer']  # Exclude these codes, removing 'interviewer' is important for NLP\n",
    "\n",
    "    # Visualisation\n",
    "    title                = \"Semantic Network (all interviews, contextual embeddings)\"\n",
    "    link_threshold       = 0.50          \n",
    "    link_color_threshold = 0.75   # set to 99 to remove black links       \n",
    "    custom_colors = True                 \n",
    "\n",
    "    # Seeds & Colours\n",
    "    seed_words = \"education: learning, teaching, student, school, classroom, curriculum, academic\"\n",
    "\n",
    "    # You can provide seed words in two formats:\n",
    "    # 1. Grouped format (for custom node colors and labels):\n",
    "    #       \"GroupName1: word1, word2; GroupName2: word3, word4; ...\"\n",
    "    #    Example: \n",
    "    #       \"Therapy: therapy, physical therapy; Caregivers: caregivers, caregiver; family\"\n",
    "    #    → 'family' will be treated as its own group, using its own color.\n",
    "    #\n",
    "    # 2. Flat list format (no grouping, default color):\n",
    "    #       \"dementia, therapy, caregivers\"\n",
    "    #    → All words will appear as individual highlighted nodes with default styling.\n",
    "    \n",
    "    # consider inductive readings, and analysis \n",
    "    cmap = cm.get_cmap(\"mako\", 5)\n",
    "    semantic_categories = {\n",
    "    \"People & Relations\": {\n",
    "        \"words\": {\n",
    "            \"people\", \"student\", \"students\", \"teacher\", \"teachers\", \"professor\", \"professors\",\n",
    "            \"advisor\", \"mentor\", \"classmates\", \"friends\", \"colleague\", \"colleagues\",\n",
    "            \"mother\", \"mom\", \"father\", \"dad\", \"family\", \"kids\", \"children\",\n",
    "            \"wife\", \"husband\", \"brother\", \"sister\"\n",
    "        },\n",
    "        \"color\": cmap(0)\n",
    "    },\n",
    "    \"Education & Career\": {\n",
    "        \"words\": {\n",
    "            \"school\", \"college\", \"university\", \"department\", \"campus\",\n",
    "            \"course\", \"courses\", \"class\", \"classes\", \"curriculum\", \"degree\",\n",
    "            \"major\", \"minor\", \"graduate\", \"graduated\", \"thesis\", \"exam\",\n",
    "            \"research\", \"lab\", \"laboratory\", \"project\", \"projects\",\n",
    "            \"engineering\", \"engineer\", \"physics\", \"mathematics\",\n",
    "            \"industry\", \"company\", \"career\", \"job\", \"internship\"\n",
    "        },\n",
    "        \"color\": cmap(1)\n",
    "    },\n",
    "    \"Emotions & Cognition\": {\n",
    "        \"words\": {\n",
    "            \"think\", \"thought\", \"know\", \"understand\", \"learn\", \"learning\",\n",
    "            \"decide\", \"decision\", \"believe\", \"remember\", \"idea\", \"ideas\",\n",
    "            \"feel\", \"feeling\", \"feelings\", \"excited\", \"interested\",\n",
    "            \"curious\", \"happy\", \"proud\", \"worried\", \"scared\", \"confused\",\n",
    "            \"reflect\", \"contemplate\", \"ponder\", \"analyze\", \"reason\"\n",
    "        },\n",
    "        \"color\": cmap(2)\n",
    "    },\n",
    "    \"Time / Duration\": {\n",
    "        \"words\": {\n",
    "            \"first\", \"second\", \"later\", \"since\", \"before\", \"after\",\n",
    "            \"started\", \"start\", \"begin\", \"early\", \"late\",\n",
    "            \"day\", \"days\", \"week\", \"weeks\", \"month\", \"months\",\n",
    "            \"year\", \"years\", \"semester\", \"summer\", \"winter\", \"spring\", \"fall\"\n",
    "        },\n",
    "        \"color\": cmap(3)\n",
    "    },\n",
    "    \"Daily Activities & Work\": {\n",
    "        \"words\": {\n",
    "            \"work\", \"working\", \"teach\", \"teaching\", \"study\", \"studying\",\n",
    "            \"read\", \"reading\", \"write\", \"writing\", \"talk\", \"talking\",\n",
    "            \"meet\", \"meeting\", \"present\", \"presentation\", \"travel\", \"move\",\n",
    "            \"build\", \"design\", \"make\", \"made\", \"fix\", \"support\", \"help\",\n",
    "            \"eat\", \"sleep\", \"walk\", \"exercise\", \"commute\", \"cook\", \"clean\",\n",
    "            \"shop\", \"plan\", \"organize\", \"schedule\", \"prepare\", \"attend\"\n",
    "        },\n",
    "        \"color\": cmap(4)\n",
    "    },\n",
    "}  \n",
    "    \n",
    "    # Network Layout Parameters\n",
    "    network_layout = \"kamada-kawai\"   # Options: \"spring\", \"kamada-kawai\", \"circular\", \"shell\"\n",
    "    \n",
    "    # Heatmap Parameters\n",
    "    num_codes    = 7              # Expand up to this many codes for heatmap\n",
    "    seed_codes   = [\"background\"] # Start from these seed codes (if None, top-N most frequent codes are used)\n",
    "    clustered    = True           # If True, apply hierarchical clustering to rows/columns\n",
    "\n",
    "    # Notes:\n",
    "    # - If seed_codes is given → heatmap expands with top-N co-occurring codes. \n",
    "    # - If seed_codes is None   → heatmap uses top-N most frequent codes in corpus. \n",
    "    # - num_codes defines N in both cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7cb401",
   "metadata": {},
   "source": [
    "### 2. Word-based heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d9e8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSemantic Network Analysis Tool\\n\" + \"=\" * 30)\n",
    "\n",
    "try:\n",
    "    # Build pydantic object\n",
    "    params = VisualsInput(\n",
    "        filepath            = csv_path,\n",
    "        stop_list           = stop_list_path if use_custom_stoplist else None,\n",
    "        num_words           = num_words,\n",
    "        clustering_method   = clustering_method,\n",
    "        distance_metric     = distance_metric,\n",
    "        window_size         = window_size,\n",
    "        min_word_frequency  = min_word_frequency,\n",
    "        projects            = projects,\n",
    "        data_groups         = data_groups,\n",
    "        codes               = codes,\n",
    "        cross_pos_normalize = cross_pos_normalize\n",
    "    )\n",
    "\n",
    "        \n",
    "\n",
    "    # Attach non-schema extras\n",
    "    setattr(params, \"reuse_clusterings\",    reuse_clusterings)\n",
    "    setattr(params, \"seed_words\",           seed_words)\n",
    "    setattr(params, \"custom_colors\",        custom_colors)\n",
    "    setattr(params, \"semantic_categories\",  semantic_categories)\n",
    "    setattr(params, \"link_threshold\",       link_threshold)\n",
    "    setattr(params, \"link_color_threshold\", link_color_threshold)\n",
    "    setattr(params, \"excluded_codes\",       excluded_codes)    \n",
    "\n",
    "\n",
    "    # Creates: (1) plain, (2) coloured, (3) coloured+seed-hidden\n",
    "    run_heatmap_pipeline(params)\n",
    "\n",
    "\n",
    "except ValidationError as ve:\n",
    "    print(\"\\n⚠ Parameter error:\\n\", ve)\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠ Unexpected error: {e}\")\n",
    "finally:\n",
    "    print(\"\\nAnalysis process completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4eb354",
   "metadata": {},
   "source": [
    "### 3. tSNE plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee8ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSemantic Network Analysis Tool\\n\" + \"=\" * 30)\n",
    "\n",
    "try:\n",
    "    # Build pydantic object\n",
    "    params = VisualsInput(\n",
    "        filepath            = csv_path,\n",
    "        stop_list           = stop_list_path if use_custom_stoplist else None,\n",
    "        num_words           = num_words,\n",
    "        clustering_method   = clustering_method,\n",
    "        distance_metric     = distance_metric,\n",
    "        window_size         = window_size,\n",
    "        min_word_frequency  = min_word_frequency,\n",
    "        projects            = projects,\n",
    "        data_groups         = data_groups,\n",
    "        codes               = codes,\n",
    "        cross_pos_normalize = cross_pos_normalize\n",
    "    )\n",
    "\n",
    "        \n",
    "\n",
    "    # Attach non-schema extras\n",
    "    setattr(params, \"reuse_clusterings\",    reuse_clusterings)\n",
    "    setattr(params, \"seed_words\",           seed_words)\n",
    "    setattr(params, \"custom_colors\",        custom_colors)\n",
    "    setattr(params, \"semantic_categories\",  semantic_categories)\n",
    "    setattr(params, \"link_threshold\",       link_threshold)\n",
    "    setattr(params, \"link_color_threshold\", link_color_threshold)\n",
    "    setattr(params, \"excluded_codes\",       excluded_codes)    \n",
    "\n",
    "\n",
    "    # Creates: (1) plain, (2) coloured, (3) coloured+seed-hidden\n",
    "    run_tsne_pipeline(params)\n",
    "\n",
    "\n",
    "except ValidationError as ve:\n",
    "    print(\"\\n⚠ Parameter error:\\n\", ve)\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠ Unexpected error: {e}\")\n",
    "finally:\n",
    "    print(\"\\nAnalysis process completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fda4de1",
   "metadata": {},
   "source": [
    "### 4. Word-based Heatmap and Social Network \n",
    "(no custom colors, just blue nodes and yellow seeds, no edge bundling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26994053",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSemantic Network Analysis Tool\\n\" + \"=\" * 30)\n",
    "\n",
    "try:\n",
    "    # Build pydantic object\n",
    "    params = VisualsInput(\n",
    "        filepath            = csv_path,\n",
    "        stop_list           = stop_list_path if use_custom_stoplist else None,\n",
    "        num_words           = num_words,\n",
    "        clustering_method   = clustering_method,\n",
    "        distance_metric     = distance_metric,\n",
    "        window_size         = window_size,\n",
    "        min_word_frequency  = min_word_frequency,\n",
    "        projects            = projects,\n",
    "        data_groups         = data_groups,\n",
    "        codes               = codes,\n",
    "        cross_pos_normalize = cross_pos_normalize\n",
    "    )\n",
    "\n",
    "        \n",
    "\n",
    "    # Attach non-schema extras\n",
    "    setattr(params, \"reuse_clusterings\",    reuse_clusterings)\n",
    "    setattr(params, \"seed_words\",           seed_words)\n",
    "    setattr(params, \"custom_colors\",        custom_colors)\n",
    "    setattr(params, \"semantic_categories\",  semantic_categories)\n",
    "    setattr(params, \"link_threshold\",       link_threshold)\n",
    "    setattr(params, \"link_color_threshold\", link_color_threshold)\n",
    "    setattr(params, \"excluded_codes\",       excluded_codes)    \n",
    "\n",
    "\n",
    "    # Creates: (1) plain, (2) coloured, (3) coloured+seed-hidden\n",
    "    run_heatmap_network_plain_pipeline(params)\n",
    "\n",
    "\n",
    "except ValidationError as ve:\n",
    "    print(\"\\n⚠ Parameter error:\\n\", ve)\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠ Unexpected error: {e}\")\n",
    "finally:\n",
    "    print(\"\\nAnalysis process completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8ba687",
   "metadata": {},
   "source": [
    "### 5. Word-based Heatmap and Social Network \n",
    "(custom colors, no edge bundling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d88201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSemantic Network Analysis Tool\\n\" + \"=\" * 30)\n",
    "\n",
    "try:\n",
    "    # Build pydantic object\n",
    "    params = VisualsInput(\n",
    "        filepath            = csv_path,\n",
    "        stop_list           = stop_list_path if use_custom_stoplist else None,\n",
    "        num_words           = num_words,\n",
    "        clustering_method   = clustering_method,\n",
    "        distance_metric     = distance_metric,\n",
    "        window_size         = window_size,\n",
    "        min_word_frequency  = min_word_frequency,\n",
    "        projects            = projects,\n",
    "        data_groups         = data_groups,\n",
    "        codes               = codes,\n",
    "        cross_pos_normalize = cross_pos_normalize\n",
    "    )\n",
    "\n",
    "        \n",
    "\n",
    "    # Attach non-schema extras\n",
    "    setattr(params, \"reuse_clusterings\",    reuse_clusterings)\n",
    "    setattr(params, \"seed_words\",           seed_words)\n",
    "    setattr(params, \"custom_colors\",        custom_colors)\n",
    "    setattr(params, \"semantic_categories\",  semantic_categories)\n",
    "    setattr(params, \"link_threshold\",       link_threshold)\n",
    "    setattr(params, \"link_color_threshold\", link_color_threshold)\n",
    "    setattr(params, \"excluded_codes\",       excluded_codes)    \n",
    "\n",
    "\n",
    "    # Creates: (1) plain, (2) coloured, (3) coloured+seed-hidden\n",
    "    run_heatmap_network_pipeline(params)\n",
    "\n",
    "\n",
    "except ValidationError as ve:\n",
    "    print(\"\\n⚠ Parameter error:\\n\", ve)\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠ Unexpected error: {e}\")\n",
    "finally:\n",
    "    print(\"\\nAnalysis process completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6b780e",
   "metadata": {},
   "source": [
    "### 6. Code-based Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d33b0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = HeatmapInput(\n",
    "    filepath    = CSV_PATH,\n",
    "    num_codes   = num_codes,\n",
    "    seed_codes  = seed_codes,\n",
    "    projects    = projects,\n",
    "    data_groups = data_groups,\n",
    "    clustered   = clustered\n",
    ")\n",
    "\n",
    "# Run the heatmap generator\n",
    "create_code_cooccurrence_heatmap(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd93f8f",
   "metadata": {},
   "source": [
    "#  Advanced Analytic Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed709f2f",
   "metadata": {},
   "source": [
    "### User Configuration for Advanced Visualization Tool\n",
    "\n",
    "This section contains advanced heatmap and network combined execution blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbc21be",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5161537",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------\n",
    "# Variables Preparation\n",
    "#---------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "OVERVIEW:\n",
    "----------------------\n",
    "This script generates semantic networks based on the specified clustering method,\n",
    "visualizing relationships between concepts in the text corpus. Heatmaps and t-SNE \n",
    "plots are also generated to help visualize the relationships.\n",
    "\n",
    "USAGE INSTRUCTIONS:\n",
    "----------------------\n",
    "1. Configure the parameters in the CONFIG section below\n",
    "2. Save this notebook, run each block\n",
    "3. Alternatively, export to a Python script\n",
    "4. Execute with: python semantic_network.py\n",
    "5. This is what a user would tweak, or be prompted to enter in an interactive version.\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # ========================= CONFIG =========================\n",
    "    # Paths & Stop-list\n",
    "    csv_path               = CSV_PATH # Your dataset path\n",
    "    stop_list_path         = STOP_LIST_FILE\n",
    "    use_custom_stoplist    = True\n",
    "\n",
    "    # Core Analysis\n",
    "    clustering_method = 2           # 1 = RoBERTa, 2 = Jaccard, 3 = PMI, 4 = TF-IDF\n",
    "    distance_metric   = \"cosine\"   \n",
    "    \n",
    "    # Note on clustering method and distance metric:\n",
    "    # - If clustering_method == 1 (RoBERTa), distance_metric is always \"default\" (ignored internally)\n",
    "    # - For clustering_method in [2, 3, 4], distance_metric can be:\n",
    "    #     \"default\" → uses raw co-occurrence or weighted scores\n",
    "    #     \"cosine\"  → uses context or TF-IDF vectors with cosine similarity\n",
    "\n",
    "    window_size            = 20 # Context window size for co-occurrence\n",
    "    num_words              = 25 # Max number of top frequent words to analyze\n",
    "    min_word_frequency     = 2 # Ignore words that appear fewer times\n",
    "    reuse_clusterings      = False # Whether to reuse saved clustering results if available\n",
    "\n",
    "\n",
    "    # Preprocessing Filters\n",
    "    cross_pos_normalize    = True # Normalize words across parts of speech (e.g., \"learn\", \"learning\", \"learned\" -> \"learn\")\n",
    "    projects               = [\"oral_history\"]     # Filter by project names \n",
    "    data_groups            = [\"interview\"]   # Filter by data_groups \n",
    "    codes                  = [\"background\"]        # Analyse specific codes\n",
    "    excluded_codes         = ['interviewer']  # Exclude these codes, removing 'interviewer' is important for NLP\n",
    "\n",
    "    # Visualisation\n",
    "    title                = \"Semantic Network (all interviews, contextual embeddings)\"\n",
    "    link_threshold       = 0.50          \n",
    "    link_color_threshold = 0.75          # set to 99 to remove black links\n",
    "    custom_colors = True                 \n",
    "\n",
    "    # Seeds & Colours\n",
    "    seed_words = \"education: learning, teaching, student, school, classroom, curriculum, academic\"\n",
    "\n",
    "    # You can provide seed words in two formats:\n",
    "    # 1. Grouped format (for custom node colors and labels):\n",
    "    #       \"GroupName1: word1, word2; GroupName2: word3, word4; ...\"\n",
    "    #    Example: \n",
    "    #       \"Therapy: therapy, physical therapy; Caregivers: caregivers, caregiver; family\"\n",
    "    #    → 'family' will be treated as its own group, using its own color.\n",
    "    #\n",
    "    # 2. Flat list format (no grouping, default color):\n",
    "    #       \"dementia, therapy, caregivers\"\n",
    "    #    → All words will appear as individual highlighted nodes with default styling.\n",
    "    \n",
    "    # consider inductive readings, and analysis \n",
    "    cmap = cm.get_cmap(\"mako\", 5)\n",
    "    semantic_categories = {\n",
    "    \"People & Relations\": {\n",
    "        \"words\": {\n",
    "            \"people\", \"student\", \"students\", \"teacher\", \"teachers\", \"professor\", \"professors\",\n",
    "            \"advisor\", \"mentor\", \"classmates\", \"friends\", \"colleague\", \"colleagues\",\n",
    "            \"mother\", \"mom\", \"father\", \"dad\", \"family\", \"kids\", \"children\",\n",
    "            \"wife\", \"husband\", \"brother\", \"sister\"\n",
    "        },\n",
    "        \"color\": cmap(0)\n",
    "    },\n",
    "    \"Education & Career\": {\n",
    "        \"words\": {\n",
    "            \"school\", \"college\", \"university\", \"department\", \"campus\",\n",
    "            \"course\", \"courses\", \"class\", \"classes\", \"curriculum\", \"degree\",\n",
    "            \"major\", \"minor\", \"graduate\", \"graduated\", \"thesis\", \"exam\",\n",
    "            \"research\", \"lab\", \"laboratory\", \"project\", \"projects\",\n",
    "            \"engineering\", \"engineer\", \"physics\", \"mathematics\",\n",
    "            \"industry\", \"company\", \"career\", \"job\", \"internship\"\n",
    "        },\n",
    "        \"color\": cmap(1)\n",
    "    },\n",
    "    \"Emotions & Cognition\": {\n",
    "        \"words\": {\n",
    "            \"think\", \"thought\", \"know\", \"understand\", \"learn\", \"learning\",\n",
    "            \"decide\", \"decision\", \"believe\", \"remember\", \"idea\", \"ideas\",\n",
    "            \"feel\", \"feeling\", \"feelings\", \"excited\", \"interested\",\n",
    "            \"curious\", \"happy\", \"proud\", \"worried\", \"scared\", \"confused\",\n",
    "            \"reflect\", \"contemplate\", \"ponder\", \"analyze\", \"reason\"\n",
    "        },\n",
    "        \"color\": cmap(2)\n",
    "    },\n",
    "    \"Time / Duration\": {\n",
    "        \"words\": {\n",
    "            \"first\", \"second\", \"later\", \"since\", \"before\", \"after\",\n",
    "            \"started\", \"start\", \"begin\", \"early\", \"late\",\n",
    "            \"day\", \"days\", \"week\", \"weeks\", \"month\", \"months\",\n",
    "            \"year\", \"years\", \"semester\", \"summer\", \"winter\", \"spring\", \"fall\"\n",
    "        },\n",
    "        \"color\": cmap(3)\n",
    "    },\n",
    "    \"Daily Activities & Work\": {\n",
    "        \"words\": {\n",
    "            \"work\", \"working\", \"teach\", \"teaching\", \"study\", \"studying\",\n",
    "            \"read\", \"reading\", \"write\", \"writing\", \"talk\", \"talking\",\n",
    "            \"meet\", \"meeting\", \"present\", \"presentation\", \"travel\", \"move\",\n",
    "            \"build\", \"design\", \"make\", \"made\", \"fix\", \"support\", \"help\",\n",
    "            \"eat\", \"sleep\", \"walk\", \"exercise\", \"commute\", \"cook\", \"clean\",\n",
    "            \"shop\", \"plan\", \"organize\", \"schedule\", \"prepare\", \"attend\"\n",
    "        },\n",
    "        \"color\": cmap(4)\n",
    "    },\n",
    "}\n",
    "     # Network Layout Parameters\n",
    "    network_layout = \"kamada-kawai\"   # Options: \"spring\", \"kamada-kawai\", \"circular\", \"shell\"\n",
    "    \n",
    "    # Heatmap Parameters\n",
    "    num_codes    = 7              # Expand up to this many codes for heatmap\n",
    "    seed_codes   = [\"background\"] # Start from these seed codes (if None, top-N most frequent codes are used)\n",
    "    clustered    = True           # If True, apply hierarchical clustering to rows/columns\n",
    "\n",
    "    # Notes:\n",
    "    # - If seed_codes is given → heatmap expands with top-N co-occurring codes. \n",
    "    # - If seed_codes is None   → heatmap uses top-N most frequent codes in corpus. \n",
    "    # - num_codes defines N in both cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace7f320",
   "metadata": {},
   "source": [
    "## Full Visuals Version\n",
    "\n",
    "Heatmap + tSNE + Social Semantic Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc57ad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSemantic Network Analysis Tool\\n\" + \"=\" * 30)\n",
    "\n",
    "try:\n",
    "    # Build pydantic object\n",
    "    params = VisualsInput(\n",
    "        filepath            = csv_path,\n",
    "        stop_list           = stop_list_path if use_custom_stoplist else None,\n",
    "        num_words           = num_words,\n",
    "        clustering_method   = clustering_method,\n",
    "        distance_metric     = distance_metric,\n",
    "        window_size         = window_size,\n",
    "        min_word_frequency  = min_word_frequency,\n",
    "        projects            = projects,\n",
    "        data_groups         = data_groups,\n",
    "        codes               = codes,\n",
    "        cross_pos_normalize = cross_pos_normalize\n",
    "    )\n",
    "\n",
    "        \n",
    "\n",
    "    # Attach non-schema extras\n",
    "    setattr(params, \"reuse_clusterings\",    reuse_clusterings)\n",
    "    setattr(params, \"seed_words\",           seed_words)\n",
    "    setattr(params, \"custom_colors\",        custom_colors)\n",
    "    setattr(params, \"semantic_categories\",  semantic_categories)\n",
    "    setattr(params, \"link_threshold\",       link_threshold)\n",
    "    setattr(params, \"link_color_threshold\", link_color_threshold)\n",
    "    setattr(params, \"excluded_codes\",       excluded_codes)    \n",
    "\n",
    "\n",
    "    # Creates: (1) plain, (2) coloured, (3) coloured+seed-hidden\n",
    "    run_visuals_pipeline(params)\n",
    "\n",
    "\n",
    "except ValidationError as ve:\n",
    "    print(\"\\n⚠ Parameter error:\\n\", ve)\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠ Unexpected error: {e}\")\n",
    "finally:\n",
    "    print(\"\\nAnalysis process completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f982c74",
   "metadata": {},
   "source": [
    "## Interactive Version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebb8936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "OVERVIEW:\n",
    "----------------------\n",
    "This tool allows you to generate a semantic network based on word embeddings \n",
    "or co-occurrence similarity. It supports multiple clustering methods (RoBERTa, \n",
    "Jaccard, PMI, TF-IDF), customizable filters, and dynamic seed word grouping.\n",
    "\n",
    "USAGE INSTRUCTIONS:\n",
    "----------------------\n",
    "\n",
    "1. CSV File Selection:\n",
    "   - On launch, the tool will ask whether to use the last CSV file you worked with.\n",
    "   - If not, you can provide a new path or use the default.\n",
    "\n",
    "2. Reuse Previous Parameters:\n",
    "   - You’ll be asked whether to reuse previous analysis settings.\n",
    "   - If 'y' and a config exists, it will skip all prompts and directly run using cached data.\n",
    "   - If 'n' or no config exists, you'll go through the full interactive setup.\n",
    "\n",
    "3. Stop Word List (Optional):\n",
    "   - You can load a custom stop words file (one word per line).\n",
    "   - If not provided, the default list will be used.\n",
    "\n",
    "4. Clustering Method & Parameters:\n",
    "   - Choose from: 1 = RoBERTa, 2 = Jaccard, 3 = PMI, 4 = TF-IDF.\n",
    "   - For methods 2–4, specify distance metric (default or cosine) and context window size.\n",
    "\n",
    "5. Analysis Filters:\n",
    "   - Specify the number of words to include, minimum frequency, and metadata filters\n",
    "     (projects, data groups, codes).\n",
    "\n",
    "6. Seed Words:\n",
    "   - You may enter structured seed groups (e.g. \"Grief: grief, sorrow; Memory: memory, recall\")\n",
    "     or leave blank to use the default.\n",
    "\n",
    "7. Output:\n",
    "   - The tool generates:\n",
    "     - A semantic network (with and without seed nodes)\n",
    "     - A similarity heatmap\n",
    "     - A t-SNE plot\n",
    "   - All visualizations are saved to the `OUTPUT_DIR` directory.\n",
    "\n",
    "Note:\n",
    "- The system caches results based on parameter combinations and data hash.\n",
    "- Settings are saved to 'last_run_config.json' and CSV path to 'last_csv_path.txt'.\n",
    "\n",
    "\"\"\"\n",
    "    try:\n",
    "        print(\"Semantic Network Analysis Tool\")\n",
    "        print(\"==============================\")\n",
    "\n",
    "        # --- Default configuration ---\n",
    "        default_csv_path           = CSV_PATH\n",
    "        default_stop_list_path     = STOP_LIST_FILE\n",
    "        default_window_size        = 20\n",
    "        default_num_words          = 25\n",
    "        default_min_word_frequency = 2\n",
    "        default_excluded_codes    = ['interviewer'] # Exclude these codes, removing 'interviewer' is important for NLP\n",
    "        default_seed_words         = \"education: learning, teaching, student, school, classroom, curriculum, academic\"\n",
    "        cmap = cm.get_cmap(\"mako\", 5)\n",
    "        default_semantic_categories = {\n",
    "    \"People & Relations\": {\n",
    "        \"words\": {\n",
    "            \"people\", \"student\", \"students\", \"teacher\", \"teachers\", \"professor\", \"professors\",\n",
    "            \"advisor\", \"mentor\", \"classmates\", \"friends\", \"colleague\", \"colleagues\",\n",
    "            \"mother\", \"mom\", \"father\", \"dad\", \"family\", \"kids\", \"children\",\n",
    "            \"wife\", \"husband\", \"brother\", \"sister\"\n",
    "        },\n",
    "        \"color\": cmap(0)\n",
    "    },\n",
    "    \"Education & Career\": {\n",
    "        \"words\": {\n",
    "            \"school\", \"college\", \"university\", \"department\", \"campus\",\n",
    "            \"course\", \"courses\", \"class\", \"classes\", \"curriculum\", \"degree\",\n",
    "            \"major\", \"minor\", \"graduate\", \"graduated\", \"thesis\", \"exam\",\n",
    "            \"research\", \"lab\", \"laboratory\", \"project\", \"projects\",\n",
    "            \"engineering\", \"engineer\", \"physics\", \"mathematics\",\n",
    "            \"industry\", \"company\", \"career\", \"job\", \"internship\"\n",
    "        },\n",
    "        \"color\": cmap(1)\n",
    "    },\n",
    "    \"Emotions & Cognition\": {\n",
    "        \"words\": {\n",
    "            \"think\", \"thought\", \"know\", \"understand\", \"learn\", \"learning\",\n",
    "            \"decide\", \"decision\", \"believe\", \"remember\", \"idea\", \"ideas\",\n",
    "            \"feel\", \"feeling\", \"feelings\", \"excited\", \"interested\",\n",
    "            \"curious\", \"happy\", \"proud\", \"worried\", \"scared\", \"confused\"\n",
    "        },\n",
    "        \"color\": cmap(2)\n",
    "    },\n",
    "    \"Time / Duration\": {\n",
    "        \"words\": {\n",
    "            \"first\", \"second\", \"later\", \"since\", \"before\", \"after\",\n",
    "            \"started\", \"start\", \"begin\", \"early\", \"late\",\n",
    "            \"day\", \"days\", \"week\", \"weeks\", \"month\", \"months\",\n",
    "            \"year\", \"years\", \"semester\", \"summer\", \"winter\", \"spring\", \"fall\"\n",
    "        },\n",
    "        \"color\": cmap(3)\n",
    "    },\n",
    "    \"Daily Activities & Work\": {\n",
    "        \"words\": {\n",
    "            \"work\", \"working\", \"teach\", \"teaching\", \"study\", \"studying\",\n",
    "            \"read\", \"reading\", \"write\", \"writing\", \"talk\", \"talking\",\n",
    "            \"meet\", \"meeting\", \"present\", \"presentation\", \"travel\", \"move\",\n",
    "            \"build\", \"design\", \"make\", \"made\", \"fix\", \"support\", \"help\"\n",
    "        },\n",
    "        \"color\": cmap(4)\n",
    "    },\n",
    "}\n",
    "\n",
    "        default_title  = \"Semantic Network: (contextual embeddings)\"\n",
    "        default_link_threshold     = 0.5\n",
    "        default_link_color_thresh  = 0.75\n",
    "        \n",
    "        # Check for previous CSV file\n",
    "        previous_csv_path = \"\"\n",
    "        if os.path.exists(LAST_CSV_PATH):\n",
    "            try:\n",
    "                with open(LAST_CSV_PATH, 'r') as f:\n",
    "                    previous_csv_path = f.read().strip()\n",
    "                if os.path.exists(previous_csv_path):\n",
    "                    use_last = ask_yes_no(f\"Use previous CSV file? ({previous_csv_path})\", default=False)\n",
    "                    if use_last:\n",
    "                        csv_path = previous_csv_path\n",
    "                    else:\n",
    "                        csv_path = ask_path(f\"Enter CSV file path\", default_csv_path)\n",
    "                else:\n",
    "                    print(f\"Previous CSV file not found: {previous_csv_path}\")\n",
    "                    csv_path = ask_path(f\"Enter CSV file path\", default_csv_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading previous CSV path: {e}\")\n",
    "                csv_path = ask_path(f\"Enter CSV file path\", default_csv_path)\n",
    "        else:\n",
    "            csv_path = ask_path(f\"Enter CSV file path\", default_csv_path)\n",
    "\n",
    "        \n",
    "        # Validate CSV path\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"Error: File {csv_path} not found\")\n",
    "            exit(1)\n",
    "\n",
    "        # Save the current CSV path for next time\n",
    "        try:\n",
    "            with open(LAST_CSV_PATH, 'w') as f:\n",
    "                f.write(csv_path)\n",
    "            print(f\"Saved current CSV path for future use: {csv_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save CSV path for future use: {e}\")\n",
    "\n",
    "        # Ask about stop words\n",
    "        use_stop_list = ask_yes_no(f\"Use a custom stop-words file? (default path shown: {default_stop_list_path})\", default=False)\n",
    "        stop_list_path = None\n",
    "        if use_stop_list:\n",
    "            stop_list_path = ask_path(\"Enter STOP words file path\", default_stop_list_path)\n",
    "            # Validate stop list path\n",
    "            if stop_list_path and not os.path.exists(stop_list_path):\n",
    "                print(f\"Warning: Stop words file {stop_list_path} not found. Using default stop words only.\")\n",
    "                stop_list_path = None\n",
    "            else:\n",
    "                # Debug stoplist\n",
    "                with open(stop_list_path, 'r') as f:\n",
    "                    custom_stopwords = [line.strip() for line in f if line.strip()]\n",
    "                print(f\"Loaded {len(custom_stopwords)} custom stopwords\")\n",
    "                print(f\"First 10 stopwords: {custom_stopwords[:10]}\")\n",
    "                print(f\"Stoplist file size: {os.path.getsize(stop_list_path)} bytes\")\n",
    "\n",
    "        reuse_clusterings = ask_yes_no(\"Reuse previous clusterings and parameters?\", default=True)\n",
    "\n",
    "        if reuse_clusterings: # reuse from the cache\n",
    "            if os.path.exists(LAST_CONFIG_PATH):\n",
    "                print(\"Loading previous parameters...\")\n",
    "                with open(LAST_CONFIG_PATH, 'r') as f:\n",
    "                    saved_params = json.load(f)\n",
    "                # Reconstruct VisualsInput from saved dictionary\n",
    "                params = VisualsInput(**saved_params)\n",
    "                setattr(params, \"custom_colors\", True)\n",
    "                setattr(params, \"link_threshold\", default_link_threshold)\n",
    "                setattr(params, \"link_color_threshold\", default_link_color_thresh)\n",
    "                run_visuals_pipeline(params)\n",
    "                return  # Exit early since pipeline is already run\n",
    "        else:\n",
    "            print(\"No saved parameter file found. Proceeding to manual input...\")\n",
    "            while True:\n",
    "                try:\n",
    "                    clustering_method = int(\n",
    "                        input(\"Choose clustering method 1=RoBERTa, 2=Jaccard, 3=PMI, 4=TF-IDF [Default 1]: \").strip() or \"1\"\n",
    "                        )\n",
    "                    if clustering_method in (1, 2, 3, 4):\n",
    "                        break\n",
    "                    print(\"Please enter 1-4\")\n",
    "                except ValueError:\n",
    "                    print(\"Please enter valid number\")\n",
    "\n",
    "            # distance metric & window\n",
    "            distance_metric = \"default\"\n",
    "            window_size = default_window_size\n",
    "            if clustering_method in (2, 3, 4):\n",
    "                metric_choice = input(\"Distance metric 1=Default, 2=Cosine [Default 1]: \").strip()\n",
    "                distance_metric = \"cosine\" if metric_choice == \"2\" else \"default\"\n",
    "                while True:\n",
    "                    try:\n",
    "                        window_size = int(\n",
    "                            ask_path(f\"Window size for co-occurrence\", default_window_size)\n",
    "                            )\n",
    "                        if window_size > 0:\n",
    "                            break\n",
    "                        print(\"Window size must be positive.\")\n",
    "                    except ValueError:\n",
    "                        print(\"Please enter valid number\")\n",
    "\n",
    "            # other analysis params\n",
    "            num_words = int(ask_path(f\"How many words to analyze?\", default_num_words))\n",
    "            min_word_frequency = int(ask_path(f\"Min word frequency\", default_min_word_frequency))\n",
    "            cross_pos_normalize = ask_yes_no(\"Normalize words across POS?\", default=True)\n",
    "            seed_input = ask_path(\"Seed words (blank for default): \", default_seed_words)\n",
    "\n",
    "            # filters\n",
    "            projects_input = input(f\"Projects comma-sep (Enter/space for skip): \").strip()\n",
    "            projects = [p.strip() for p in projects_input.split(\",\") if p.strip()] if projects_input else None\n",
    "            data_groups_input = input(\"Data groups comma-sep (Enter/space for skip): \").strip()\n",
    "            data_groups = [g.strip() for g in data_groups_input.split(\",\") if g.strip()] if data_groups_input else None\n",
    "            codes_input = input(\"Codes comma-sep (Enter/space for skip): \").strip()\n",
    "            codes = [c.strip() for c in codes_input.split(\",\") if c.strip()] if codes_input else None\n",
    "            excluded_codes_input = input(\"Codes you want to exclude from analysis comma-sep (Enter/space for skip): \").strip()\n",
    "            excluded_codes = [e.strip() for e in excluded_codes_input.split(\",\") if e.strip()] if excluded_codes_input else default_excluded_codes\n",
    "\n",
    "            # Consider inductive readings, and analysis \n",
    "            print(\n",
    "                \"\\n▶ OPTIONAL — define semantic colour groups.\\n\"\n",
    "                \"  Format per group:  GroupName:{'color':'#HEX','words':[w1,w2]}\\n\"\n",
    "                \"  Separate multiple groups with a semicolon ‘;’ on one line.\\n\"\n",
    "                \"  Example:\\n\"\n",
    "                \"    Health:{'color':'#F8961E','words':['health','clinician']};\\n\"\n",
    "                \"    Roles :{'color':'#43AA8B','words':['parent','caregiver']}\\n\"\n",
    "                \"  Press <Enter> to keep the default shown below.\\n\"\n",
    "                f\"  Default = {default_semantic_categories}\\n\"\n",
    "            )\n",
    "\n",
    "            raw_sc = input(\"semantic_categories → \").strip()\n",
    "\n",
    "            if not raw_sc:                                   # user kept the defaults\n",
    "                semantic_categories = default_semantic_categories.copy()\n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    semantic_categories = {}\n",
    "                    # allow several groups separated by ';'\n",
    "                    for block in filter(None, map(str.strip, raw_sc.split(';'))):\n",
    "                        if ':' not in block:\n",
    "                            raise ValueError(f\"missing ':' in «{block}»\")\n",
    "\n",
    "                        label, dict_part = map(str.strip, block.split(':', 1))\n",
    "\n",
    "                        # try JSON first, then Python literal\n",
    "                        try:\n",
    "                            cat_dict = json.loads(dict_part)\n",
    "                        except json.JSONDecodeError:\n",
    "                            cat_dict = ast.literal_eval(dict_part)\n",
    "\n",
    "                        # minimal validation\n",
    "                        if not isinstance(cat_dict, dict) or \\\n",
    "                        'color' not in cat_dict or 'words' not in cat_dict:\n",
    "                            raise ValueError(\n",
    "                                f\"Group ‘{label}’ must contain 'color' and 'words' keys\"\n",
    "                            )\n",
    "\n",
    "                        # normalise words to lower-case for matching\n",
    "                        cat_dict['words'] = [w.lower() for w in cat_dict['words']]\n",
    "                        semantic_categories[label] = cat_dict\n",
    "\n",
    "                    if not semantic_categories:\n",
    "                        raise ValueError(\"no valid groups parsed\")\n",
    "\n",
    "                except Exception as err:\n",
    "                    print(\"⚠  Could not parse your custom groups. \"\n",
    "                        \"Falling back to the default set.\\n\", err)\n",
    "        semantic_categories = default_semantic_categories.copy()\n",
    "\n",
    "        print(\"\\nRunning analysis...\\n\")\n",
    "\n",
    "        params = VisualsInput(\n",
    "            filepath=csv_path,\n",
    "            stop_list=stop_list_path,\n",
    "            num_words=num_words,\n",
    "            clustering_method=clustering_method,\n",
    "            distance_metric=distance_metric,\n",
    "            window_size=window_size,\n",
    "            min_word_frequency=min_word_frequency,\n",
    "            projects=projects,\n",
    "            data_groups= data_groups,\n",
    "            codes=codes,\n",
    "            cross_pos_normalize=cross_pos_normalize\n",
    "        )\n",
    "\n",
    "        setattr(params, \"reuse_clusterings\", reuse_clusterings)\n",
    "        setattr(params, \"seed_words\", seed_input)\n",
    "        setattr(params, \"custom_colors\", True)\n",
    "        setattr(params, \"semantic_categories\", semantic_categories)\n",
    "        setattr(params, \"link_threshold\", default_link_threshold)\n",
    "        setattr(params, \"link_color_threshold\", default_link_color_thresh)\n",
    "        setattr(params, \"excluded_codes\", excluded_codes)\n",
    "\n",
    "        try:\n",
    "            with open(LAST_CONFIG_PATH, 'w') as f:\n",
    "                json.dump(params.__dict__, f, indent=2)\n",
    "            print(f\"Parameters saved to {LAST_CONFIG_PATH}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to save parameters: {e}\")\n",
    "\n",
    "        run_visuals_pipeline(params)\n",
    "\n",
    "    except ValidationError as ve:\n",
    "        print(\"\\n⚠ Parameter validation error:\\n\", ve)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n⚠ Unexpected error: {e}\")\n",
    "    finally:\n",
    "        print(\"\\nAnalysis process completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536568e4-3b86-4507-852a-9bee92d6d18e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddb3204-ce24-4895-8c23-ed640eb6e407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d443a3f1-86ae-4d39-bd87-9781538cd388",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmap_visualization_toolkit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
